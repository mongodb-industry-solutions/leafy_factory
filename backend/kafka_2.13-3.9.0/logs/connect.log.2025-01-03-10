[2025-01-03 10:09:35,267] INFO Kafka Connect worker initializing ... (org.apache.kafka.connect.cli.AbstractConnectCli:114)
[2025-01-03 10:09:35,269] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote=true, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 23, 23+37-2369
	jvm.classpath = /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/activation-1.1.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/aopalliance-repackaged-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/argparse4j-0.7.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/audience-annotations-0.12.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/caffeine-2.9.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-beanutils-1.9.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-cli-1.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-collections-3.2.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-digester-2.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-io-2.14.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-lang3-3.12.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-logging-1.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-validator-1.7.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-basic-auth-extension-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-json-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-mirror-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-mirror-client-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-runtime-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-transforms-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/error_prone_annotations-2.10.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-api-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-locator-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-utils-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-annotations-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-core-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-databind-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-dataformat-csv-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-datatype-jdk8-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-jaxrs-base-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-jaxrs-json-provider-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-afterburner-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-jaxb-annotations-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-scala_2.13-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.activation-api-1.2.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.annotation-api-1.3.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.inject-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.validation-api-2.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.xml.bind-api-2.3.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javassist-3.29.2-GA.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.activation-api-1.2.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.annotation-api-1.3.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.servlet-api-3.1.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.ws.rs-api-2.1.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jaxb-api-2.3.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-client-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-common-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-container-servlet-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-container-servlet-core-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-hk2-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-server-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-client-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-continuation-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-http-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-io-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-security-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-server-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-servlet-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-servlets-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-util-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-util-ajax-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jline-3.25.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jopt-simple-5.0.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jose4j-0.9.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jsr305-3.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-clients-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-group-coordinator-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-group-coordinator-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-metadata-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-raft-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-server-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-server-common-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-shell-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-storage-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-storage-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-examples-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-scala_2.13-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-test-utils-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-tools-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-tools-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-transaction-coordinator-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka_2.13-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/lz4-java-1.8.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/maven-artifact-3.9.6.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/metrics-core-2.2.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/metrics-core-4.1.12.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-buffer-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-codec-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-common-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-handler-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-resolver-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-classes-epoll-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-native-epoll-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-native-unix-common-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/opentelemetry-proto-1.0.0-alpha.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/osgi-resource-locator-1.0.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/paranamer-2.8.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/pcollections-4.0.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/plexus-utils-3.5.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/protobuf-java-3.25.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/reflections-0.10.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/reload4j-1.2.25.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/rocksdbjni-7.9.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-collection-compat_2.13-2.10.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-java8-compat_2.13-1.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-library-2.13.14.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-logging_2.13-3.9.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-reflect-2.13.14.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/slf4j-api-1.7.36.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/slf4j-reload4j-1.7.36.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/snappy-java-1.1.10.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/swagger-annotations-2.2.8.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/trogdor-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zookeeper-3.8.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zookeeper-jute-3.8.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zstd-jni-1.5.6-4.jar
	os.spec = Mac OS X, aarch64, 15.0
	os.vcpus = 11
 (org.apache.kafka.connect.runtime.WorkerInfo:72)
[2025-01-03 10:09:35,270] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.AbstractConnectCli:120)
[2025-01-03 10:09:35,280] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:09:35,313] INFO Using up-to-date JsonConverter implementation (io.debezium.converters.CloudEventsConverter:120)
[2025-01-03 10:09:35,397] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:09:35,398] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:09:35,409] INFO Using up-to-date JsonConverter implementation (io.debezium.converters.CloudEventsConverter:120)
[2025-01-03 10:09:35,423] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:09:35,425] INFO Loading plugin from: classpath (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:09:35,428] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@5a07e868 (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:09:35,428] INFO Scanning plugins with ServiceLoaderScanner took 148 ms (org.apache.kafka.connect.runtime.isolation.PluginScanner:71)
[2025-01-03 10:09:35,429] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:09:35,552] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:09:35,552] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:09:35,677] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:09:35,677] INFO Loading plugin from: classpath (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:09:36,147] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@5a07e868 (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:09:36,147] INFO Scanning plugins with ReflectionScanner took 718 ms (org.apache.kafka.connect.runtime.isolation.PluginScanner:71)
[2025-01-03 10:09:36,148] WARN One or more plugins are missing ServiceLoader manifests may not be usable with plugin.discovery=service_load: [
file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/	io.debezium.connector.mongodb.MongoDbSinkConnector	sink	3.0.5.Final
]
Read the documentation at https://kafka.apache.org/documentation.html#connect_plugindiscovery for instructions on migrating your plugins to take advantage of the performance improvements of service_load mode. To silence this warning, set plugin.discovery=only_scan in the worker config. (org.apache.kafka.connect.runtime.isolation.Plugins:123)
[2025-01-03 10:09:36,149] INFO Added plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,149] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,149] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,149] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,149] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,149] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,149] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,149] INFO Added plugin 'io.debezium.transforms.HeaderToValue' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,149] INFO Added plugin 'io.debezium.transforms.SchemaChangeEventFilter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,149] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,149] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,149] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,149] INFO Added plugin 'io.debezium.transforms.ExtractSchemaToNewRecord' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,150] INFO Added plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,150] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,150] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,150] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,150] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,150] INFO Added plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,150] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,150] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,150] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,150] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,150] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,150] INFO Added plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,150] INFO Added plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,150] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,150] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,150] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,150] INFO Added plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,150] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,150] INFO Added plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,150] INFO Added plugin 'io.debezium.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,150] INFO Added plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,150] INFO Added plugin 'io.debezium.transforms.partitions.PartitionRouting' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,150] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,150] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,150] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,150] INFO Added plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,150] INFO Added plugin 'io.debezium.transforms.TimezoneConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,151] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,151] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,151] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,151] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,151] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,151] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,151] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,151] INFO Added plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,151] INFO Added plugin 'io.debezium.transforms.ExtractChangedRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,151] INFO Added plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,151] INFO Added plugin 'io.debezium.connector.mongodb.transforms.outbox.MongoEventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,151] INFO Added plugin 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,151] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,151] INFO Added plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,151] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,151] INFO Added plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,151] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,151] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,151] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,151] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,151] INFO Added plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,151] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,151] INFO Added plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,151] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,151] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,151] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,151] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,151] INFO Added plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,151] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,151] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:09:36,152] INFO Added alias 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,152] INFO Added alias 'ExtractNewDocumentState' to plugin 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,152] INFO Added alias 'JsonConverter' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,152] INFO Added alias 'DebeziumMySql' to plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,152] INFO Added alias 'CloudEventsConverter' to plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,152] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,152] INFO Added alias 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,152] INFO Added alias 'DebeziumMySqlConnectRestExtension' to plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,152] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,152] INFO Added alias 'MySql' to plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,152] INFO Added alias 'MongoDbSinkConnector' to plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,152] INFO Added alias 'EnvVar' to plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,152] INFO Added alias 'EnvVarConfigProvider' to plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,152] INFO Added alias 'MongoDb' to plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,152] INFO Added alias 'MirrorCheckpointConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'Boolean' to plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'MySqlConnector' to plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'NoneConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'HeaderToValue' to plugin 'io.debezium.transforms.HeaderToValue' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'PartitionRouting' to plugin 'io.debezium.transforms.partitions.PartitionRouting' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'MongoDbSink' to plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'StringConverter' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'ByLogicalTableRouter' to plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'MongoDbConnector' to plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'FileConfigProvider' to plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'IntegerConverter' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'EventRouter' to plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'SchemaChangeEventFilter' to plugin 'io.debezium.transforms.SchemaChangeEventFilter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'LongConverter' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'DropHeaders' to plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'ExtractSchemaToNewRecord' to plugin 'io.debezium.transforms.ExtractSchemaToNewRecord' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'BinaryData' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'File' to plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'FloatConverter' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'DirectoryConfigProvider' to plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'ReadToInsertEvent' to plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'ShortConverter' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'CloudEvents' to plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'ActivateTracingSpan' to plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'TimezoneConverter' to plugin 'io.debezium.transforms.TimezoneConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'DoubleConverter' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'Simple' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'BinaryDataConverter' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,153] INFO Added alias 'AllConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,154] INFO Added alias 'ExtractNewRecordState' to plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,154] INFO Added alias 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,154] INFO Added alias 'DebeziumMongoDb' to plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,154] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,154] INFO Added alias 'MirrorHeartbeatConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,154] INFO Added alias 'InsertHeader' to plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,154] INFO Added alias 'Directory' to plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,154] INFO Added alias 'MirrorSourceConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,154] INFO Added alias 'PrincipalConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,154] INFO Added alias 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,154] INFO Added alias 'DebeziumMongoDbConnectRestExtension' to plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,154] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,154] INFO Added alias 'BooleanConverter' to plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,154] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,154] INFO Added alias 'MongoEventRouter' to plugin 'io.debezium.connector.mongodb.transforms.outbox.MongoEventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,154] INFO Added alias 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,154] INFO Added alias 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,154] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,154] INFO Added alias 'ExtractChangedRecordState' to plugin 'io.debezium.transforms.ExtractChangedRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,154] INFO Added alias 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,154] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,154] INFO Added alias 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:09:36,157] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	config.providers = []
	connector.client.config.override.policy = All
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = [http://:8083]
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.discovery = hybrid_warn
	plugin.path = [/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins]
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:371)
[2025-01-03 10:09:36,157] INFO Creating Kafka admin client (org.apache.kafka.connect.runtime.WorkerConfig:281)
[2025-01-03 10:09:36,158] INFO AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:371)
[2025-01-03 10:09:36,178] INFO These configurations '[offset.flush.interval.ms, key.converter.schemas.enable, offset.storage.file.filename, value.converter.schemas.enable, plugin.path, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2025-01-03 10:09:36,178] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:09:36,178] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:09:36,178] INFO Kafka startTimeMs: 1735920576178 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:09:36,294] INFO Kafka cluster ID: 7MkOCEgqRyytrrS9sGNpvQ (org.apache.kafka.connect.runtime.WorkerConfig:298)
[2025-01-03 10:09:36,294] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:09:36,296] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:09:36,296] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:09:36,296] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:09:36,298] INFO PublicConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	listeners = [http://:8083]
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
 (org.apache.kafka.connect.runtime.rest.RestServerConfig$PublicConfig:371)
[2025-01-03 10:09:36,302] INFO Logging initialized @1280ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:170)
[2025-01-03 10:09:36,315] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:125)
[2025-01-03 10:09:36,315] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:196)
[2025-01-03 10:09:36,323] INFO jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 23+37-2369 (org.eclipse.jetty.server.Server:375)
[2025-01-03 10:09:36,331] INFO Started http_8083@7c8f047a{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:333)
[2025-01-03 10:09:36,331] INFO Started @1310ms (org.eclipse.jetty.server.Server:415)
[2025-01-03 10:09:36,339] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 10:09:36,340] INFO REST server listening at http://192.168.1.5:8083/, advertising URL http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:216)
[2025-01-03 10:09:36,340] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 10:09:36,340] INFO REST admin endpoints at http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2025-01-03 10:09:36,340] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 10:09:36,340] INFO Setting up All Policy for ConnectorClientConfigOverride. This will allow all client configurations to be overridden (org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy:45)
[2025-01-03 10:09:36,341] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:09:36,345] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:09:36,345] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:09:36,345] INFO Kafka startTimeMs: 1735920576345 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:09:36,347] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:09:36,347] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:09:36,350] INFO Kafka Connect worker initialization took 1082ms (org.apache.kafka.connect.cli.AbstractConnectCli:141)
[2025-01-03 10:09:36,350] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:67)
[2025-01-03 10:09:36,350] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:112)
[2025-01-03 10:09:36,350] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:233)
[2025-01-03 10:09:36,351] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:64)
[2025-01-03 10:09:36,351] INFO Worker started (org.apache.kafka.connect.runtime.Worker:243)
[2025-01-03 10:09:36,351] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:114)
[2025-01-03 10:09:36,351] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2025-01-03 10:09:36,361] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:238)
[2025-01-03 10:09:36,379] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:334)
[2025-01-03 10:09:36,379] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:339)
[2025-01-03 10:09:36,379] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:132)
[2025-01-03 10:09:36,507] INFO Started o.e.j.s.ServletContextHandler@69de72ec{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:921)
[2025-01-03 10:09:36,508] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:303)
[2025-01-03 10:09:36,509] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:77)
[2025-01-03 10:09:36,531] INFO Loading the custom source info struct maker plugin: io.debezium.connector.mysql.MySqlSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2025-01-03 10:09:36,979] INFO Using 'SHOW MASTER STATUS' to get binary log status (io.debezium.connector.mysql.jdbc.MySqlConnection:41)
[2025-01-03 10:09:36,984] INFO Successfully tested connection for jdbc:mysql://localhost:3306/?useInformationSchema=true&nullCatalogMeansCurrent=false&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8&zeroDateTimeBehavior=CONVERT_TO_NULL&connectTimeout=30000 with user 'root' (io.debezium.connector.binlog.BinlogConnector:66)
[2025-01-03 10:09:36,985] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection:983)
[2025-01-03 10:09:36,986] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:371)
[2025-01-03 10:09:36,989] INFO [mariadb-connector|worker] Creating connector mariadb-connector of type io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:313)
[2025-01-03 10:09:36,990] INFO [mariadb-connector|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2025-01-03 10:09:36,990] INFO [mariadb-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 10:09:36,991] INFO [mariadb-connector|worker] Instantiated connector mariadb-connector with version 3.0.5.Final of type class io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:335)
[2025-01-03 10:09:36,992] INFO [mariadb-connector|worker] Finished creating connector mariadb-connector (org.apache.kafka.connect.runtime.Worker:356)
[2025-01-03 10:09:36,992] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2025-01-03 10:09:36,992] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 10:09:36,994] INFO [mariadb-connector|task-0] Creating task mariadb-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2025-01-03 10:09:36,995] INFO [mariadb-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mariadb-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2025-01-03 10:09:36,995] INFO [mariadb-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mariadb-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 10:09:36,995] INFO [mariadb-connector|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.mysql.MySqlConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2025-01-03 10:09:36,996] INFO [mariadb-connector|task-0] Instantiated task mariadb-connector-0 with version 3.0.5.Final of type io.debezium.connector.mysql.MySqlConnectorTask (org.apache.kafka.connect.runtime.Worker:665)
[2025-01-03 10:09:36,996] INFO [mariadb-connector|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:09:36,996] INFO [mariadb-connector|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mariadb-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:678)
[2025-01-03 10:09:36,996] INFO [mariadb-connector|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:09:36,996] INFO [mariadb-connector|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mariadb-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:684)
[2025-01-03 10:09:36,996] INFO [mariadb-connector|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mariadb-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2025-01-03 10:09:36,997] INFO [mariadb-connector|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:1795)
[2025-01-03 10:09:36,997] INFO [mariadb-connector|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2025-01-03 10:09:36,997] INFO [mariadb-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 10:09:36,999] INFO [mariadb-connector|task-0] ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-mariadb-connector-0
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 10:09:37,006] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:09:37,013] INFO [mariadb-connector|task-0] These configurations '[metrics.context.connect.kafka.cluster.id]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 10:09:37,013] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:09:37,013] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:09:37,013] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735920577013 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:09:37,018] INFO Created connector mariadb-connector (org.apache.kafka.connect.cli.ConnectStandalone:89)
[2025-01-03 10:09:37,018] INFO [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] Cluster ID: 7MkOCEgqRyytrrS9sGNpvQ (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:09:37,018] INFO [mariadb-connector|task-0] Starting MySqlConnectorTask with configuration:
   connector.class = io.debezium.connector.mysql.MySqlConnector
   database.user = root
   database.server.id = 184054
   database.history.kafka.bootstrap.servers = localhost:9092
   database.history.kafka.topic = db.history.leafy_factory
   database.server.name = leafy_factory
   schema.history.internal.kafka.bootstrap.servers = localhost:9092
   database.port = 3306
   include.schema.changes = false
   topic.prefix = db_
   schema.history.internal.kafka.topic = db.history.internal
   task.class = io.debezium.connector.mysql.MySqlConnectorTask
   database.hostname = localhost
   database.password = ********
   name = mariadb-connector
   database.include.list = leafy_factory
 (io.debezium.connector.common.BaseSourceTask:250)
[2025-01-03 10:09:37,019] INFO [mariadb-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mysql.MySqlSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2025-01-03 10:09:37,019] ERROR Failed to create connector for config/mongodb-sink.properties (org.apache.kafka.connect.cli.ConnectStandalone:87)
[2025-01-03 10:09:37,019] INFO [mariadb-connector|task-0] Loading the custom topic naming strategy plugin: io.debezium.schema.DefaultTopicNamingStrategy (io.debezium.config.CommonConnectorConfig:1401)
[2025-01-03 10:09:37,019] ERROR Stopping after connector error (org.apache.kafka.connect.cli.ConnectStandalone:99)
java.util.concurrent.ExecutionException: org.apache.kafka.connect.errors.ConnectException: Failed to find any class that implements Connector and which name matches com.mongodb.kafka.connect.MongoSinkConnector, available connectors are: PluginDesc{klass=class io.debezium.connector.mongodb.MongoDbConnector, name='io.debezium.connector.mongodb.MongoDbConnector', version='3.0.5.Final', encodedVersion=3.0.5.Final, type=source, typeName='source', location='file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/'}, PluginDesc{klass=class io.debezium.connector.mongodb.MongoDbSinkConnector, name='io.debezium.connector.mongodb.MongoDbSinkConnector', version='3.0.5.Final', encodedVersion=3.0.5.Final, type=sink, typeName='sink', location='file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/'}, PluginDesc{klass=class io.debezium.connector.mysql.MySqlConnector, name='io.debezium.connector.mysql.MySqlConnector', version='3.0.5.Final', encodedVersion=3.0.5.Final, type=source, typeName='source', location='file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql/'}, PluginDesc{klass=class org.apache.kafka.connect.mirror.MirrorCheckpointConnector, name='org.apache.kafka.connect.mirror.MirrorCheckpointConnector', version='3.9.0', encodedVersion=3.9.0, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.mirror.MirrorHeartbeatConnector, name='org.apache.kafka.connect.mirror.MirrorHeartbeatConnector', version='3.9.0', encodedVersion=3.9.0, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.mirror.MirrorSourceConnector, name='org.apache.kafka.connect.mirror.MirrorSourceConnector', version='3.9.0', encodedVersion=3.9.0, type=source, typeName='source', location='classpath'}
	at org.apache.kafka.connect.util.ConvertingFutureCallback.result(ConvertingFutureCallback.java:135)
	at org.apache.kafka.connect.util.ConvertingFutureCallback.get(ConvertingFutureCallback.java:108)
	at org.apache.kafka.connect.cli.ConnectStandalone.processExtraArgs(ConnectStandalone.java:95)
	at org.apache.kafka.connect.cli.AbstractConnectCli.run(AbstractConnectCli.java:96)
	at org.apache.kafka.connect.cli.ConnectStandalone.main(ConnectStandalone.java:185)
Caused by: org.apache.kafka.connect.errors.ConnectException: Failed to find any class that implements Connector and which name matches com.mongodb.kafka.connect.MongoSinkConnector, available connectors are: PluginDesc{klass=class io.debezium.connector.mongodb.MongoDbConnector, name='io.debezium.connector.mongodb.MongoDbConnector', version='3.0.5.Final', encodedVersion=3.0.5.Final, type=source, typeName='source', location='file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/'}, PluginDesc{klass=class io.debezium.connector.mongodb.MongoDbSinkConnector, name='io.debezium.connector.mongodb.MongoDbSinkConnector', version='3.0.5.Final', encodedVersion=3.0.5.Final, type=sink, typeName='sink', location='file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/'}, PluginDesc{klass=class io.debezium.connector.mysql.MySqlConnector, name='io.debezium.connector.mysql.MySqlConnector', version='3.0.5.Final', encodedVersion=3.0.5.Final, type=source, typeName='source', location='file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql/'}, PluginDesc{klass=class org.apache.kafka.connect.mirror.MirrorCheckpointConnector, name='org.apache.kafka.connect.mirror.MirrorCheckpointConnector', version='3.9.0', encodedVersion=3.9.0, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.mirror.MirrorHeartbeatConnector, name='org.apache.kafka.connect.mirror.MirrorHeartbeatConnector', version='3.9.0', encodedVersion=3.9.0, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.mirror.MirrorSourceConnector, name='org.apache.kafka.connect.mirror.MirrorSourceConnector', version='3.9.0', encodedVersion=3.9.0, type=source, typeName='source', location='classpath'}
	at org.apache.kafka.connect.runtime.isolation.Plugins.connectorClass(Plugins.java:321)
	at org.apache.kafka.connect.runtime.isolation.Plugins.newConnector(Plugins.java:292)
	at org.apache.kafka.connect.runtime.AbstractHerder.lambda$getConnector$7(AbstractHerder.java:954)
	at java.base/java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1713)
	at org.apache.kafka.connect.runtime.AbstractHerder.getConnector(AbstractHerder.java:954)
	at org.apache.kafka.connect.runtime.AbstractHerder.validateConnectorConfig(AbstractHerder.java:667)
	at org.apache.kafka.connect.runtime.AbstractHerder.lambda$validateConnectorConfig$3(AbstractHerder.java:579)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:09:37,021] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:87)
[2025-01-03 10:09:37,021] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:358)
[2025-01-03 10:09:37,024] INFO Stopped o.e.j.s.ServletContextHandler@69de72ec{/,null,STOPPED} (org.eclipse.jetty.server.handler.ContextHandler:1159)
[2025-01-03 10:09:37,026] INFO Stopped http_8083@7c8f047a{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:383)
[2025-01-03 10:09:37,026] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:149)
[2025-01-03 10:09:37,027] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:387)
[2025-01-03 10:09:37,027] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:119)
[2025-01-03 10:09:37,027] INFO [mariadb-connector|task-0] Stopping task mariadb-connector-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2025-01-03 10:09:37,045] INFO [mariadb-connector|task-0] Using 'SHOW MASTER STATUS' to get binary log status (io.debezium.connector.mysql.jdbc.MySqlConnection:41)
[2025-01-03 10:09:37,049] INFO [mariadb-connector|task-0] No previous offsets found (io.debezium.connector.common.BaseSourceTask:536)
[2025-01-03 10:09:37,069] INFO [mariadb-connector|task-0] KafkaSchemaHistory Consumer config: {key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, enable.auto.commit=false, group.id=db_-schemahistory, bootstrap.servers=localhost:9092, fetch.min.bytes=1, session.timeout.ms=10000, auto.offset.reset=earliest, client.id=db_-schemahistory} (io.debezium.storage.kafka.history.KafkaSchemaHistory:245)
[2025-01-03 10:09:37,069] INFO [mariadb-connector|task-0] KafkaSchemaHistory Producer config: {retries=1, value.serializer=org.apache.kafka.common.serialization.StringSerializer, acks=1, batch.size=32768, max.block.ms=10000, bootstrap.servers=localhost:9092, buffer.memory=1048576, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=db_-schemahistory, linger.ms=0} (io.debezium.storage.kafka.history.KafkaSchemaHistory:246)
[2025-01-03 10:09:37,069] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = db-history-config-check (io.debezium.util.Threads:270)
[2025-01-03 10:09:37,070] INFO [mariadb-connector|task-0] Idempotence will be disabled because acks is set to 1, not set to 'all'. (org.apache.kafka.clients.producer.ProducerConfig:587)
[2025-01-03 10:09:37,070] INFO [mariadb-connector|task-0] ProducerConfig values: 
	acks = 1
	auto.include.jmx.reporter = true
	batch.size = 32768
	bootstrap.servers = [localhost:9092]
	buffer.memory = 1048576
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 10:09:37,071] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:09:37,072] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:09:37,073] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:09:37,073] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735920577072 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:09:37,075] INFO [mariadb-connector|task-0] [Producer clientId=db_-schemahistory] Cluster ID: 7MkOCEgqRyytrrS9sGNpvQ (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:09:37,139] INFO [mariadb-connector|task-0] Using 'SHOW MASTER STATUS' to get binary log status (io.debezium.connector.mysql.jdbc.MySqlConnection:41)
[2025-01-03 10:09:37,145] INFO [mariadb-connector|task-0] Closing connection before starting schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:123)
[2025-01-03 10:09:37,145] INFO [mariadb-connector|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:983)
[2025-01-03 10:09:37,145] INFO [mariadb-connector|task-0] Connector started for the first time. (io.debezium.connector.common.BaseSourceTask:89)
[2025-01-03 10:09:37,148] INFO [mariadb-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = db_-schemahistory
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 10:09:37,151] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:09:37,163] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:09:37,163] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:09:37,163] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735920577163 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:09:37,166] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Cluster ID: 7MkOCEgqRyytrrS9sGNpvQ (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:09:37,167] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 10:09:37,167] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 10:09:37,169] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:09:37,169] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:09:37,169] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:09:37,169] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:09:37,170] INFO [mariadb-connector|task-0] App info kafka.consumer for db_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:09:37,170] INFO [mariadb-connector|task-0] AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:371)
[2025-01-03 10:09:37,171] INFO [mariadb-connector|task-0] These configurations '[value.serializer, acks, batch.size, max.block.ms, buffer.memory, key.serializer, linger.ms]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2025-01-03 10:09:37,171] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:09:37,171] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:09:37,171] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735920577171 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:09:37,268] INFO [mariadb-connector|task-0] Database schema history topic '(name=db.history.internal, numPartitions=1, replicationFactor=default, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807, retention.bytes=-1})' created (io.debezium.storage.kafka.history.KafkaSchemaHistory:555)
[2025-01-03 10:09:37,268] INFO [mariadb-connector|task-0] App info kafka.admin.client for db_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:09:37,269] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:09:37,269] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:09:37,269] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:09:37,269] INFO [mariadb-connector|task-0] Reconnecting after finishing schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:136)
[2025-01-03 10:09:37,298] INFO [mariadb-connector|task-0] No previous offset found (io.debezium.connector.mysql.MySqlConnectorTask:147)
[2025-01-03 10:09:37,304] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = SignalProcessor (io.debezium.util.Threads:270)
[2025-01-03 10:09:37,311] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = change-event-source-coordinator (io.debezium.util.Threads:270)
[2025-01-03 10:09:37,311] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = blocking-snapshot (io.debezium.util.Threads:270)
[2025-01-03 10:09:37,311] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-change-event-source-coordinator (io.debezium.util.Threads:287)
[2025-01-03 10:09:37,312] INFO [mariadb-connector|task-0] WorkerSourceTask{id=mariadb-connector-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.AbstractWorkerSourceTask:280)
[2025-01-03 10:09:37,312] INFO [mariadb-connector|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:434)
[2025-01-03 10:09:37,313] INFO [mariadb-connector|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:137)
[2025-01-03 10:09:37,313] INFO [mariadb-connector|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:140)
[2025-01-03 10:09:37,315] INFO [mariadb-connector|task-0] According to the connector configuration both schema and data will be snapshot. (io.debezium.relational.RelationalSnapshotChangeEventSource:282)
[2025-01-03 10:09:37,316] INFO [mariadb-connector|task-0] Snapshot step 1 - Preparing (io.debezium.relational.RelationalSnapshotChangeEventSource:135)
[2025-01-03 10:09:37,318] INFO [mariadb-connector|task-0] Snapshot step 2 - Determining captured tables (io.debezium.relational.RelationalSnapshotChangeEventSource:144)
[2025-01-03 10:09:37,318] INFO [mariadb-connector|task-0] Read list of available databases (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:116)
[2025-01-03 10:09:37,322] INFO [mariadb-connector|task-0] 	 list of available databases is: [information_schema, leafy_factory, mysql, performance_schema, sys, test] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:118)
[2025-01-03 10:09:37,322] INFO [mariadb-connector|task-0] Read list of available tables in each database (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:126)
[2025-01-03 10:09:37,393] INFO [mariadb-connector|task-0] 	snapshot continuing with database(s): [leafy_factory] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:147)
[2025-01-03 10:09:37,393] INFO [mariadb-connector|task-0] Adding table leafy_factory.jobs_machines to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:09:37,393] INFO [mariadb-connector|task-0] Adding table leafy_factory.product_cost to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:09:37,393] INFO [mariadb-connector|task-0] Adding table leafy_factory.production_lines to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:09:37,393] INFO [mariadb-connector|task-0] Adding table leafy_factory.products_raw_materials to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:09:37,393] INFO [mariadb-connector|task-0] Adding table leafy_factory.machines to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:09:37,393] INFO [mariadb-connector|task-0] Adding table leafy_factory.jobs to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:09:37,393] INFO [mariadb-connector|task-0] Adding table leafy_factory.raw_materials to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:09:37,393] INFO [mariadb-connector|task-0] Adding table leafy_factory.work_orders to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:09:37,393] INFO [mariadb-connector|task-0] Adding table leafy_factory.products to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:09:37,393] INFO [mariadb-connector|task-0] Adding table leafy_factory.factories to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:09:37,394] INFO [mariadb-connector|task-0] Created connection pool with 1 threads (io.debezium.relational.RelationalSnapshotChangeEventSource:236)
[2025-01-03 10:09:37,394] INFO [mariadb-connector|task-0] Snapshot step 3 - Locking captured tables [leafy_factory.factories, leafy_factory.jobs, leafy_factory.jobs_machines, leafy_factory.machines, leafy_factory.product_cost, leafy_factory.production_lines, leafy_factory.products, leafy_factory.products_raw_materials, leafy_factory.raw_materials, leafy_factory.work_orders] (io.debezium.relational.RelationalSnapshotChangeEventSource:153)
[2025-01-03 10:09:37,397] INFO [mariadb-connector|task-0] Flush and obtain global read lock to prevent writes to database (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:488)
[2025-01-03 10:09:37,401] INFO [mariadb-connector|task-0] Snapshot step 4 - Determining snapshot offset (io.debezium.relational.RelationalSnapshotChangeEventSource:159)
[2025-01-03 10:09:37,403] INFO [mariadb-connector|task-0] Read binlog position of MySQL primary server (io.debezium.connector.mysql.MySqlSnapshotChangeEventSource:58)
[2025-01-03 10:09:37,404] INFO [mariadb-connector|task-0] Snapshot step 5 - Reading structure of captured tables (io.debezium.relational.RelationalSnapshotChangeEventSource:162)
[2025-01-03 10:09:37,404] INFO [mariadb-connector|task-0] All eligible tables schema should be captured, capturing: [leafy_factory.factories, leafy_factory.jobs, leafy_factory.jobs_machines, leafy_factory.machines, leafy_factory.product_cost, leafy_factory.production_lines, leafy_factory.products, leafy_factory.products_raw_materials, leafy_factory.raw_materials, leafy_factory.work_orders] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:314)
[2025-01-03 10:09:37,921] ERROR [mariadb-connector|task-0] Error during snapshot (io.debezium.relational.RelationalSnapshotChangeEventSource:193)
java.lang.InterruptedException: Interrupted while emitting initial DROP TABLE events
	at io.debezium.connector.binlog.BinlogSnapshotChangeEventSource.readTableStructure(BinlogSnapshotChangeEventSource.java:327)
	at io.debezium.connector.binlog.BinlogSnapshotChangeEventSource.readTableStructure(BinlogSnapshotChangeEventSource.java:67)
	at io.debezium.relational.RelationalSnapshotChangeEventSource.doExecute(RelationalSnapshotChangeEventSource.java:163)
	at io.debezium.pipeline.source.AbstractSnapshotChangeEventSource.execute(AbstractSnapshotChangeEventSource.java:96)
	at io.debezium.pipeline.ChangeEventSourceCoordinator.doSnapshot(ChangeEventSourceCoordinator.java:297)
	at io.debezium.pipeline.ChangeEventSourceCoordinator.doSnapshot(ChangeEventSourceCoordinator.java:281)
	at io.debezium.pipeline.ChangeEventSourceCoordinator.executeChangeEventSources(ChangeEventSourceCoordinator.java:192)
	at io.debezium.pipeline.ChangeEventSourceCoordinator.lambda$start$0(ChangeEventSourceCoordinator.java:143)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:09:37,921] WARN [mariadb-connector|task-0] Snapshot was interrupted before completion (io.debezium.pipeline.source.AbstractSnapshotChangeEventSource:100)
[2025-01-03 10:09:37,921] INFO [mariadb-connector|task-0] Snapshot - Final stage (io.debezium.pipeline.source.AbstractSnapshotChangeEventSource:108)
[2025-01-03 10:09:37,921] WARN [mariadb-connector|task-0] Snapshot was not completed successfully , it will be re-executed upon connector restart (io.debezium.pipeline.source.AbstractSnapshotChangeEventSource:129)
[2025-01-03 10:09:37,922] WARN [mariadb-connector|task-0] Change event source executor was interrupted (io.debezium.pipeline.ChangeEventSourceCoordinator:147)
java.lang.InterruptedException: Interrupted while emitting initial DROP TABLE events
	at io.debezium.connector.binlog.BinlogSnapshotChangeEventSource.readTableStructure(BinlogSnapshotChangeEventSource.java:327)
	at io.debezium.connector.binlog.BinlogSnapshotChangeEventSource.readTableStructure(BinlogSnapshotChangeEventSource.java:67)
	at io.debezium.relational.RelationalSnapshotChangeEventSource.doExecute(RelationalSnapshotChangeEventSource.java:163)
	at io.debezium.pipeline.source.AbstractSnapshotChangeEventSource.execute(AbstractSnapshotChangeEventSource.java:96)
	at io.debezium.pipeline.ChangeEventSourceCoordinator.doSnapshot(ChangeEventSourceCoordinator.java:297)
	at io.debezium.pipeline.ChangeEventSourceCoordinator.doSnapshot(ChangeEventSourceCoordinator.java:281)
	at io.debezium.pipeline.ChangeEventSourceCoordinator.executeChangeEventSources(ChangeEventSourceCoordinator.java:192)
	at io.debezium.pipeline.ChangeEventSourceCoordinator.lambda$start$0(ChangeEventSourceCoordinator.java:143)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:09:37,922] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-SignalProcessor (io.debezium.util.Threads:287)
[2025-01-03 10:09:37,922] INFO [mariadb-connector|task-0] SignalProcessor stopped (io.debezium.pipeline.signal.SignalProcessor:127)
[2025-01-03 10:09:37,922] INFO [mariadb-connector|task-0] Debezium ServiceRegistry stopped. (io.debezium.service.DefaultServiceRegistry:105)
[2025-01-03 10:09:37,923] INFO [mariadb-connector|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:983)
[2025-01-03 10:09:37,923] INFO [mariadb-connector|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:983)
[2025-01-03 10:09:37,924] INFO [mariadb-connector|task-0] [Producer clientId=db_-schemahistory] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 10:09:37,925] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:09:37,925] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:09:37,925] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:09:37,925] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:09:37,925] INFO [mariadb-connector|task-0] App info kafka.producer for db_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:09:37,925] INFO [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 10:09:37,926] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:09:37,926] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:09:37,926] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:09:37,926] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:09:37,926] INFO [mariadb-connector|task-0] App info kafka.producer for connector-producer-mariadb-connector-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:09:37,928] INFO [mariadb-connector|worker] Stopping connector mariadb-connector (org.apache.kafka.connect.runtime.Worker:452)
[2025-01-03 10:09:37,928] INFO [mariadb-connector|worker] Scheduled shutdown for WorkerConnector{id=mariadb-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2025-01-03 10:09:37,928] INFO [mariadb-connector|worker] Completed shutdown for WorkerConnector{id=mariadb-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2025-01-03 10:09:37,928] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:250)
[2025-01-03 10:09:37,928] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:72)
[2025-01-03 10:09:37,928] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:09:37,928] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:09:37,928] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:09:37,928] INFO App info kafka.connect for 192.168.1.5:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:09:37,928] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:271)
[2025-01-03 10:09:37,929] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:130)
[2025-01-03 10:09:37,929] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:92)
[2025-01-03 10:11:41,548] INFO Kafka Connect worker initializing ... (org.apache.kafka.connect.cli.AbstractConnectCli:114)
[2025-01-03 10:11:41,550] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote=true, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 23, 23+37-2369
	jvm.classpath = /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/activation-1.1.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/aopalliance-repackaged-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/argparse4j-0.7.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/audience-annotations-0.12.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/caffeine-2.9.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-beanutils-1.9.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-cli-1.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-collections-3.2.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-digester-2.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-io-2.14.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-lang3-3.12.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-logging-1.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-validator-1.7.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-basic-auth-extension-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-json-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-mirror-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-mirror-client-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-runtime-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-transforms-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/error_prone_annotations-2.10.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-api-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-locator-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-utils-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-annotations-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-core-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-databind-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-dataformat-csv-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-datatype-jdk8-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-jaxrs-base-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-jaxrs-json-provider-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-afterburner-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-jaxb-annotations-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-scala_2.13-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.activation-api-1.2.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.annotation-api-1.3.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.inject-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.validation-api-2.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.xml.bind-api-2.3.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javassist-3.29.2-GA.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.activation-api-1.2.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.annotation-api-1.3.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.servlet-api-3.1.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.ws.rs-api-2.1.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jaxb-api-2.3.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-client-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-common-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-container-servlet-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-container-servlet-core-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-hk2-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-server-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-client-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-continuation-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-http-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-io-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-security-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-server-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-servlet-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-servlets-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-util-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-util-ajax-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jline-3.25.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jopt-simple-5.0.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jose4j-0.9.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jsr305-3.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-clients-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-group-coordinator-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-group-coordinator-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-metadata-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-raft-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-server-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-server-common-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-shell-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-storage-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-storage-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-examples-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-scala_2.13-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-test-utils-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-tools-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-tools-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-transaction-coordinator-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka_2.13-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/lz4-java-1.8.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/maven-artifact-3.9.6.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/metrics-core-2.2.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/metrics-core-4.1.12.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-buffer-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-codec-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-common-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-handler-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-resolver-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-classes-epoll-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-native-epoll-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-native-unix-common-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/opentelemetry-proto-1.0.0-alpha.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/osgi-resource-locator-1.0.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/paranamer-2.8.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/pcollections-4.0.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/plexus-utils-3.5.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/protobuf-java-3.25.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/reflections-0.10.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/reload4j-1.2.25.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/rocksdbjni-7.9.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-collection-compat_2.13-2.10.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-java8-compat_2.13-1.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-library-2.13.14.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-logging_2.13-3.9.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-reflect-2.13.14.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/slf4j-api-1.7.36.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/slf4j-reload4j-1.7.36.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/snappy-java-1.1.10.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/swagger-annotations-2.2.8.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/trogdor-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zookeeper-3.8.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zookeeper-jute-3.8.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zstd-jni-1.5.6-4.jar
	os.spec = Mac OS X, aarch64, 15.0
	os.vcpus = 11
 (org.apache.kafka.connect.runtime.WorkerInfo:72)
[2025-01-03 10:11:41,551] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.AbstractConnectCli:120)
[2025-01-03 10:11:41,561] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:11:41,589] INFO Using up-to-date JsonConverter implementation (io.debezium.converters.CloudEventsConverter:120)
[2025-01-03 10:11:41,665] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:11:41,666] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:11:41,678] INFO Using up-to-date JsonConverter implementation (io.debezium.converters.CloudEventsConverter:120)
[2025-01-03 10:11:41,691] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:11:41,694] INFO Loading plugin from: classpath (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:11:41,696] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@5a07e868 (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:11:41,696] INFO Scanning plugins with ServiceLoaderScanner took 136 ms (org.apache.kafka.connect.runtime.isolation.PluginScanner:71)
[2025-01-03 10:11:41,697] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:11:41,837] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:11:41,837] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:11:41,963] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:11:41,964] INFO Loading plugin from: classpath (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:11:42,439] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@5a07e868 (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:11:42,439] INFO Scanning plugins with ReflectionScanner took 742 ms (org.apache.kafka.connect.runtime.isolation.PluginScanner:71)
[2025-01-03 10:11:42,440] WARN One or more plugins are missing ServiceLoader manifests may not be usable with plugin.discovery=service_load: [
file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/	io.debezium.connector.mongodb.MongoDbSinkConnector	sink	3.0.5.Final
]
Read the documentation at https://kafka.apache.org/documentation.html#connect_plugindiscovery for instructions on migrating your plugins to take advantage of the performance improvements of service_load mode. To silence this warning, set plugin.discovery=only_scan in the worker config. (org.apache.kafka.connect.runtime.isolation.Plugins:123)
[2025-01-03 10:11:42,440] INFO Added plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,440] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,440] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,441] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,441] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,441] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,441] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,441] INFO Added plugin 'io.debezium.transforms.HeaderToValue' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,441] INFO Added plugin 'io.debezium.transforms.SchemaChangeEventFilter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,441] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,441] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,441] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,441] INFO Added plugin 'io.debezium.transforms.ExtractSchemaToNewRecord' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,441] INFO Added plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,441] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,441] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,441] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,441] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,441] INFO Added plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,441] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,441] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,441] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,441] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,441] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,441] INFO Added plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,441] INFO Added plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,441] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,441] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'io.debezium.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'io.debezium.transforms.partitions.PartitionRouting' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'io.debezium.transforms.TimezoneConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'io.debezium.transforms.ExtractChangedRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'io.debezium.connector.mongodb.transforms.outbox.MongoEventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,442] INFO Added plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,443] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,443] INFO Added plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,443] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,443] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,443] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,443] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,443] INFO Added plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,443] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,443] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:11:42,443] INFO Added alias 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,443] INFO Added alias 'ExtractNewDocumentState' to plugin 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,443] INFO Added alias 'JsonConverter' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,443] INFO Added alias 'DebeziumMySql' to plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,443] INFO Added alias 'CloudEventsConverter' to plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,443] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,444] INFO Added alias 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,444] INFO Added alias 'DebeziumMySqlConnectRestExtension' to plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,444] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,444] INFO Added alias 'MySql' to plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,444] INFO Added alias 'MongoDbSinkConnector' to plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,444] INFO Added alias 'EnvVar' to plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,444] INFO Added alias 'EnvVarConfigProvider' to plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,444] INFO Added alias 'MongoDb' to plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,444] INFO Added alias 'MirrorCheckpointConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,444] INFO Added alias 'Boolean' to plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,444] INFO Added alias 'MySqlConnector' to plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,444] INFO Added alias 'NoneConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,444] INFO Added alias 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,444] INFO Added alias 'HeaderToValue' to plugin 'io.debezium.transforms.HeaderToValue' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,444] INFO Added alias 'PartitionRouting' to plugin 'io.debezium.transforms.partitions.PartitionRouting' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,444] INFO Added alias 'MongoDbSink' to plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,444] INFO Added alias 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,444] INFO Added alias 'StringConverter' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,444] INFO Added alias 'ByLogicalTableRouter' to plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,444] INFO Added alias 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,444] INFO Added alias 'MongoDbConnector' to plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,444] INFO Added alias 'FileConfigProvider' to plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,444] INFO Added alias 'IntegerConverter' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,444] INFO Added alias 'EventRouter' to plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,444] INFO Added alias 'SchemaChangeEventFilter' to plugin 'io.debezium.transforms.SchemaChangeEventFilter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,444] INFO Added alias 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,444] INFO Added alias 'LongConverter' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,444] INFO Added alias 'DropHeaders' to plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,444] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,444] INFO Added alias 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,444] INFO Added alias 'ExtractSchemaToNewRecord' to plugin 'io.debezium.transforms.ExtractSchemaToNewRecord' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'BinaryData' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'File' to plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'FloatConverter' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'DirectoryConfigProvider' to plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'ReadToInsertEvent' to plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'ShortConverter' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'CloudEvents' to plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'ActivateTracingSpan' to plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'TimezoneConverter' to plugin 'io.debezium.transforms.TimezoneConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'DoubleConverter' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'Simple' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'BinaryDataConverter' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'AllConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'ExtractNewRecordState' to plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'DebeziumMongoDb' to plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'MirrorHeartbeatConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'InsertHeader' to plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'Directory' to plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'MirrorSourceConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'PrincipalConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'DebeziumMongoDbConnectRestExtension' to plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'BooleanConverter' to plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'MongoEventRouter' to plugin 'io.debezium.connector.mongodb.transforms.outbox.MongoEventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'ExtractChangedRecordState' to plugin 'io.debezium.transforms.ExtractChangedRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,445] INFO Added alias 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,446] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,446] INFO Added alias 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:11:42,448] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	config.providers = []
	connector.client.config.override.policy = All
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = [http://:8083]
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.discovery = hybrid_warn
	plugin.path = [/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins]
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:371)
[2025-01-03 10:11:42,449] INFO Creating Kafka admin client (org.apache.kafka.connect.runtime.WorkerConfig:281)
[2025-01-03 10:11:42,450] INFO AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:371)
[2025-01-03 10:11:42,469] INFO These configurations '[offset.flush.interval.ms, key.converter.schemas.enable, offset.storage.file.filename, value.converter.schemas.enable, plugin.path, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2025-01-03 10:11:42,470] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:11:42,470] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:11:42,470] INFO Kafka startTimeMs: 1735920702470 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:11:42,579] INFO Kafka cluster ID: 7MkOCEgqRyytrrS9sGNpvQ (org.apache.kafka.connect.runtime.WorkerConfig:298)
[2025-01-03 10:11:42,580] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:11:42,582] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:11:42,582] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:11:42,582] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:11:42,584] INFO PublicConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	listeners = [http://:8083]
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
 (org.apache.kafka.connect.runtime.rest.RestServerConfig$PublicConfig:371)
[2025-01-03 10:11:42,588] INFO Logging initialized @1294ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:170)
[2025-01-03 10:11:42,603] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:125)
[2025-01-03 10:11:42,603] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:196)
[2025-01-03 10:11:42,613] INFO jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 23+37-2369 (org.eclipse.jetty.server.Server:375)
[2025-01-03 10:11:42,621] INFO Started http_8083@133aacbe{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:333)
[2025-01-03 10:11:42,621] INFO Started @1327ms (org.eclipse.jetty.server.Server:415)
[2025-01-03 10:11:42,627] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 10:11:42,627] INFO REST server listening at http://192.168.1.5:8083/, advertising URL http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:216)
[2025-01-03 10:11:42,627] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 10:11:42,628] INFO REST admin endpoints at http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2025-01-03 10:11:42,628] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 10:11:42,628] INFO Setting up All Policy for ConnectorClientConfigOverride. This will allow all client configurations to be overridden (org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy:45)
[2025-01-03 10:11:42,628] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:11:42,632] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:11:42,632] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:11:42,632] INFO Kafka startTimeMs: 1735920702632 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:11:42,634] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:11:42,634] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:11:42,637] INFO Kafka Connect worker initialization took 1088ms (org.apache.kafka.connect.cli.AbstractConnectCli:141)
[2025-01-03 10:11:42,637] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:67)
[2025-01-03 10:11:42,638] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:112)
[2025-01-03 10:11:42,638] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:233)
[2025-01-03 10:11:42,638] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:64)
[2025-01-03 10:11:42,639] INFO Worker started (org.apache.kafka.connect.runtime.Worker:243)
[2025-01-03 10:11:42,639] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:114)
[2025-01-03 10:11:42,639] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2025-01-03 10:11:42,649] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:238)
[2025-01-03 10:11:42,667] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:334)
[2025-01-03 10:11:42,667] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:339)
[2025-01-03 10:11:42,667] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:132)
[2025-01-03 10:11:42,794] INFO Started o.e.j.s.ServletContextHandler@2bb717d7{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:921)
[2025-01-03 10:11:42,795] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:303)
[2025-01-03 10:11:42,796] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:77)
[2025-01-03 10:11:42,817] INFO Loading the custom source info struct maker plugin: io.debezium.connector.mysql.MySqlSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2025-01-03 10:11:43,217] INFO Using 'SHOW MASTER STATUS' to get binary log status (io.debezium.connector.mysql.jdbc.MySqlConnection:41)
[2025-01-03 10:11:43,221] INFO Successfully tested connection for jdbc:mysql://localhost:3306/?useInformationSchema=true&nullCatalogMeansCurrent=false&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8&zeroDateTimeBehavior=CONVERT_TO_NULL&connectTimeout=30000 with user 'root' (io.debezium.connector.binlog.BinlogConnector:66)
[2025-01-03 10:11:43,222] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection:983)
[2025-01-03 10:11:43,223] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:371)
[2025-01-03 10:11:43,226] INFO [mariadb-connector|worker] Creating connector mariadb-connector of type io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:313)
[2025-01-03 10:11:43,227] INFO [mariadb-connector|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2025-01-03 10:11:43,227] INFO [mariadb-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 10:11:43,228] INFO [mariadb-connector|worker] Instantiated connector mariadb-connector with version 3.0.5.Final of type class io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:335)
[2025-01-03 10:11:43,229] INFO [mariadb-connector|worker] Finished creating connector mariadb-connector (org.apache.kafka.connect.runtime.Worker:356)
[2025-01-03 10:11:43,229] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2025-01-03 10:11:43,229] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 10:11:43,231] INFO [mariadb-connector|task-0] Creating task mariadb-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2025-01-03 10:11:43,232] INFO [mariadb-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mariadb-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2025-01-03 10:11:43,232] INFO [mariadb-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mariadb-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 10:11:43,232] INFO [mariadb-connector|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.mysql.MySqlConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2025-01-03 10:11:43,233] INFO [mariadb-connector|task-0] Instantiated task mariadb-connector-0 with version 3.0.5.Final of type io.debezium.connector.mysql.MySqlConnectorTask (org.apache.kafka.connect.runtime.Worker:665)
[2025-01-03 10:11:43,233] INFO [mariadb-connector|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:11:43,233] INFO [mariadb-connector|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mariadb-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:678)
[2025-01-03 10:11:43,233] INFO [mariadb-connector|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:11:43,233] INFO [mariadb-connector|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mariadb-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:684)
[2025-01-03 10:11:43,234] INFO [mariadb-connector|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mariadb-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2025-01-03 10:11:43,235] INFO [mariadb-connector|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:1795)
[2025-01-03 10:11:43,235] INFO [mariadb-connector|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2025-01-03 10:11:43,235] INFO [mariadb-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 10:11:43,236] INFO [mariadb-connector|task-0] ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-mariadb-connector-0
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 10:11:43,244] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:11:43,251] INFO [mariadb-connector|task-0] These configurations '[metrics.context.connect.kafka.cluster.id]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 10:11:43,251] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:11:43,251] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:11:43,251] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735920703251 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:11:43,255] INFO [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] Cluster ID: 7MkOCEgqRyytrrS9sGNpvQ (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:11:43,256] INFO Created connector mariadb-connector (org.apache.kafka.connect.cli.ConnectStandalone:89)
[2025-01-03 10:11:43,257] INFO [mariadb-connector|task-0] Starting MySqlConnectorTask with configuration:
   connector.class = io.debezium.connector.mysql.MySqlConnector
   database.user = root
   database.server.id = 184054
   database.history.kafka.bootstrap.servers = localhost:9092
   database.history.kafka.topic = db.history.leafy_factory
   database.server.name = leafy_factory
   schema.history.internal.kafka.bootstrap.servers = localhost:9092
   database.port = 3306
   include.schema.changes = false
   topic.prefix = db_
   schema.history.internal.kafka.topic = db.history.internal
   task.class = io.debezium.connector.mysql.MySqlConnectorTask
   database.hostname = localhost
   database.password = ********
   name = mariadb-connector
   database.include.list = leafy_factory
 (io.debezium.connector.common.BaseSourceTask:250)
[2025-01-03 10:11:43,257] INFO [mariadb-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mysql.MySqlSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2025-01-03 10:11:43,257] INFO [mariadb-connector|task-0] Loading the custom topic naming strategy plugin: io.debezium.schema.DefaultTopicNamingStrategy (io.debezium.config.CommonConnectorConfig:1401)
[2025-01-03 10:11:43,290] INFO [mariadb-connector|task-0] Using 'SHOW MASTER STATUS' to get binary log status (io.debezium.connector.mysql.jdbc.MySqlConnection:41)
[2025-01-03 10:11:43,294] INFO [mariadb-connector|task-0] No previous offsets found (io.debezium.connector.common.BaseSourceTask:536)
[2025-01-03 10:11:43,314] INFO [mariadb-connector|task-0] KafkaSchemaHistory Consumer config: {key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, enable.auto.commit=false, group.id=db_-schemahistory, bootstrap.servers=localhost:9092, fetch.min.bytes=1, session.timeout.ms=10000, auto.offset.reset=earliest, client.id=db_-schemahistory} (io.debezium.storage.kafka.history.KafkaSchemaHistory:245)
[2025-01-03 10:11:43,314] INFO [mariadb-connector|task-0] KafkaSchemaHistory Producer config: {retries=1, value.serializer=org.apache.kafka.common.serialization.StringSerializer, acks=1, batch.size=32768, max.block.ms=10000, bootstrap.servers=localhost:9092, buffer.memory=1048576, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=db_-schemahistory, linger.ms=0} (io.debezium.storage.kafka.history.KafkaSchemaHistory:246)
[2025-01-03 10:11:43,314] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = db-history-config-check (io.debezium.util.Threads:270)
[2025-01-03 10:11:43,315] INFO [mariadb-connector|task-0] Idempotence will be disabled because acks is set to 1, not set to 'all'. (org.apache.kafka.clients.producer.ProducerConfig:587)
[2025-01-03 10:11:43,315] INFO [mariadb-connector|task-0] ProducerConfig values: 
	acks = 1
	auto.include.jmx.reporter = true
	batch.size = 32768
	bootstrap.servers = [localhost:9092]
	buffer.memory = 1048576
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 10:11:43,316] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:11:43,318] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:11:43,318] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:11:43,318] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735920703318 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:11:43,320] INFO [mariadb-connector|task-0] [Producer clientId=db_-schemahistory] Cluster ID: 7MkOCEgqRyytrrS9sGNpvQ (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:11:43,384] INFO [mariadb-connector|task-0] Using 'SHOW MASTER STATUS' to get binary log status (io.debezium.connector.mysql.jdbc.MySqlConnection:41)
[2025-01-03 10:11:43,389] INFO [mariadb-connector|task-0] Closing connection before starting schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:123)
[2025-01-03 10:11:43,390] INFO [mariadb-connector|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:983)
[2025-01-03 10:11:43,390] INFO [mariadb-connector|task-0] Connector started for the first time. (io.debezium.connector.common.BaseSourceTask:89)
[2025-01-03 10:11:43,392] INFO [mariadb-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = db_-schemahistory
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 10:11:43,396] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:11:43,408] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:11:43,408] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:11:43,408] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735920703408 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:11:43,410] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Cluster ID: 7MkOCEgqRyytrrS9sGNpvQ (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:11:43,415] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 10:11:43,415] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 10:11:43,416] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:11:43,416] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:11:43,417] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:11:43,417] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:11:43,418] INFO [mariadb-connector|task-0] App info kafka.consumer for db_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:11:43,418] INFO [mariadb-connector|task-0] Reconnecting after finishing schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:136)
[2025-01-03 10:11:43,440] INFO [mariadb-connector|task-0] No previous offset found (io.debezium.connector.mysql.MySqlConnectorTask:147)
[2025-01-03 10:11:43,446] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = SignalProcessor (io.debezium.util.Threads:270)
[2025-01-03 10:11:43,452] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = change-event-source-coordinator (io.debezium.util.Threads:270)
[2025-01-03 10:11:43,452] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = blocking-snapshot (io.debezium.util.Threads:270)
[2025-01-03 10:11:43,452] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-change-event-source-coordinator (io.debezium.util.Threads:287)
[2025-01-03 10:11:43,452] INFO [mariadb-connector|task-0] WorkerSourceTask{id=mariadb-connector-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.AbstractWorkerSourceTask:280)
[2025-01-03 10:11:43,453] INFO [mariadb-connector|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:137)
[2025-01-03 10:11:43,454] INFO [mariadb-connector|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:140)
[2025-01-03 10:11:43,456] INFO [mariadb-connector|task-0] According to the connector configuration both schema and data will be snapshot. (io.debezium.relational.RelationalSnapshotChangeEventSource:282)
[2025-01-03 10:11:43,457] INFO [mariadb-connector|task-0] Snapshot step 1 - Preparing (io.debezium.relational.RelationalSnapshotChangeEventSource:135)
[2025-01-03 10:11:43,458] INFO [mariadb-connector|task-0] Snapshot step 2 - Determining captured tables (io.debezium.relational.RelationalSnapshotChangeEventSource:144)
[2025-01-03 10:11:43,458] INFO [mariadb-connector|task-0] Read list of available databases (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:116)
[2025-01-03 10:11:43,473] INFO [mariadb-connector|task-0] 	 list of available databases is: [information_schema, leafy_factory, mysql, performance_schema, sys, test] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:118)
[2025-01-03 10:11:43,473] INFO [mariadb-connector|task-0] Read list of available tables in each database (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:126)
[2025-01-03 10:11:43,527] INFO [mariadb-connector|task-0] 	snapshot continuing with database(s): [leafy_factory] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:147)
[2025-01-03 10:11:43,527] INFO [mariadb-connector|task-0] Adding table leafy_factory.jobs_machines to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:11:43,527] INFO [mariadb-connector|task-0] Adding table leafy_factory.product_cost to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:11:43,527] INFO [mariadb-connector|task-0] Adding table leafy_factory.production_lines to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:11:43,527] INFO [mariadb-connector|task-0] Adding table leafy_factory.products_raw_materials to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:11:43,527] INFO [mariadb-connector|task-0] Adding table leafy_factory.machines to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:11:43,527] INFO [mariadb-connector|task-0] Adding table leafy_factory.jobs to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:11:43,527] INFO [mariadb-connector|task-0] Adding table leafy_factory.raw_materials to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:11:43,527] INFO [mariadb-connector|task-0] Adding table leafy_factory.work_orders to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:11:43,527] INFO [mariadb-connector|task-0] Adding table leafy_factory.products to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:11:43,527] INFO [mariadb-connector|task-0] Adding table leafy_factory.factories to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:11:43,528] INFO [mariadb-connector|task-0] Created connection pool with 1 threads (io.debezium.relational.RelationalSnapshotChangeEventSource:236)
[2025-01-03 10:11:43,528] INFO [mariadb-connector|task-0] Snapshot step 3 - Locking captured tables [leafy_factory.factories, leafy_factory.jobs, leafy_factory.jobs_machines, leafy_factory.machines, leafy_factory.product_cost, leafy_factory.production_lines, leafy_factory.products, leafy_factory.products_raw_materials, leafy_factory.raw_materials, leafy_factory.work_orders] (io.debezium.relational.RelationalSnapshotChangeEventSource:153)
[2025-01-03 10:11:43,530] INFO [mariadb-connector|task-0] Flush and obtain global read lock to prevent writes to database (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:488)
[2025-01-03 10:11:43,530] INFO [mariadb-connector|task-0] Snapshot step 4 - Determining snapshot offset (io.debezium.relational.RelationalSnapshotChangeEventSource:159)
[2025-01-03 10:11:43,532] INFO [mariadb-connector|task-0] Read binlog position of MySQL primary server (io.debezium.connector.mysql.MySqlSnapshotChangeEventSource:58)
[2025-01-03 10:11:43,533] INFO [mariadb-connector|task-0] Snapshot step 5 - Reading structure of captured tables (io.debezium.relational.RelationalSnapshotChangeEventSource:162)
[2025-01-03 10:11:43,533] INFO [mariadb-connector|task-0] All eligible tables schema should be captured, capturing: [leafy_factory.factories, leafy_factory.jobs, leafy_factory.jobs_machines, leafy_factory.machines, leafy_factory.product_cost, leafy_factory.production_lines, leafy_factory.products, leafy_factory.products_raw_materials, leafy_factory.raw_materials, leafy_factory.work_orders] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:314)
[2025-01-03 10:11:44,038] INFO [mariadb-connector|task-0] Reading structure of database 'leafy_factory' (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:348)
[2025-01-03 10:11:44,108] INFO [mariadb-connector|task-0] Snapshot step 6 - Persisting schema history (io.debezium.relational.RelationalSnapshotChangeEventSource:166)
[2025-01-03 10:11:44,183] INFO [mariadb-connector|task-0] Releasing global read lock to enable MySQL writes (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:497)
[2025-01-03 10:11:44,185] INFO [mariadb-connector|task-0] Writes to MySQL tables prevented for a total of 00:00:00.655 (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:501)
[2025-01-03 10:11:44,185] INFO [mariadb-connector|task-0] Snapshot step 7 - Snapshotting data (io.debezium.relational.RelationalSnapshotChangeEventSource:178)
[2025-01-03 10:11:44,186] INFO [mariadb-connector|task-0] Creating snapshot worker pool with 1 worker thread(s) (io.debezium.relational.RelationalSnapshotChangeEventSource:480)
[2025-01-03 10:11:44,187] INFO [mariadb-connector|task-0] For table 'leafy_factory.factories' using select statement: 'SELECT `id_factory`, `factory_name`, `factory_location`, `factory_timestamp` FROM `leafy_factory`.`factories`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:11:44,189] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.factories is OptionalLong[0] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:11:44,190] INFO [mariadb-connector|task-0] For table 'leafy_factory.jobs' using select statement: 'SELECT `id_job`, `target_output`, `nOk_products`, `quality_rate`, `job_status`, `creation_date`, `work_id` FROM `leafy_factory`.`jobs`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:11:44,191] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.jobs is OptionalLong[0] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:11:44,191] INFO [mariadb-connector|task-0] For table 'leafy_factory.jobs_machines' using select statement: 'SELECT `id_jobs_machines`, `job_id`, `machine_id` FROM `leafy_factory`.`jobs_machines`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:11:44,192] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.jobs_machines is OptionalLong[2] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:11:44,192] INFO [mariadb-connector|task-0] For table 'leafy_factory.machines' using select statement: 'SELECT `id_machine`, `machine_status`, `last_maintenance`, `operator`, `avg_output`, `reject_count`, `production_line_id` FROM `leafy_factory`.`machines`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:11:44,193] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.machines is OptionalLong[4] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:11:44,193] INFO [mariadb-connector|task-0] For table 'leafy_factory.product_cost' using select statement: 'SELECT `id_cost`, `raw_material_cost_per_product`, `overhead_per_product`, `total_cost_per_product`, `cost_ok_with_overhead`, `cost_nok_with_overhead`, `actual_total_cost`, `work_id` FROM `leafy_factory`.`product_cost`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:11:44,194] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.product_cost is OptionalLong[12] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:11:44,194] INFO [mariadb-connector|task-0] For table 'leafy_factory.production_lines' using select statement: 'SELECT `id_production_line`, `factory_id` FROM `leafy_factory`.`production_lines`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:11:44,195] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.production_lines is OptionalLong[2] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:11:44,195] INFO [mariadb-connector|task-0] For table 'leafy_factory.products' using select statement: 'SELECT `id_product`, `product_name`, `product_description` FROM `leafy_factory`.`products`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:11:44,196] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.products is OptionalLong[2] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:11:44,196] INFO [mariadb-connector|task-0] For table 'leafy_factory.products_raw_materials' using select statement: 'SELECT `id_products_raw_materials`, `product_id`, `raw_material_id` FROM `leafy_factory`.`products_raw_materials`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:11:44,197] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.products_raw_materials is OptionalLong[9] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:11:44,197] INFO [mariadb-connector|task-0] For table 'leafy_factory.raw_materials' using select statement: 'SELECT `id_raw_material`, `item_code`, `raw_material_name`, `raw_material_description`, `unit_measurement`, `raw_material_stock`, `raw_material_status`, `raw_material_currency`, `cost_per_part` FROM `leafy_factory`.`raw_materials`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:11:44,198] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.raw_materials is OptionalLong[8] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:11:44,198] INFO [mariadb-connector|task-0] For table 'leafy_factory.work_orders' using select statement: 'SELECT `id_work`, `planned_start_date`, `planned_end_date`, `actual_start_date`, `actual_end_date`, `quantity`, `wo_status`, `creation_date`, `product_id`, `nOk_products` FROM `leafy_factory`.`work_orders`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:11:44,200] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.work_orders is OptionalLong[12] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:11:44,201] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.factories' (1 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:11:44,214] INFO [mariadb-connector|task-0] 	 Finished exporting 1 records for table 'leafy_factory.factories' (1 of 10 tables); total duration '00:00:00.013' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:11:44,215] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.jobs' (2 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:11:44,216] INFO [mariadb-connector|task-0] 	 Finished exporting 1 records for table 'leafy_factory.jobs' (2 of 10 tables); total duration '00:00:00.001' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:11:44,216] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.jobs_machines' (3 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:11:44,218] INFO [mariadb-connector|task-0] 	 Finished exporting 2 records for table 'leafy_factory.jobs_machines' (3 of 10 tables); total duration '00:00:00.002' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:11:44,219] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.machines' (4 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:11:44,222] INFO [mariadb-connector|task-0] 	 Finished exporting 4 records for table 'leafy_factory.machines' (4 of 10 tables); total duration '00:00:00.003' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:11:44,223] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.product_cost' (5 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:11:44,225] INFO [mariadb-connector|task-0] 	 Finished exporting 20 records for table 'leafy_factory.product_cost' (5 of 10 tables); total duration '00:00:00.002' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:11:44,226] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.production_lines' (6 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:11:44,227] INFO [mariadb-connector|task-0] 	 Finished exporting 2 records for table 'leafy_factory.production_lines' (6 of 10 tables); total duration '00:00:00.001' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:11:44,227] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.products' (7 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:11:44,228] INFO [mariadb-connector|task-0] 	 Finished exporting 2 records for table 'leafy_factory.products' (7 of 10 tables); total duration '00:00:00.001' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:11:44,229] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.products_raw_materials' (8 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:11:44,230] INFO [mariadb-connector|task-0] 	 Finished exporting 8 records for table 'leafy_factory.products_raw_materials' (8 of 10 tables); total duration '00:00:00.001' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:11:44,230] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.raw_materials' (9 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:11:44,231] INFO [mariadb-connector|task-0] 	 Finished exporting 8 records for table 'leafy_factory.raw_materials' (9 of 10 tables); total duration '00:00:00.001' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:11:44,232] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.work_orders' (10 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:11:44,236] INFO [mariadb-connector|task-0] 	 Finished exporting 20 records for table 'leafy_factory.work_orders' (10 of 10 tables); total duration '00:00:00.004' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:11:44,238] INFO [mariadb-connector|task-0] Snapshot - Final stage (io.debezium.pipeline.source.AbstractSnapshotChangeEventSource:108)
[2025-01-03 10:11:44,238] INFO [mariadb-connector|task-0] Snapshot completed (io.debezium.pipeline.source.AbstractSnapshotChangeEventSource:112)
[2025-01-03 10:11:44,244] INFO [mariadb-connector|task-0] Snapshot ended with SnapshotResult [status=COMPLETED, offset=BinlogOffsetContext{sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=BinlogSourceInfo{currentGtid='null', currentBinlogFilename='mariadb-bin.000001', currentBinlogPosition=49121, currentRowNumber=0, serverId=0, sourceTime=2025-01-03T16:11:44Z, threadId=-1, currentQuery='null', tableIds=[leafy_factory.work_orders], databaseName='leafy_factory'}, snapshotCompleted=true, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet='null', currentGtidSet='null', restartBinlogFilename='mariadb-bin.000001', restartBinlogPosition=49121, restartRowsToSkip=0, restartEventsToSkip=0, currentEventLengthInBytes=0, inTransaction=false, transactionId='null', incrementalSnapshotContext=IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]}] (io.debezium.pipeline.ChangeEventSourceCoordinator:298)
[2025-01-03 10:11:44,246] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = binlog-client (io.debezium.util.Threads:270)
[2025-01-03 10:11:44,248] INFO [mariadb-connector|task-0] Enable ssl PREFERRED mode for connector db_ (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:1289)
[2025-01-03 10:11:44,253] INFO [mariadb-connector|task-0] SignalProcessor started. Scheduling it every 5000ms (io.debezium.pipeline.signal.SignalProcessor:105)
[2025-01-03 10:11:44,253] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-SignalProcessor (io.debezium.util.Threads:287)
[2025-01-03 10:11:44,253] INFO [mariadb-connector|task-0] Starting streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:323)
[2025-01-03 10:11:44,254] INFO [mariadb-connector|task-0] Skip 0 events on streaming start (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:278)
[2025-01-03 10:11:44,254] INFO [mariadb-connector|task-0] Skip 0 rows on streaming start (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:282)
[2025-01-03 10:11:44,254] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-binlog-client (io.debezium.util.Threads:287)
[2025-01-03 10:11:44,255] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-binlog-client (io.debezium.util.Threads:287)
[2025-01-03 10:11:44,290] INFO [mariadb-connector|task-0] Connected to binlog at localhost:3306, starting at BinlogOffsetContext{sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=BinlogSourceInfo{currentGtid='null', currentBinlogFilename='mariadb-bin.000001', currentBinlogPosition=49121, currentRowNumber=0, serverId=0, sourceTime=2025-01-03T16:11:44Z, threadId=-1, currentQuery='null', tableIds=[leafy_factory.work_orders], databaseName='leafy_factory'}, snapshotCompleted=true, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet='null', currentGtidSet='null', restartBinlogFilename='mariadb-bin.000001', restartBinlogPosition=49121, restartRowsToSkip=0, restartEventsToSkip=0, currentEventLengthInBytes=0, inTransaction=false, transactionId='null', incrementalSnapshotContext=IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]} (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:1232)
[2025-01-03 10:11:44,291] INFO [mariadb-connector|task-0] Waiting for keepalive thread to start (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:299)
[2025-01-03 10:11:44,291] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-binlog-client (io.debezium.util.Threads:287)
[2025-01-03 10:11:44,396] INFO [mariadb-connector|task-0] Keepalive thread is running (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:306)
[2025-01-03 10:11:44,502] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 4 : {db_.leafy_factory.factories=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:11:44,633] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 7 : {db_.leafy_factory.jobs=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:11:44,775] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 10 : {db_.leafy_factory.jobs_machines=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:11:44,917] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 13 : {db_.leafy_factory.machines=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:11:45,065] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 17 : {db_.leafy_factory.product_cost=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:11:45,204] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 23 : {db_.leafy_factory.production_lines=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:11:45,345] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 31 : {db_.leafy_factory.products=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:11:45,481] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 35 : {db_.leafy_factory.products_raw_materials=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:11:45,605] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 39 : {db_.leafy_factory.raw_materials=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:11:45,745] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 44 : {db_.leafy_factory.work_orders=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:11:53,261] INFO [mariadb-connector|task-0|offsets] WorkerSourceTask{id=mariadb-connector-0} Committing offsets for 68 acknowledged messages (org.apache.kafka.connect.runtime.WorkerSourceTask:236)
[2025-01-03 10:12:15,755] INFO Kafka Connect worker initializing ... (org.apache.kafka.connect.cli.AbstractConnectCli:114)
[2025-01-03 10:12:15,757] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote=true, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 23, 23+37-2369
	jvm.classpath = /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/activation-1.1.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/aopalliance-repackaged-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/argparse4j-0.7.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/audience-annotations-0.12.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/caffeine-2.9.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-beanutils-1.9.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-cli-1.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-collections-3.2.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-digester-2.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-io-2.14.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-lang3-3.12.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-logging-1.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-validator-1.7.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-basic-auth-extension-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-json-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-mirror-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-mirror-client-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-runtime-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-transforms-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/error_prone_annotations-2.10.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-api-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-locator-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-utils-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-annotations-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-core-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-databind-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-dataformat-csv-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-datatype-jdk8-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-jaxrs-base-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-jaxrs-json-provider-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-afterburner-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-jaxb-annotations-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-scala_2.13-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.activation-api-1.2.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.annotation-api-1.3.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.inject-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.validation-api-2.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.xml.bind-api-2.3.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javassist-3.29.2-GA.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.activation-api-1.2.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.annotation-api-1.3.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.servlet-api-3.1.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.ws.rs-api-2.1.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jaxb-api-2.3.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-client-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-common-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-container-servlet-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-container-servlet-core-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-hk2-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-server-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-client-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-continuation-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-http-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-io-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-security-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-server-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-servlet-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-servlets-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-util-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-util-ajax-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jline-3.25.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jopt-simple-5.0.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jose4j-0.9.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jsr305-3.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-clients-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-group-coordinator-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-group-coordinator-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-metadata-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-raft-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-server-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-server-common-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-shell-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-storage-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-storage-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-examples-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-scala_2.13-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-test-utils-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-tools-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-tools-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-transaction-coordinator-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka_2.13-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/lz4-java-1.8.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/maven-artifact-3.9.6.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/metrics-core-2.2.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/metrics-core-4.1.12.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-buffer-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-codec-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-common-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-handler-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-resolver-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-classes-epoll-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-native-epoll-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-native-unix-common-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/opentelemetry-proto-1.0.0-alpha.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/osgi-resource-locator-1.0.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/paranamer-2.8.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/pcollections-4.0.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/plexus-utils-3.5.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/protobuf-java-3.25.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/reflections-0.10.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/reload4j-1.2.25.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/rocksdbjni-7.9.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-collection-compat_2.13-2.10.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-java8-compat_2.13-1.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-library-2.13.14.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-logging_2.13-3.9.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-reflect-2.13.14.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/slf4j-api-1.7.36.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/slf4j-reload4j-1.7.36.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/snappy-java-1.1.10.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/swagger-annotations-2.2.8.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/trogdor-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zookeeper-3.8.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zookeeper-jute-3.8.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zstd-jni-1.5.6-4.jar
	os.spec = Mac OS X, aarch64, 15.0
	os.vcpus = 11
 (org.apache.kafka.connect.runtime.WorkerInfo:72)
[2025-01-03 10:12:15,758] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.AbstractConnectCli:120)
[2025-01-03 10:12:15,767] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:12:15,793] INFO Using up-to-date JsonConverter implementation (io.debezium.converters.CloudEventsConverter:120)
[2025-01-03 10:12:15,870] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:12:15,871] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:12:15,878] INFO Using up-to-date JsonConverter implementation (io.debezium.converters.CloudEventsConverter:120)
[2025-01-03 10:12:15,891] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:12:15,893] INFO Loading plugin from: classpath (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:12:15,895] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@5a07e868 (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:12:15,895] INFO Scanning plugins with ServiceLoaderScanner took 128 ms (org.apache.kafka.connect.runtime.isolation.PluginScanner:71)
[2025-01-03 10:12:15,896] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:12:16,024] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:12:16,024] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:12:16,161] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:12:16,161] INFO Loading plugin from: classpath (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:12:16,621] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@5a07e868 (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:12:16,621] INFO Scanning plugins with ReflectionScanner took 725 ms (org.apache.kafka.connect.runtime.isolation.PluginScanner:71)
[2025-01-03 10:12:16,622] WARN One or more plugins are missing ServiceLoader manifests may not be usable with plugin.discovery=service_load: [
file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/	io.debezium.connector.mongodb.MongoDbSinkConnector	sink	3.0.5.Final
]
Read the documentation at https://kafka.apache.org/documentation.html#connect_plugindiscovery for instructions on migrating your plugins to take advantage of the performance improvements of service_load mode. To silence this warning, set plugin.discovery=only_scan in the worker config. (org.apache.kafka.connect.runtime.isolation.Plugins:123)
[2025-01-03 10:12:16,622] INFO Added plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,622] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,622] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,622] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,622] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,623] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,623] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,623] INFO Added plugin 'io.debezium.transforms.HeaderToValue' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,623] INFO Added plugin 'io.debezium.transforms.SchemaChangeEventFilter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,623] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,623] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,623] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,623] INFO Added plugin 'io.debezium.transforms.ExtractSchemaToNewRecord' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,623] INFO Added plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,623] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,623] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,623] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,623] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,623] INFO Added plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,623] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,623] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,623] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,623] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,623] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,623] INFO Added plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,623] INFO Added plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,623] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,623] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,624] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,624] INFO Added plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,624] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,624] INFO Added plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,624] INFO Added plugin 'io.debezium.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,624] INFO Added plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,624] INFO Added plugin 'io.debezium.transforms.partitions.PartitionRouting' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,624] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,624] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,624] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,624] INFO Added plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,624] INFO Added plugin 'io.debezium.transforms.TimezoneConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,624] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,624] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,624] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,624] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,624] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,624] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,624] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,624] INFO Added plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,624] INFO Added plugin 'io.debezium.transforms.ExtractChangedRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,624] INFO Added plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,624] INFO Added plugin 'io.debezium.connector.mongodb.transforms.outbox.MongoEventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,624] INFO Added plugin 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,624] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,624] INFO Added plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,625] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,625] INFO Added plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,625] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,625] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,625] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,625] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,625] INFO Added plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,625] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,625] INFO Added plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,625] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,625] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,625] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,625] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,625] INFO Added plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,625] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,625] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:12:16,626] INFO Added alias 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,626] INFO Added alias 'ExtractNewDocumentState' to plugin 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,626] INFO Added alias 'JsonConverter' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,626] INFO Added alias 'DebeziumMySql' to plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,626] INFO Added alias 'CloudEventsConverter' to plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,626] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,626] INFO Added alias 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,626] INFO Added alias 'DebeziumMySqlConnectRestExtension' to plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,626] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,626] INFO Added alias 'MySql' to plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,626] INFO Added alias 'MongoDbSinkConnector' to plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,626] INFO Added alias 'EnvVar' to plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,626] INFO Added alias 'EnvVarConfigProvider' to plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,626] INFO Added alias 'MongoDb' to plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,626] INFO Added alias 'MirrorCheckpointConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,627] INFO Added alias 'Boolean' to plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,627] INFO Added alias 'MySqlConnector' to plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,627] INFO Added alias 'NoneConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,627] INFO Added alias 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,627] INFO Added alias 'HeaderToValue' to plugin 'io.debezium.transforms.HeaderToValue' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,627] INFO Added alias 'PartitionRouting' to plugin 'io.debezium.transforms.partitions.PartitionRouting' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,627] INFO Added alias 'MongoDbSink' to plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,627] INFO Added alias 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,627] INFO Added alias 'StringConverter' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,627] INFO Added alias 'ByLogicalTableRouter' to plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,627] INFO Added alias 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,627] INFO Added alias 'MongoDbConnector' to plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,627] INFO Added alias 'FileConfigProvider' to plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,627] INFO Added alias 'IntegerConverter' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,627] INFO Added alias 'EventRouter' to plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,627] INFO Added alias 'SchemaChangeEventFilter' to plugin 'io.debezium.transforms.SchemaChangeEventFilter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,627] INFO Added alias 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,627] INFO Added alias 'LongConverter' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,627] INFO Added alias 'DropHeaders' to plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,627] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,627] INFO Added alias 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,627] INFO Added alias 'ExtractSchemaToNewRecord' to plugin 'io.debezium.transforms.ExtractSchemaToNewRecord' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,627] INFO Added alias 'BinaryData' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,627] INFO Added alias 'File' to plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,627] INFO Added alias 'FloatConverter' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,627] INFO Added alias 'DirectoryConfigProvider' to plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,627] INFO Added alias 'ReadToInsertEvent' to plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,627] INFO Added alias 'ShortConverter' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,627] INFO Added alias 'CloudEvents' to plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,627] INFO Added alias 'ActivateTracingSpan' to plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,628] INFO Added alias 'TimezoneConverter' to plugin 'io.debezium.transforms.TimezoneConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,628] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,628] INFO Added alias 'DoubleConverter' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,628] INFO Added alias 'Simple' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,628] INFO Added alias 'BinaryDataConverter' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,628] INFO Added alias 'AllConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,628] INFO Added alias 'ExtractNewRecordState' to plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,628] INFO Added alias 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,628] INFO Added alias 'DebeziumMongoDb' to plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,628] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,628] INFO Added alias 'MirrorHeartbeatConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,628] INFO Added alias 'InsertHeader' to plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,628] INFO Added alias 'Directory' to plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,628] INFO Added alias 'MirrorSourceConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,628] INFO Added alias 'PrincipalConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,628] INFO Added alias 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,628] INFO Added alias 'DebeziumMongoDbConnectRestExtension' to plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,628] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,628] INFO Added alias 'BooleanConverter' to plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,628] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,628] INFO Added alias 'MongoEventRouter' to plugin 'io.debezium.connector.mongodb.transforms.outbox.MongoEventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,628] INFO Added alias 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,628] INFO Added alias 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,628] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,628] INFO Added alias 'ExtractChangedRecordState' to plugin 'io.debezium.transforms.ExtractChangedRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,628] INFO Added alias 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,628] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,628] INFO Added alias 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:12:16,631] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	config.providers = []
	connector.client.config.override.policy = All
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = [http://:8083]
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.discovery = hybrid_warn
	plugin.path = [/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins]
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:371)
[2025-01-03 10:12:16,631] INFO Creating Kafka admin client (org.apache.kafka.connect.runtime.WorkerConfig:281)
[2025-01-03 10:12:16,633] INFO AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:371)
[2025-01-03 10:12:16,652] INFO These configurations '[offset.flush.interval.ms, key.converter.schemas.enable, offset.storage.file.filename, value.converter.schemas.enable, plugin.path, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2025-01-03 10:12:16,652] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:12:16,652] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:12:16,652] INFO Kafka startTimeMs: 1735920736652 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:12:16,761] INFO Kafka cluster ID: 7MkOCEgqRyytrrS9sGNpvQ (org.apache.kafka.connect.runtime.WorkerConfig:298)
[2025-01-03 10:12:16,761] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:12:16,763] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:12:16,763] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:12:16,763] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:12:16,765] INFO PublicConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	listeners = [http://:8083]
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
 (org.apache.kafka.connect.runtime.rest.RestServerConfig$PublicConfig:371)
[2025-01-03 10:12:16,769] INFO Logging initialized @1253ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:170)
[2025-01-03 10:12:16,781] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:125)
[2025-01-03 10:12:16,781] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:196)
[2025-01-03 10:12:16,790] INFO jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 23+37-2369 (org.eclipse.jetty.server.Server:375)
[2025-01-03 10:12:16,797] ERROR Stopping due to error (org.apache.kafka.connect.cli.AbstractConnectCli:102)
org.apache.kafka.connect.errors.ConnectException: Unable to initialize REST server
	at org.apache.kafka.connect.runtime.rest.RestServer.initializeServer(RestServer.java:213)
	at org.apache.kafka.connect.cli.AbstractConnectCli.startConnect(AbstractConnectCli.java:129)
	at org.apache.kafka.connect.cli.AbstractConnectCli.run(AbstractConnectCli.java:95)
	at org.apache.kafka.connect.cli.ConnectStandalone.main(ConnectStandalone.java:185)
Caused by: java.io.IOException: Failed to bind to 0.0.0.0/0.0.0.0:8083
	at org.eclipse.jetty.server.ServerConnector.openAcceptChannel(ServerConnector.java:349)
	at org.eclipse.jetty.server.ServerConnector.open(ServerConnector.java:310)
	at org.eclipse.jetty.server.AbstractNetworkConnector.doStart(AbstractNetworkConnector.java:80)
	at org.eclipse.jetty.server.ServerConnector.doStart(ServerConnector.java:234)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.eclipse.jetty.server.Server.doStart(Server.java:401)
	at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:73)
	at org.apache.kafka.connect.runtime.rest.RestServer.initializeServer(RestServer.java:211)
	... 3 more
Caused by: java.net.BindException: Address already in use
	at java.base/sun.nio.ch.Net.bind0(Native Method)
	at java.base/sun.nio.ch.Net.bind(Net.java:565)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.netBind(ServerSocketChannelImpl.java:344)
	at java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:301)
	at java.base/sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:89)
	at org.eclipse.jetty.server.ServerConnector.openAcceptChannel(ServerConnector.java:344)
	... 10 more
[2025-01-03 10:19:39,906] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:87)
[2025-01-03 10:19:39,908] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:358)
[2025-01-03 10:19:39,917] INFO Stopped http_8083@133aacbe{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:383)
[2025-01-03 10:19:39,918] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:149)
[2025-01-03 10:19:39,924] INFO Stopped o.e.j.s.ServletContextHandler@2bb717d7{/,null,STOPPED} (org.eclipse.jetty.server.handler.ContextHandler:1159)
[2025-01-03 10:19:39,924] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:387)
[2025-01-03 10:19:39,924] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:119)
[2025-01-03 10:19:39,925] INFO [mariadb-connector|task-0] Stopping task mariadb-connector-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2025-01-03 10:19:39,972] INFO [mariadb-connector|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:434)
[2025-01-03 10:19:40,021] INFO [mariadb-connector|task-0] Finished streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:325)
[2025-01-03 10:19:40,021] INFO [mariadb-connector|task-0] Stopped reading binlog after 11 events, last recorded offset: {ts_sec=1735920704, file=mariadb-bin.000001, pos=49121, server_id=1, event=1} (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:1218)
[2025-01-03 10:19:40,022] INFO [mariadb-connector|task-0] SignalProcessor stopped (io.debezium.pipeline.signal.SignalProcessor:127)
[2025-01-03 10:19:40,022] INFO [mariadb-connector|task-0] Debezium ServiceRegistry stopped. (io.debezium.service.DefaultServiceRegistry:105)
[2025-01-03 10:19:40,025] INFO [mariadb-connector|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:983)
[2025-01-03 10:19:40,026] INFO [mariadb-connector|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:983)
[2025-01-03 10:19:40,026] INFO [mariadb-connector|task-0] [Producer clientId=db_-schemahistory] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 10:19:40,030] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:19:40,030] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:19:40,030] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:19:40,030] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:19:40,031] INFO [mariadb-connector|task-0] App info kafka.producer for db_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:19:40,032] INFO [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 10:19:40,034] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:19:40,034] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:19:40,035] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:19:40,035] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:19:40,035] INFO [mariadb-connector|task-0] App info kafka.producer for connector-producer-mariadb-connector-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:19:40,040] INFO [mariadb-connector|worker] Stopping connector mariadb-connector (org.apache.kafka.connect.runtime.Worker:452)
[2025-01-03 10:19:40,040] INFO [mariadb-connector|worker] Scheduled shutdown for WorkerConnector{id=mariadb-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2025-01-03 10:19:40,040] INFO [mariadb-connector|worker] Completed shutdown for WorkerConnector{id=mariadb-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2025-01-03 10:19:40,040] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:250)
[2025-01-03 10:19:40,041] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:72)
[2025-01-03 10:19:40,041] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:19:40,041] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:19:40,041] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:19:40,041] INFO App info kafka.connect for 192.168.1.5:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:19:40,041] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:271)
[2025-01-03 10:19:40,042] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:130)
[2025-01-03 10:19:40,042] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:92)
[2025-01-03 10:20:37,130] INFO Kafka Connect worker initializing ... (org.apache.kafka.connect.cli.AbstractConnectCli:114)
[2025-01-03 10:20:37,132] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote=true, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 23, 23+37-2369
	jvm.classpath = /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/activation-1.1.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/aopalliance-repackaged-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/argparse4j-0.7.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/audience-annotations-0.12.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/caffeine-2.9.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-beanutils-1.9.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-cli-1.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-collections-3.2.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-digester-2.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-io-2.14.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-lang3-3.12.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-logging-1.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-validator-1.7.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-basic-auth-extension-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-json-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-mirror-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-mirror-client-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-runtime-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-transforms-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/error_prone_annotations-2.10.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-api-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-locator-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-utils-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-annotations-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-core-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-databind-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-dataformat-csv-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-datatype-jdk8-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-jaxrs-base-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-jaxrs-json-provider-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-afterburner-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-jaxb-annotations-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-scala_2.13-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.activation-api-1.2.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.annotation-api-1.3.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.inject-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.validation-api-2.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.xml.bind-api-2.3.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javassist-3.29.2-GA.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.activation-api-1.2.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.annotation-api-1.3.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.servlet-api-3.1.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.ws.rs-api-2.1.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jaxb-api-2.3.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-client-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-common-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-container-servlet-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-container-servlet-core-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-hk2-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-server-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-client-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-continuation-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-http-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-io-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-security-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-server-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-servlet-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-servlets-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-util-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-util-ajax-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jline-3.25.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jopt-simple-5.0.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jose4j-0.9.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jsr305-3.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-clients-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-group-coordinator-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-group-coordinator-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-metadata-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-raft-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-server-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-server-common-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-shell-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-storage-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-storage-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-examples-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-scala_2.13-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-test-utils-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-tools-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-tools-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-transaction-coordinator-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka_2.13-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/lz4-java-1.8.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/maven-artifact-3.9.6.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/metrics-core-2.2.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/metrics-core-4.1.12.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-buffer-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-codec-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-common-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-handler-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-resolver-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-classes-epoll-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-native-epoll-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-native-unix-common-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/opentelemetry-proto-1.0.0-alpha.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/osgi-resource-locator-1.0.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/paranamer-2.8.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/pcollections-4.0.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/plexus-utils-3.5.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/protobuf-java-3.25.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/reflections-0.10.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/reload4j-1.2.25.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/rocksdbjni-7.9.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-collection-compat_2.13-2.10.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-java8-compat_2.13-1.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-library-2.13.14.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-logging_2.13-3.9.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-reflect-2.13.14.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/slf4j-api-1.7.36.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/slf4j-reload4j-1.7.36.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/snappy-java-1.1.10.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/swagger-annotations-2.2.8.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/trogdor-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zookeeper-3.8.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zookeeper-jute-3.8.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zstd-jni-1.5.6-4.jar
	os.spec = Mac OS X, aarch64, 15.0
	os.vcpus = 11
 (org.apache.kafka.connect.runtime.WorkerInfo:72)
[2025-01-03 10:20:37,132] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.AbstractConnectCli:120)
[2025-01-03 10:20:37,140] INFO Loading plugin from: classpath (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:20:37,218] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@5a07e868 (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:20:37,219] INFO Scanning plugins with ServiceLoaderScanner took 79 ms (org.apache.kafka.connect.runtime.isolation.PluginScanner:71)
[2025-01-03 10:20:37,219] INFO Loading plugin from: classpath (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:20:37,913] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@5a07e868 (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:20:37,913] INFO Scanning plugins with ReflectionScanner took 694 ms (org.apache.kafka.connect.runtime.isolation.PluginScanner:71)
[2025-01-03 10:20:37,914] WARN All plugins have ServiceLoader manifests, consider reconfiguring plugin.discovery=service_load (org.apache.kafka.connect.runtime.isolation.Plugins:106)
[2025-01-03 10:20:37,914] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,914] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,914] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,914] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,914] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,914] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,914] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,914] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,914] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,914] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,915] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,915] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,915] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,915] INFO Added plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,915] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,915] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,915] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,915] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,915] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,915] INFO Added plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,915] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,915] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,915] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,915] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,915] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,915] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,915] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,915] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,915] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,915] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,915] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,915] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,915] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,916] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,916] INFO Added plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,916] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,916] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,916] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,916] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,916] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,916] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,916] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,916] INFO Added plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,916] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,916] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,916] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,916] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,916] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,916] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:20:37,917] INFO Added alias 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,917] INFO Added alias 'JsonConverter' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,917] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,917] INFO Added alias 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,917] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,917] INFO Added alias 'EnvVar' to plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,917] INFO Added alias 'EnvVarConfigProvider' to plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,917] INFO Added alias 'ByteArray' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,917] INFO Added alias 'MirrorCheckpointConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,917] INFO Added alias 'Boolean' to plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,917] INFO Added alias 'NoneConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,917] INFO Added alias 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,917] INFO Added alias 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,917] INFO Added alias 'StringConverter' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,917] INFO Added alias 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,917] INFO Added alias 'FileConfigProvider' to plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,917] INFO Added alias 'IntegerConverter' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,917] INFO Added alias 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,917] INFO Added alias 'LongConverter' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,917] INFO Added alias 'DropHeaders' to plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,917] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,917] INFO Added alias 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,917] INFO Added alias 'File' to plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,917] INFO Added alias 'FloatConverter' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,917] INFO Added alias 'DirectoryConfigProvider' to plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,918] INFO Added alias 'ByteArrayConverter' to plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,918] INFO Added alias 'ShortConverter' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,918] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,918] INFO Added alias 'DoubleConverter' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,918] INFO Added alias 'Simple' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,918] INFO Added alias 'AllConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,918] INFO Added alias 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,918] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,918] INFO Added alias 'MirrorHeartbeatConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,918] INFO Added alias 'InsertHeader' to plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,918] INFO Added alias 'Directory' to plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,918] INFO Added alias 'MirrorSourceConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,918] INFO Added alias 'PrincipalConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,918] INFO Added alias 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,918] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,918] INFO Added alias 'BooleanConverter' to plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,918] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,918] INFO Added alias 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,918] INFO Added alias 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,918] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,918] INFO Added alias 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,918] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,918] INFO Added alias 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:20:37,934] INFO DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	config.providers = []
	config.storage.replication.factor = 1
	config.storage.topic = connect-configs
	connect.protocol = sessioned
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = All
	exactly.once.source.support = disabled
	group.id = connect-cluster
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	heartbeat.interval.ms = 3000
	inter.worker.key.generation.algorithm = HmacSHA256
	inter.worker.key.size = null
	inter.worker.key.ttl.ms = 3600000
	inter.worker.signature.algorithm = HmacSHA256
	inter.worker.verification.algorithms = [HmacSHA256]
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = [http://:8083]
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 1
	offset.storage.topic = connect-offsets
	plugin.discovery = hybrid_warn
	plugin.path = null
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	status.storage.partitions = 5
	status.storage.replication.factor = 1
	status.storage.topic = connect-status
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000
 (org.apache.kafka.connect.runtime.distributed.DistributedConfig:371)
[2025-01-03 10:20:37,934] INFO Creating Kafka admin client (org.apache.kafka.connect.runtime.WorkerConfig:281)
[2025-01-03 10:20:37,935] INFO AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:371)
[2025-01-03 10:20:37,959] INFO These configurations '[config.storage.topic, status.storage.topic, group.id, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2025-01-03 10:20:37,959] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:20:37,960] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:20:37,960] INFO Kafka startTimeMs: 1735921237959 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:20:38,067] INFO Kafka cluster ID: 7MkOCEgqRyytrrS9sGNpvQ (org.apache.kafka.connect.runtime.WorkerConfig:298)
[2025-01-03 10:20:38,067] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:20:38,071] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:20:38,071] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:20:38,071] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:20:38,073] INFO PublicConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	listeners = [http://:8083]
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
 (org.apache.kafka.connect.runtime.rest.RestServerConfig$PublicConfig:371)
[2025-01-03 10:20:38,077] INFO Logging initialized @1179ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:170)
[2025-01-03 10:20:38,090] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:125)
[2025-01-03 10:20:38,090] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:196)
[2025-01-03 10:20:38,098] INFO jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 23+37-2369 (org.eclipse.jetty.server.Server:375)
[2025-01-03 10:20:38,106] INFO Started http_8083@10e5bf9c{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:333)
[2025-01-03 10:20:38,106] INFO Started @1208ms (org.eclipse.jetty.server.Server:415)
[2025-01-03 10:20:38,112] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 10:20:38,112] INFO REST server listening at http://192.168.1.5:8083/, advertising URL http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:216)
[2025-01-03 10:20:38,112] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 10:20:38,112] INFO REST admin endpoints at http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2025-01-03 10:20:38,112] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 10:20:38,113] INFO Setting up All Policy for ConnectorClientConfigOverride. This will allow all client configurations to be overridden (org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy:45)
[2025-01-03 10:20:38,114] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:20:38,119] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:20:38,119] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:20:38,119] INFO Kafka startTimeMs: 1735921238119 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:20:38,121] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:20:38,121] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:20:38,126] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 10:20:38,134] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:20:38,134] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:20:38,134] INFO Kafka startTimeMs: 1735921238134 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:20:38,135] INFO Kafka Connect worker initialization took 1005ms (org.apache.kafka.connect.cli.AbstractConnectCli:141)
[2025-01-03 10:20:38,135] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:67)
[2025-01-03 10:20:38,136] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2025-01-03 10:20:38,136] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder starting (org.apache.kafka.connect.runtime.distributed.DistributedHerder:375)
[2025-01-03 10:20:38,137] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:233)
[2025-01-03 10:20:38,137] INFO Starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:232)
[2025-01-03 10:20:38,137] INFO Starting KafkaBasedLog with topic connect-offsets reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog:254)
[2025-01-03 10:20:38,137] INFO AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-shared-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:371)
[2025-01-03 10:20:38,138] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, group.id, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2025-01-03 10:20:38,138] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:20:38,138] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:20:38,138] INFO Kafka startTimeMs: 1735921238138 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:20:38,150] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:238)
[2025-01-03 10:20:38,164] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:334)
[2025-01-03 10:20:38,166] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:339)
[2025-01-03 10:20:38,166] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session:132)
[2025-01-03 10:20:38,348] INFO Started o.e.j.s.ServletContextHandler@27585351{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:921)
[2025-01-03 10:20:38,352] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:303)
[2025-01-03 10:20:38,352] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:77)
[2025-01-03 10:20:38,434] INFO Created topic (name=connect-offsets, numPartitions=25, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:9092 (org.apache.kafka.connect.util.TopicAdmin:416)
[2025-01-03 10:20:38,436] INFO ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-offsets
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 10:20:38,443] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:20:38,449] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, group.id, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 10:20:38,449] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:20:38,449] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:20:38,449] INFO Kafka startTimeMs: 1735921238449 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:20:38,452] INFO [Producer clientId=connect-cluster-offsets] Cluster ID: 7MkOCEgqRyytrrS9sGNpvQ (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:20:38,452] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-offsets
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 10:20:38,456] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:20:38,465] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2025-01-03 10:20:38,465] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:20:38,465] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:20:38,465] INFO Kafka startTimeMs: 1735921238465 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:20:38,468] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Cluster ID: 7MkOCEgqRyytrrS9sGNpvQ (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:20:38,470] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Assigned to partition(s): connect-offsets-0, connect-offsets-5, connect-offsets-10, connect-offsets-20, connect-offsets-15, connect-offsets-9, connect-offsets-11, connect-offsets-4, connect-offsets-16, connect-offsets-17, connect-offsets-3, connect-offsets-24, connect-offsets-23, connect-offsets-13, connect-offsets-18, connect-offsets-22, connect-offsets-8, connect-offsets-2, connect-offsets-12, connect-offsets-19, connect-offsets-14, connect-offsets-1, connect-offsets-6, connect-offsets-7, connect-offsets-21 (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:579)
[2025-01-03 10:20:38,471] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:20:38,471] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-5 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:20:38,471] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-10 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:20:38,471] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-20 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:20:38,471] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-15 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:20:38,471] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-9 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:20:38,471] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-11 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:20:38,471] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:20:38,471] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-16 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:20:38,471] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-17 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:20:38,471] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:20:38,471] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-24 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:20:38,471] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-23 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:20:38,471] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-13 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:20:38,471] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-18 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:20:38,471] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-22 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:20:38,471] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-8 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:20:38,471] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:20:38,471] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-12 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:20:38,471] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-19 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:20:38,471] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-14 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:20:38,471] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:20:38,472] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-6 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:20:38,472] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-7 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:20:38,472] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-21 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:20:38,492] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:20:38,492] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:20:38,492] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:20:38,492] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:20:38,492] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:20:38,492] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:20:38,492] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:20:38,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:20:38,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:20:38,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:20:38,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:20:38,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:20:38,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:20:38,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:20:38,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:20:38,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:20:38,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:20:38,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:20:38,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:20:38,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:20:38,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:20:38,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:20:38,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:20:38,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:20:38,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:20:38,493] INFO Finished reading KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:311)
[2025-01-03 10:20:38,494] INFO Started KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:313)
[2025-01-03 10:20:38,494] INFO Finished reading offsets topic and starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:249)
[2025-01-03 10:20:38,494] INFO Worker started (org.apache.kafka.connect.runtime.Worker:243)
[2025-01-03 10:20:38,494] INFO Starting KafkaBasedLog with topic connect-status reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog:254)
[2025-01-03 10:20:38,558] INFO Created topic (name=connect-status, numPartitions=5, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:9092 (org.apache.kafka.connect.util.TopicAdmin:416)
[2025-01-03 10:20:38,558] INFO ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-statuses
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 10:20:38,558] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:20:38,560] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, group.id, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 10:20:38,560] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:20:38,560] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:20:38,560] INFO Kafka startTimeMs: 1735921238560 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:20:38,560] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-statuses
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 10:20:38,561] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:20:38,562] INFO [Producer clientId=connect-cluster-statuses] Cluster ID: 7MkOCEgqRyytrrS9sGNpvQ (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:20:38,562] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2025-01-03 10:20:38,562] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:20:38,562] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:20:38,562] INFO Kafka startTimeMs: 1735921238562 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:20:38,564] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Cluster ID: 7MkOCEgqRyytrrS9sGNpvQ (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:20:38,564] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Assigned to partition(s): connect-status-0, connect-status-4, connect-status-1, connect-status-2, connect-status-3 (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:579)
[2025-01-03 10:20:38,564] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:20:38,564] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:20:38,564] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:20:38,564] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:20:38,564] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:20:38,572] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:20:38,573] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:20:38,573] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:20:38,573] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:20:38,573] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:20:38,573] INFO Finished reading KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:311)
[2025-01-03 10:20:38,573] INFO Started KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:313)
[2025-01-03 10:20:38,574] INFO Starting KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:378)
[2025-01-03 10:20:38,574] INFO Starting KafkaBasedLog with topic connect-configs reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog:254)
[2025-01-03 10:20:38,605] INFO Created topic (name=connect-configs, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:9092 (org.apache.kafka.connect.util.TopicAdmin:416)
[2025-01-03 10:20:38,605] INFO ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-configs
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 10:20:38,606] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:20:38,607] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, group.id, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 10:20:38,607] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:20:38,607] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:20:38,607] INFO Kafka startTimeMs: 1735921238607 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:20:38,607] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-configs
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 10:20:38,608] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:20:38,608] INFO [Producer clientId=connect-cluster-configs] Cluster ID: 7MkOCEgqRyytrrS9sGNpvQ (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:20:38,609] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2025-01-03 10:20:38,609] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:20:38,609] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:20:38,609] INFO Kafka startTimeMs: 1735921238609 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:20:38,611] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Cluster ID: 7MkOCEgqRyytrrS9sGNpvQ (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:20:38,611] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Assigned to partition(s): connect-configs-0 (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:579)
[2025-01-03 10:20:38,611] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Seeking to earliest offset of partition connect-configs-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:20:38,616] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Resetting offset for partition connect-configs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:20:38,616] INFO Finished reading KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:311)
[2025-01-03 10:20:38,616] INFO Started KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:313)
[2025-01-03 10:20:38,616] INFO Started KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:402)
[2025-01-03 10:20:38,619] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Cluster ID: 7MkOCEgqRyytrrS9sGNpvQ (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:20:39,343] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Discovered group coordinator 192.168.1.5:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:937)
[2025-01-03 10:20:39,346] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2025-01-03 10:20:39,346] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 10:20:39,367] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 10:20:39,377] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=1, memberId='connect-192.168.1.5:8083-b23d094e-e8fa-4217-826b-504990d49221', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2025-01-03 10:20:39,397] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=1, memberId='connect-192.168.1.5:8083-b23d094e-e8fa-4217-826b-504990d49221', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2025-01-03 10:20:39,398] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Joined group at generation 1 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.1.5:8083-b23d094e-e8fa-4217-826b-504990d49221', leaderUrl='http://192.168.1.5:8083/', offset=-1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2025-01-03 10:20:39,398] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder started (org.apache.kafka.connect.runtime.distributed.DistributedHerder:387)
[2025-01-03 10:20:39,398] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connectors and tasks using config offset -1 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2025-01-03 10:20:39,398] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2025-01-03 10:20:39,435] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Session key updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2510)
[2025-01-03 10:21:03,195] ERROR Uncaught exception in REST call to /connectors (org.apache.kafka.connect.runtime.rest.errors.ConnectExceptionMapper:65)
org.apache.kafka.connect.errors.ConnectException: Failed to find any class that implements Connector and which name matches io.debezium.connector.mysql.MySqlConnector, available connectors are: PluginDesc{klass=class org.apache.kafka.connect.mirror.MirrorCheckpointConnector, name='org.apache.kafka.connect.mirror.MirrorCheckpointConnector', version='3.9.0', encodedVersion=3.9.0, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.mirror.MirrorHeartbeatConnector, name='org.apache.kafka.connect.mirror.MirrorHeartbeatConnector', version='3.9.0', encodedVersion=3.9.0, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.mirror.MirrorSourceConnector, name='org.apache.kafka.connect.mirror.MirrorSourceConnector', version='3.9.0', encodedVersion=3.9.0, type=source, typeName='source', location='classpath'}
	at org.apache.kafka.connect.runtime.isolation.Plugins.connectorClass(Plugins.java:321)
	at org.apache.kafka.connect.runtime.isolation.Plugins.newConnector(Plugins.java:292)
	at org.apache.kafka.connect.runtime.AbstractHerder.lambda$getConnector$7(AbstractHerder.java:954)
	at java.base/java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1713)
	at org.apache.kafka.connect.runtime.AbstractHerder.getConnector(AbstractHerder.java:954)
	at org.apache.kafka.connect.runtime.AbstractHerder.validateConnectorConfig(AbstractHerder.java:667)
	at org.apache.kafka.connect.runtime.AbstractHerder.lambda$validateConnectorConfig$3(AbstractHerder.java:579)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:21:03,211] INFO [0:0:0:0:0:0:0:1] - - [03/Jan/2025:16:21:03 +0000] "POST /connectors HTTP/1.1" 500 889 "-" "curl/8.7.1" 78 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-01-03 10:21:26,513] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:87)
[2025-01-03 10:21:26,514] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:358)
[2025-01-03 10:21:26,520] INFO Stopped http_8083@10e5bf9c{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:383)
[2025-01-03 10:21:26,521] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:149)
[2025-01-03 10:21:26,526] INFO Stopped o.e.j.s.ServletContextHandler@27585351{/,null,STOPPED} (org.eclipse.jetty.server.handler.ContextHandler:1159)
[2025-01-03 10:21:26,527] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:387)
[2025-01-03 10:21:26,527] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder stopping (org.apache.kafka.connect.runtime.distributed.DistributedHerder:851)
[2025-01-03 10:21:26,527] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Stopping connectors and tasks that are still assigned to this worker. (org.apache.kafka.connect.runtime.distributed.DistributedHerder:808)
[2025-01-03 10:21:26,527] INFO Stopping KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:317)
[2025-01-03 10:21:26,528] INFO [Producer clientId=connect-cluster-statuses] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 10:21:26,531] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:21:26,531] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:21:26,532] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:21:26,532] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:21:26,532] INFO App info kafka.producer for connect-cluster-statuses unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:21:26,534] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 10:21:26,534] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 10:21:26,573] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:21:26,573] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:21:26,573] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:21:26,573] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:21:26,575] INFO App info kafka.consumer for connect-cluster-statuses unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:21:26,575] INFO Stopped KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:341)
[2025-01-03 10:21:26,576] INFO Closing KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:407)
[2025-01-03 10:21:26,576] INFO Stopping KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:317)
[2025-01-03 10:21:26,576] INFO [Producer clientId=connect-cluster-configs] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 10:21:26,579] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:21:26,580] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:21:26,580] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:21:26,580] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:21:26,580] INFO App info kafka.producer for connect-cluster-configs unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:21:26,580] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 10:21:26,580] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 10:21:26,937] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:21:26,938] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:21:26,938] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:21:26,938] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:21:26,942] INFO App info kafka.consumer for connect-cluster-configs unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:21:26,942] INFO Stopped KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:341)
[2025-01-03 10:21:26,943] INFO Closed KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:412)
[2025-01-03 10:21:26,943] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:250)
[2025-01-03 10:21:26,944] INFO Stopping KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:261)
[2025-01-03 10:21:26,944] INFO Stopping KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:317)
[2025-01-03 10:21:26,945] INFO [Producer clientId=connect-cluster-offsets] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 10:21:26,948] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:21:26,948] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:21:26,948] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:21:26,948] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:21:26,949] INFO App info kafka.producer for connect-cluster-offsets unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:21:26,949] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 10:21:26,949] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 10:21:27,079] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:21:27,080] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:21:27,080] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:21:27,081] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:21:27,084] INFO App info kafka.consumer for connect-cluster-offsets unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:21:27,084] INFO Stopped KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:341)
[2025-01-03 10:21:27,084] INFO Stopped KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:263)
[2025-01-03 10:21:27,084] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:21:27,084] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:21:27,085] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:21:27,085] INFO App info kafka.connect for 192.168.1.5:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:21:27,085] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:271)
[2025-01-03 10:21:27,086] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Member connect-192.168.1.5:8083-b23d094e-e8fa-4217-826b-504990d49221 sending LeaveGroup request to coordinator 192.168.1.5:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1174)
[2025-01-03 10:21:27,087] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1056)
[2025-01-03 10:21:27,088] WARN [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Close timed out with 1 pending requests to coordinator, terminating client connections (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1141)
[2025-01-03 10:21:27,088] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:21:27,088] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:21:27,088] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:21:27,090] INFO App info kafka.connect for connect-192.168.1.5:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:21:27,092] INFO App info kafka.admin.client for connect-cluster-shared-admin unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:21:27,093] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:21:27,093] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:21:27,094] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:21:27,094] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder stopped (org.apache.kafka.connect.runtime.distributed.DistributedHerder:394)
[2025-01-03 10:21:27,094] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder stopped (org.apache.kafka.connect.runtime.distributed.DistributedHerder:858)
[2025-01-03 10:21:27,094] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:92)
[2025-01-03 10:24:10,065] INFO Kafka Connect worker initializing ... (org.apache.kafka.connect.cli.AbstractConnectCli:114)
[2025-01-03 10:24:10,067] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote=true, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 23, 23+37-2369
	jvm.classpath = /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/activation-1.1.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/aopalliance-repackaged-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/argparse4j-0.7.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/audience-annotations-0.12.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/caffeine-2.9.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-beanutils-1.9.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-cli-1.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-collections-3.2.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-digester-2.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-io-2.14.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-lang3-3.12.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-logging-1.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-validator-1.7.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-basic-auth-extension-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-json-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-mirror-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-mirror-client-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-runtime-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-transforms-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/error_prone_annotations-2.10.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-api-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-locator-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-utils-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-annotations-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-core-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-databind-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-dataformat-csv-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-datatype-jdk8-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-jaxrs-base-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-jaxrs-json-provider-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-afterburner-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-jaxb-annotations-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-scala_2.13-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.activation-api-1.2.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.annotation-api-1.3.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.inject-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.validation-api-2.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.xml.bind-api-2.3.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javassist-3.29.2-GA.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.activation-api-1.2.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.annotation-api-1.3.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.servlet-api-3.1.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.ws.rs-api-2.1.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jaxb-api-2.3.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-client-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-common-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-container-servlet-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-container-servlet-core-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-hk2-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-server-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-client-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-continuation-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-http-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-io-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-security-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-server-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-servlet-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-servlets-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-util-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-util-ajax-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jline-3.25.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jopt-simple-5.0.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jose4j-0.9.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jsr305-3.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-clients-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-group-coordinator-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-group-coordinator-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-metadata-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-raft-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-server-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-server-common-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-shell-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-storage-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-storage-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-examples-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-scala_2.13-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-test-utils-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-tools-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-tools-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-transaction-coordinator-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka_2.13-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/lz4-java-1.8.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/maven-artifact-3.9.6.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/metrics-core-2.2.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/metrics-core-4.1.12.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-buffer-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-codec-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-common-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-handler-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-resolver-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-classes-epoll-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-native-epoll-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-native-unix-common-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/opentelemetry-proto-1.0.0-alpha.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/osgi-resource-locator-1.0.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/paranamer-2.8.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/pcollections-4.0.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/plexus-utils-3.5.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/protobuf-java-3.25.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/reflections-0.10.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/reload4j-1.2.25.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/rocksdbjni-7.9.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-collection-compat_2.13-2.10.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-java8-compat_2.13-1.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-library-2.13.14.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-logging_2.13-3.9.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-reflect-2.13.14.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/slf4j-api-1.7.36.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/slf4j-reload4j-1.7.36.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/snappy-java-1.1.10.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/swagger-annotations-2.2.8.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/trogdor-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zookeeper-3.8.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zookeeper-jute-3.8.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zstd-jni-1.5.6-4.jar
	os.spec = Mac OS X, aarch64, 15.0
	os.vcpus = 11
 (org.apache.kafka.connect.runtime.WorkerInfo:72)
[2025-01-03 10:24:10,068] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.AbstractConnectCli:120)
[2025-01-03 10:24:10,078] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:24:10,106] INFO Using up-to-date JsonConverter implementation (io.debezium.converters.CloudEventsConverter:120)
[2025-01-03 10:24:10,185] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:24:10,185] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:24:10,197] INFO Using up-to-date JsonConverter implementation (io.debezium.converters.CloudEventsConverter:120)
[2025-01-03 10:24:10,211] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:24:10,213] INFO Loading plugin from: classpath (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:24:10,215] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@5a07e868 (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:24:10,216] INFO Scanning plugins with ServiceLoaderScanner took 138 ms (org.apache.kafka.connect.runtime.isolation.PluginScanner:71)
[2025-01-03 10:24:10,216] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:24:10,335] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:24:10,336] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:24:10,521] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:24:10,521] INFO Loading plugin from: classpath (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:24:11,047] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@5a07e868 (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:24:11,047] INFO Scanning plugins with ReflectionScanner took 831 ms (org.apache.kafka.connect.runtime.isolation.PluginScanner:71)
[2025-01-03 10:24:11,048] WARN One or more plugins are missing ServiceLoader manifests may not be usable with plugin.discovery=service_load: [
file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/	io.debezium.connector.mongodb.MongoDbSinkConnector	sink	3.0.5.Final
]
Read the documentation at https://kafka.apache.org/documentation.html#connect_plugindiscovery for instructions on migrating your plugins to take advantage of the performance improvements of service_load mode. To silence this warning, set plugin.discovery=only_scan in the worker config. (org.apache.kafka.connect.runtime.isolation.Plugins:123)
[2025-01-03 10:24:11,049] INFO Added plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,049] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,049] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,049] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,049] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,049] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,049] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,049] INFO Added plugin 'io.debezium.transforms.HeaderToValue' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,049] INFO Added plugin 'io.debezium.transforms.SchemaChangeEventFilter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,049] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,049] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,049] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,049] INFO Added plugin 'io.debezium.transforms.ExtractSchemaToNewRecord' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,049] INFO Added plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,049] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,049] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,049] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,049] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,049] INFO Added plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,049] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,049] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,050] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,050] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,050] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,050] INFO Added plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,050] INFO Added plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,050] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,050] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,050] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,050] INFO Added plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,050] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,050] INFO Added plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,050] INFO Added plugin 'io.debezium.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,050] INFO Added plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,050] INFO Added plugin 'io.debezium.transforms.partitions.PartitionRouting' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,050] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,050] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,050] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,050] INFO Added plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,050] INFO Added plugin 'io.debezium.transforms.TimezoneConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,050] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,050] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,050] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,050] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,050] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,051] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,051] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,051] INFO Added plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,051] INFO Added plugin 'io.debezium.transforms.ExtractChangedRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,051] INFO Added plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,051] INFO Added plugin 'io.debezium.connector.mongodb.transforms.outbox.MongoEventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,051] INFO Added plugin 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,051] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,051] INFO Added plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,051] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,051] INFO Added plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,051] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,051] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,051] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,051] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,051] INFO Added plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,051] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,051] INFO Added plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,051] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,051] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,051] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,051] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,051] INFO Added plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,051] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,051] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:24:11,052] INFO Added alias 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,052] INFO Added alias 'ExtractNewDocumentState' to plugin 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,052] INFO Added alias 'JsonConverter' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,052] INFO Added alias 'DebeziumMySql' to plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,052] INFO Added alias 'CloudEventsConverter' to plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,052] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,052] INFO Added alias 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,052] INFO Added alias 'DebeziumMySqlConnectRestExtension' to plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,052] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,052] INFO Added alias 'MySql' to plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,052] INFO Added alias 'MongoDbSinkConnector' to plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,052] INFO Added alias 'EnvVar' to plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,052] INFO Added alias 'EnvVarConfigProvider' to plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,052] INFO Added alias 'MongoDb' to plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,053] INFO Added alias 'MirrorCheckpointConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,053] INFO Added alias 'Boolean' to plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,053] INFO Added alias 'MySqlConnector' to plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,053] INFO Added alias 'NoneConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,053] INFO Added alias 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,053] INFO Added alias 'HeaderToValue' to plugin 'io.debezium.transforms.HeaderToValue' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,053] INFO Added alias 'PartitionRouting' to plugin 'io.debezium.transforms.partitions.PartitionRouting' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,053] INFO Added alias 'MongoDbSink' to plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,053] INFO Added alias 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,053] INFO Added alias 'StringConverter' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,053] INFO Added alias 'ByLogicalTableRouter' to plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,053] INFO Added alias 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,053] INFO Added alias 'MongoDbConnector' to plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,053] INFO Added alias 'FileConfigProvider' to plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,053] INFO Added alias 'IntegerConverter' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,053] INFO Added alias 'EventRouter' to plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,053] INFO Added alias 'SchemaChangeEventFilter' to plugin 'io.debezium.transforms.SchemaChangeEventFilter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,053] INFO Added alias 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,053] INFO Added alias 'LongConverter' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,053] INFO Added alias 'DropHeaders' to plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,053] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,053] INFO Added alias 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,053] INFO Added alias 'ExtractSchemaToNewRecord' to plugin 'io.debezium.transforms.ExtractSchemaToNewRecord' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,053] INFO Added alias 'BinaryData' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,053] INFO Added alias 'File' to plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,053] INFO Added alias 'FloatConverter' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,053] INFO Added alias 'DirectoryConfigProvider' to plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,053] INFO Added alias 'ReadToInsertEvent' to plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,054] INFO Added alias 'ShortConverter' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,054] INFO Added alias 'CloudEvents' to plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,054] INFO Added alias 'ActivateTracingSpan' to plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,054] INFO Added alias 'TimezoneConverter' to plugin 'io.debezium.transforms.TimezoneConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,054] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,054] INFO Added alias 'DoubleConverter' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,054] INFO Added alias 'Simple' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,054] INFO Added alias 'BinaryDataConverter' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,054] INFO Added alias 'AllConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,054] INFO Added alias 'ExtractNewRecordState' to plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,054] INFO Added alias 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,054] INFO Added alias 'DebeziumMongoDb' to plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,054] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,054] INFO Added alias 'MirrorHeartbeatConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,054] INFO Added alias 'InsertHeader' to plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,054] INFO Added alias 'Directory' to plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,054] INFO Added alias 'MirrorSourceConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,054] INFO Added alias 'PrincipalConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,054] INFO Added alias 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,054] INFO Added alias 'DebeziumMongoDbConnectRestExtension' to plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,055] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,055] INFO Added alias 'BooleanConverter' to plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,055] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,055] INFO Added alias 'MongoEventRouter' to plugin 'io.debezium.connector.mongodb.transforms.outbox.MongoEventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,055] INFO Added alias 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,055] INFO Added alias 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,055] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,055] INFO Added alias 'ExtractChangedRecordState' to plugin 'io.debezium.transforms.ExtractChangedRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,055] INFO Added alias 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,055] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,055] INFO Added alias 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:24:11,070] INFO DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	config.providers = []
	config.storage.replication.factor = 1
	config.storage.topic = connect-configs
	connect.protocol = sessioned
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = All
	exactly.once.source.support = disabled
	group.id = connect-cluster
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	heartbeat.interval.ms = 3000
	inter.worker.key.generation.algorithm = HmacSHA256
	inter.worker.key.size = null
	inter.worker.key.ttl.ms = 3600000
	inter.worker.signature.algorithm = HmacSHA256
	inter.worker.verification.algorithms = [HmacSHA256]
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = [http://:8083]
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 1
	offset.storage.topic = connect-offsets
	plugin.discovery = hybrid_warn
	plugin.path = [/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins]
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	status.storage.partitions = 5
	status.storage.replication.factor = 1
	status.storage.topic = connect-status
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000
 (org.apache.kafka.connect.runtime.distributed.DistributedConfig:371)
[2025-01-03 10:24:11,071] INFO Creating Kafka admin client (org.apache.kafka.connect.runtime.WorkerConfig:281)
[2025-01-03 10:24:11,072] INFO AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:371)
[2025-01-03 10:24:11,092] INFO These configurations '[config.storage.topic, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2025-01-03 10:24:11,092] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:24:11,092] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:24:11,092] INFO Kafka startTimeMs: 1735921451092 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:24:11,202] INFO Kafka cluster ID: lG5B1M_nTOa7_oILZ1X4hw (org.apache.kafka.connect.runtime.WorkerConfig:298)
[2025-01-03 10:24:11,202] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:24:11,204] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:24:11,204] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:24:11,204] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:24:11,206] INFO PublicConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	listeners = [http://:8083]
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
 (org.apache.kafka.connect.runtime.rest.RestServerConfig$PublicConfig:371)
[2025-01-03 10:24:11,210] INFO Logging initialized @1373ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:170)
[2025-01-03 10:24:11,222] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:125)
[2025-01-03 10:24:11,222] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:196)
[2025-01-03 10:24:11,231] INFO jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 23+37-2369 (org.eclipse.jetty.server.Server:375)
[2025-01-03 10:24:11,238] INFO Started http_8083@2a5abd3c{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:333)
[2025-01-03 10:24:11,238] INFO Started @1402ms (org.eclipse.jetty.server.Server:415)
[2025-01-03 10:24:11,245] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 10:24:11,245] INFO REST server listening at http://192.168.1.5:8083/, advertising URL http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:216)
[2025-01-03 10:24:11,245] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 10:24:11,245] INFO REST admin endpoints at http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2025-01-03 10:24:11,245] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 10:24:11,245] INFO Setting up All Policy for ConnectorClientConfigOverride. This will allow all client configurations to be overridden (org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy:45)
[2025-01-03 10:24:11,247] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:24:11,252] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:24:11,252] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:24:11,252] INFO Kafka startTimeMs: 1735921451252 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:24:11,254] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:24:11,254] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:24:11,261] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 10:24:11,271] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:24:11,272] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:24:11,272] INFO Kafka startTimeMs: 1735921451271 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:24:11,273] INFO Kafka Connect worker initialization took 1206ms (org.apache.kafka.connect.cli.AbstractConnectCli:141)
[2025-01-03 10:24:11,273] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:67)
[2025-01-03 10:24:11,273] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2025-01-03 10:24:11,273] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder starting (org.apache.kafka.connect.runtime.distributed.DistributedHerder:375)
[2025-01-03 10:24:11,274] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:233)
[2025-01-03 10:24:11,274] INFO Starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:232)
[2025-01-03 10:24:11,274] INFO Starting KafkaBasedLog with topic connect-offsets reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog:254)
[2025-01-03 10:24:11,274] INFO AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-shared-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:371)
[2025-01-03 10:24:11,278] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2025-01-03 10:24:11,278] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:24:11,278] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:24:11,278] INFO Kafka startTimeMs: 1735921451278 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:24:11,289] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:238)
[2025-01-03 10:24:11,305] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:334)
[2025-01-03 10:24:11,305] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:339)
[2025-01-03 10:24:11,305] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:132)
[2025-01-03 10:24:11,453] INFO Started o.e.j.s.ServletContextHandler@9f9146d{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:921)
[2025-01-03 10:24:11,455] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:303)
[2025-01-03 10:24:11,455] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:77)
[2025-01-03 10:24:11,630] INFO Created topic (name=connect-offsets, numPartitions=25, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:9092 (org.apache.kafka.connect.util.TopicAdmin:416)
[2025-01-03 10:24:11,633] INFO ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-offsets
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 10:24:11,640] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:24:11,647] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 10:24:11,647] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:24:11,647] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:24:11,647] INFO Kafka startTimeMs: 1735921451647 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:24:11,650] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-offsets
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 10:24:11,650] INFO [Producer clientId=connect-cluster-offsets] Cluster ID: lG5B1M_nTOa7_oILZ1X4hw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:24:11,654] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:24:11,663] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2025-01-03 10:24:11,664] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:24:11,664] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:24:11,664] INFO Kafka startTimeMs: 1735921451664 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:24:11,667] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Cluster ID: lG5B1M_nTOa7_oILZ1X4hw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:24:11,670] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Assigned to partition(s): connect-offsets-0, connect-offsets-5, connect-offsets-10, connect-offsets-20, connect-offsets-15, connect-offsets-9, connect-offsets-11, connect-offsets-4, connect-offsets-16, connect-offsets-17, connect-offsets-3, connect-offsets-24, connect-offsets-23, connect-offsets-13, connect-offsets-18, connect-offsets-22, connect-offsets-8, connect-offsets-2, connect-offsets-12, connect-offsets-19, connect-offsets-14, connect-offsets-1, connect-offsets-6, connect-offsets-7, connect-offsets-21 (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:579)
[2025-01-03 10:24:11,671] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:24:11,671] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-5 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:24:11,671] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-10 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:24:11,671] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-20 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:24:11,671] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-15 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:24:11,671] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-9 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:24:11,671] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-11 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:24:11,671] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:24:11,671] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-16 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:24:11,671] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-17 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:24:11,671] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:24:11,671] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-24 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:24:11,672] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-23 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:24:11,672] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-13 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:24:11,672] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-18 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:24:11,672] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-22 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:24:11,672] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-8 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:24:11,672] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:24:11,672] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-12 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:24:11,672] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-19 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:24:11,672] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-14 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:24:11,672] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:24:11,672] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-6 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:24:11,672] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-7 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:24:11,672] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-21 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:24:11,691] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:24:11,691] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:24:11,692] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:24:11,692] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:24:11,692] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:24:11,692] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:24:11,692] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:24:11,692] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:24:11,692] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:24:11,692] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:24:11,692] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:24:11,692] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:24:11,692] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:24:11,692] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:24:11,692] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:24:11,692] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:24:11,692] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:24:11,692] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:24:11,692] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:24:11,692] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:24:11,692] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:24:11,692] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:24:11,692] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:24:11,692] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:24:11,692] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:24:11,693] INFO Finished reading KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:311)
[2025-01-03 10:24:11,693] INFO Started KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:313)
[2025-01-03 10:24:11,693] INFO Finished reading offsets topic and starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:249)
[2025-01-03 10:24:11,693] INFO Worker started (org.apache.kafka.connect.runtime.Worker:243)
[2025-01-03 10:24:11,693] INFO Starting KafkaBasedLog with topic connect-status reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog:254)
[2025-01-03 10:24:11,764] INFO Created topic (name=connect-status, numPartitions=5, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:9092 (org.apache.kafka.connect.util.TopicAdmin:416)
[2025-01-03 10:24:11,764] INFO ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-statuses
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 10:24:11,764] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:24:11,766] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 10:24:11,766] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:24:11,766] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:24:11,766] INFO Kafka startTimeMs: 1735921451766 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:24:11,766] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-statuses
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 10:24:11,766] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:24:11,768] INFO [Producer clientId=connect-cluster-statuses] Cluster ID: lG5B1M_nTOa7_oILZ1X4hw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:24:11,768] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2025-01-03 10:24:11,768] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:24:11,768] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:24:11,768] INFO Kafka startTimeMs: 1735921451768 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:24:11,771] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Cluster ID: lG5B1M_nTOa7_oILZ1X4hw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:24:11,771] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Assigned to partition(s): connect-status-0, connect-status-4, connect-status-1, connect-status-2, connect-status-3 (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:579)
[2025-01-03 10:24:11,771] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:24:11,771] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:24:11,771] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:24:11,771] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:24:11,771] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:24:11,776] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:24:11,776] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:24:11,776] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:24:11,776] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:24:11,776] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:24:11,776] INFO Finished reading KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:311)
[2025-01-03 10:24:11,776] INFO Started KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:313)
[2025-01-03 10:24:11,777] INFO Starting KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:378)
[2025-01-03 10:24:11,778] INFO Starting KafkaBasedLog with topic connect-configs reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog:254)
[2025-01-03 10:24:11,809] INFO Created topic (name=connect-configs, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:9092 (org.apache.kafka.connect.util.TopicAdmin:416)
[2025-01-03 10:24:11,809] INFO ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-configs
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 10:24:11,809] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:24:11,811] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 10:24:11,811] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:24:11,811] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:24:11,811] INFO Kafka startTimeMs: 1735921451811 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:24:11,811] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-configs
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 10:24:11,811] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:24:11,813] INFO [Producer clientId=connect-cluster-configs] Cluster ID: lG5B1M_nTOa7_oILZ1X4hw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:24:11,813] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2025-01-03 10:24:11,813] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:24:11,813] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:24:11,813] INFO Kafka startTimeMs: 1735921451813 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:24:11,814] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Cluster ID: lG5B1M_nTOa7_oILZ1X4hw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:24:11,815] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Assigned to partition(s): connect-configs-0 (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:579)
[2025-01-03 10:24:11,815] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Seeking to earliest offset of partition connect-configs-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:24:11,821] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Resetting offset for partition connect-configs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:24:11,821] INFO Finished reading KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:311)
[2025-01-03 10:24:11,821] INFO Started KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:313)
[2025-01-03 10:24:11,821] INFO Started KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:402)
[2025-01-03 10:24:11,825] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Cluster ID: lG5B1M_nTOa7_oILZ1X4hw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:24:12,505] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Discovered group coordinator 192.168.1.5:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:937)
[2025-01-03 10:24:12,508] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2025-01-03 10:24:12,508] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 10:24:12,525] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 10:24:12,533] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=1, memberId='connect-192.168.1.5:8083-b90e671e-a8d1-4e20-b74d-99c309313aee', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2025-01-03 10:24:12,564] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=1, memberId='connect-192.168.1.5:8083-b90e671e-a8d1-4e20-b74d-99c309313aee', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2025-01-03 10:24:12,564] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Joined group at generation 1 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.1.5:8083-b90e671e-a8d1-4e20-b74d-99c309313aee', leaderUrl='http://192.168.1.5:8083/', offset=-1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2025-01-03 10:24:12,564] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder started (org.apache.kafka.connect.runtime.distributed.DistributedHerder:387)
[2025-01-03 10:24:12,565] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connectors and tasks using config offset -1 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2025-01-03 10:24:12,565] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2025-01-03 10:24:12,603] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Session key updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2510)
[2025-01-03 10:24:29,910] INFO Loading the custom source info struct maker plugin: io.debezium.connector.mysql.MySqlSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2025-01-03 10:24:30,346] INFO Using 'SHOW MASTER STATUS' to get binary log status (io.debezium.connector.mysql.jdbc.MySqlConnection:41)
[2025-01-03 10:24:30,363] INFO Successfully tested connection for jdbc:mysql://localhost:3306/?useInformationSchema=true&nullCatalogMeansCurrent=false&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8&zeroDateTimeBehavior=CONVERT_TO_NULL&connectTimeout=30000 with user 'root' (io.debezium.connector.binlog.BinlogConnector:66)
[2025-01-03 10:24:30,365] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection:983)
[2025-01-03 10:24:30,366] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:371)
[2025-01-03 10:24:30,371] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Connector mariadb-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2025-01-03 10:24:30,371] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2025-01-03 10:24:30,371] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 10:24:30,373] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=2, memberId='connect-192.168.1.5:8083-b90e671e-a8d1-4e20-b74d-99c309313aee', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2025-01-03 10:24:30,376] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=2, memberId='connect-192.168.1.5:8083-b90e671e-a8d1-4e20-b74d-99c309313aee', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2025-01-03 10:24:30,376] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Joined group at generation 2 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.1.5:8083-b90e671e-a8d1-4e20-b74d-99c309313aee', leaderUrl='http://192.168.1.5:8083/', offset=2, connectorIds=[mariadb-connector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2025-01-03 10:24:30,376] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 2 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2025-01-03 10:24:30,377] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connector mariadb-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2025-01-03 10:24:30,378] INFO [mariadb-connector|worker] Creating connector mariadb-connector of type io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:313)
[2025-01-03 10:24:30,378] INFO [mariadb-connector|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2025-01-03 10:24:30,378] INFO [mariadb-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 10:24:30,380] INFO [mariadb-connector|worker] Instantiated connector mariadb-connector with version 3.0.5.Final of type class io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:335)
[2025-01-03 10:24:30,380] INFO [mariadb-connector|worker] Finished creating connector mariadb-connector (org.apache.kafka.connect.runtime.Worker:356)
[2025-01-03 10:24:30,380] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2025-01-03 10:24:30,383] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2025-01-03 10:24:30,383] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 10:24:30,387] INFO [0:0:0:0:0:0:0:1] - - [03/Jan/2025:16:24:29 +0000] "POST /connectors HTTP/1.1" 201 681 "-" "curl/8.7.1" 550 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-01-03 10:24:30,391] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Tasks [mariadb-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2025-01-03 10:24:30,391] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2025-01-03 10:24:30,392] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 10:24:30,392] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=3, memberId='connect-192.168.1.5:8083-b90e671e-a8d1-4e20-b74d-99c309313aee', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2025-01-03 10:24:30,394] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=3, memberId='connect-192.168.1.5:8083-b90e671e-a8d1-4e20-b74d-99c309313aee', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2025-01-03 10:24:30,394] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Joined group at generation 3 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.1.5:8083-b90e671e-a8d1-4e20-b74d-99c309313aee', leaderUrl='http://192.168.1.5:8083/', offset=4, connectorIds=[mariadb-connector], taskIds=[mariadb-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2025-01-03 10:24:30,394] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 4 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2025-01-03 10:24:30,395] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting task mariadb-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2025-01-03 10:24:30,396] INFO [mariadb-connector|task-0] Creating task mariadb-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2025-01-03 10:24:30,396] INFO [mariadb-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mariadb-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2025-01-03 10:24:30,396] INFO [mariadb-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mariadb-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 10:24:30,397] INFO [mariadb-connector|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.mysql.MySqlConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2025-01-03 10:24:30,398] INFO [mariadb-connector|task-0] Instantiated task mariadb-connector-0 with version 3.0.5.Final of type io.debezium.connector.mysql.MySqlConnectorTask (org.apache.kafka.connect.runtime.Worker:665)
[2025-01-03 10:24:30,398] INFO [mariadb-connector|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:24:30,398] INFO [mariadb-connector|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mariadb-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:678)
[2025-01-03 10:24:30,399] INFO [mariadb-connector|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:24:30,399] INFO [mariadb-connector|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mariadb-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:684)
[2025-01-03 10:24:30,399] INFO [mariadb-connector|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mariadb-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2025-01-03 10:24:30,400] INFO [mariadb-connector|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:1795)
[2025-01-03 10:24:30,400] INFO [mariadb-connector|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2025-01-03 10:24:30,400] INFO [mariadb-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 10:24:30,400] INFO [mariadb-connector|task-0] ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-mariadb-connector-0
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 10:24:30,400] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:24:30,401] INFO [mariadb-connector|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 10:24:30,401] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:24:30,401] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:24:30,401] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735921470401 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:24:30,403] INFO [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] Cluster ID: lG5B1M_nTOa7_oILZ1X4hw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:24:30,404] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2025-01-03 10:24:30,405] INFO [mariadb-connector|task-0] Starting MySqlConnectorTask with configuration:
   connector.class = io.debezium.connector.mysql.MySqlConnector
   database.user = root
   database.server.id = 184054
   database.history.kafka.bootstrap.servers = localhost:9092
   database.history.kafka.topic = db.history.leafy_factory
   database.server.name = leafy_factory
   schema.history.internal.kafka.bootstrap.servers = localhost:9092
   database.port = 3306
   include.schema.changes = false
   topic.prefix = db_
   schema.history.internal.kafka.topic = db.history.internal
   task.class = io.debezium.connector.mysql.MySqlConnectorTask
   database.hostname = localhost
   database.password = ********
   name = mariadb-connector
   database.include.list = leafy_factory
 (io.debezium.connector.common.BaseSourceTask:250)
[2025-01-03 10:24:30,405] INFO [mariadb-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mysql.MySqlSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2025-01-03 10:24:30,405] INFO [mariadb-connector|task-0] Loading the custom topic naming strategy plugin: io.debezium.schema.DefaultTopicNamingStrategy (io.debezium.config.CommonConnectorConfig:1401)
[2025-01-03 10:24:30,441] INFO [mariadb-connector|task-0] Using 'SHOW MASTER STATUS' to get binary log status (io.debezium.connector.mysql.jdbc.MySqlConnection:41)
[2025-01-03 10:24:30,444] INFO [mariadb-connector|task-0] No previous offsets found (io.debezium.connector.common.BaseSourceTask:536)
[2025-01-03 10:24:30,462] INFO [mariadb-connector|task-0] KafkaSchemaHistory Consumer config: {key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, enable.auto.commit=false, group.id=db_-schemahistory, bootstrap.servers=localhost:9092, fetch.min.bytes=1, session.timeout.ms=10000, auto.offset.reset=earliest, client.id=db_-schemahistory} (io.debezium.storage.kafka.history.KafkaSchemaHistory:245)
[2025-01-03 10:24:30,462] INFO [mariadb-connector|task-0] KafkaSchemaHistory Producer config: {retries=1, value.serializer=org.apache.kafka.common.serialization.StringSerializer, acks=1, batch.size=32768, max.block.ms=10000, bootstrap.servers=localhost:9092, buffer.memory=1048576, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=db_-schemahistory, linger.ms=0} (io.debezium.storage.kafka.history.KafkaSchemaHistory:246)
[2025-01-03 10:24:30,462] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = db-history-config-check (io.debezium.util.Threads:270)
[2025-01-03 10:24:30,463] INFO [mariadb-connector|task-0] Idempotence will be disabled because acks is set to 1, not set to 'all'. (org.apache.kafka.clients.producer.ProducerConfig:587)
[2025-01-03 10:24:30,463] INFO [mariadb-connector|task-0] ProducerConfig values: 
	acks = 1
	auto.include.jmx.reporter = true
	batch.size = 32768
	bootstrap.servers = [localhost:9092]
	buffer.memory = 1048576
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 10:24:30,463] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:24:30,465] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:24:30,465] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:24:30,465] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735921470465 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:24:30,466] INFO [mariadb-connector|task-0] [Producer clientId=db_-schemahistory] Cluster ID: lG5B1M_nTOa7_oILZ1X4hw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:24:30,518] INFO [mariadb-connector|task-0] Using 'SHOW MASTER STATUS' to get binary log status (io.debezium.connector.mysql.jdbc.MySqlConnection:41)
[2025-01-03 10:24:30,525] INFO [mariadb-connector|task-0] Closing connection before starting schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:123)
[2025-01-03 10:24:30,525] INFO [mariadb-connector|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:983)
[2025-01-03 10:24:30,525] INFO [mariadb-connector|task-0] Connector started for the first time. (io.debezium.connector.common.BaseSourceTask:89)
[2025-01-03 10:24:30,525] INFO [mariadb-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = db_-schemahistory
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 10:24:30,526] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:24:30,527] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:24:30,527] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:24:30,527] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735921470527 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:24:30,529] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Cluster ID: lG5B1M_nTOa7_oILZ1X4hw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:24:30,530] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 10:24:30,530] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 10:24:30,531] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:24:30,531] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:24:30,531] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:24:30,531] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:24:30,532] INFO [mariadb-connector|task-0] App info kafka.consumer for db_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:24:30,532] INFO [mariadb-connector|task-0] AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:371)
[2025-01-03 10:24:30,532] INFO [mariadb-connector|task-0] These configurations '[value.serializer, acks, batch.size, max.block.ms, buffer.memory, key.serializer, linger.ms]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2025-01-03 10:24:30,532] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:24:30,533] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:24:30,533] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735921470532 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:24:30,569] INFO [mariadb-connector|task-0] Database schema history topic '(name=db.history.internal, numPartitions=1, replicationFactor=default, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807, retention.bytes=-1})' created (io.debezium.storage.kafka.history.KafkaSchemaHistory:555)
[2025-01-03 10:24:30,570] INFO [mariadb-connector|task-0] App info kafka.admin.client for db_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:24:30,570] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:24:30,570] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:24:30,570] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:24:30,570] INFO [mariadb-connector|task-0] Reconnecting after finishing schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:136)
[2025-01-03 10:24:30,595] INFO [mariadb-connector|task-0] No previous offset found (io.debezium.connector.mysql.MySqlConnectorTask:147)
[2025-01-03 10:24:30,602] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = SignalProcessor (io.debezium.util.Threads:270)
[2025-01-03 10:24:30,610] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = change-event-source-coordinator (io.debezium.util.Threads:270)
[2025-01-03 10:24:30,610] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = blocking-snapshot (io.debezium.util.Threads:270)
[2025-01-03 10:24:30,614] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-change-event-source-coordinator (io.debezium.util.Threads:287)
[2025-01-03 10:24:30,614] INFO [mariadb-connector|task-0] WorkerSourceTask{id=mariadb-connector-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.AbstractWorkerSourceTask:280)
[2025-01-03 10:24:30,615] INFO [mariadb-connector|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:137)
[2025-01-03 10:24:30,615] INFO [mariadb-connector|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:140)
[2025-01-03 10:24:30,618] INFO [mariadb-connector|task-0] According to the connector configuration both schema and data will be snapshot. (io.debezium.relational.RelationalSnapshotChangeEventSource:282)
[2025-01-03 10:24:30,618] INFO [mariadb-connector|task-0] Snapshot step 1 - Preparing (io.debezium.relational.RelationalSnapshotChangeEventSource:135)
[2025-01-03 10:24:30,620] INFO [mariadb-connector|task-0] Snapshot step 2 - Determining captured tables (io.debezium.relational.RelationalSnapshotChangeEventSource:144)
[2025-01-03 10:24:30,620] INFO [mariadb-connector|task-0] Read list of available databases (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:116)
[2025-01-03 10:24:30,623] INFO [mariadb-connector|task-0] 	 list of available databases is: [information_schema, leafy_factory, mysql, performance_schema, sys, test] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:118)
[2025-01-03 10:24:30,623] INFO [mariadb-connector|task-0] Read list of available tables in each database (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:126)
[2025-01-03 10:24:30,688] INFO [mariadb-connector|task-0] 	snapshot continuing with database(s): [leafy_factory] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:147)
[2025-01-03 10:24:30,688] INFO [mariadb-connector|task-0] Adding table leafy_factory.jobs_machines to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:24:30,688] INFO [mariadb-connector|task-0] Adding table leafy_factory.product_cost to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:24:30,688] INFO [mariadb-connector|task-0] Adding table leafy_factory.production_lines to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:24:30,688] INFO [mariadb-connector|task-0] Adding table leafy_factory.products_raw_materials to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:24:30,688] INFO [mariadb-connector|task-0] Adding table leafy_factory.machines to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:24:30,688] INFO [mariadb-connector|task-0] Adding table leafy_factory.jobs to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:24:30,688] INFO [mariadb-connector|task-0] Adding table leafy_factory.raw_materials to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:24:30,688] INFO [mariadb-connector|task-0] Adding table leafy_factory.work_orders to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:24:30,688] INFO [mariadb-connector|task-0] Adding table leafy_factory.products to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:24:30,688] INFO [mariadb-connector|task-0] Adding table leafy_factory.factories to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:24:30,689] INFO [mariadb-connector|task-0] Created connection pool with 1 threads (io.debezium.relational.RelationalSnapshotChangeEventSource:236)
[2025-01-03 10:24:30,689] INFO [mariadb-connector|task-0] Snapshot step 3 - Locking captured tables [leafy_factory.factories, leafy_factory.jobs, leafy_factory.jobs_machines, leafy_factory.machines, leafy_factory.product_cost, leafy_factory.production_lines, leafy_factory.products, leafy_factory.products_raw_materials, leafy_factory.raw_materials, leafy_factory.work_orders] (io.debezium.relational.RelationalSnapshotChangeEventSource:153)
[2025-01-03 10:24:30,691] INFO [mariadb-connector|task-0] Flush and obtain global read lock to prevent writes to database (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:488)
[2025-01-03 10:24:30,692] INFO [mariadb-connector|task-0] Snapshot step 4 - Determining snapshot offset (io.debezium.relational.RelationalSnapshotChangeEventSource:159)
[2025-01-03 10:24:30,693] INFO [mariadb-connector|task-0] Read binlog position of MySQL primary server (io.debezium.connector.mysql.MySqlSnapshotChangeEventSource:58)
[2025-01-03 10:24:30,694] INFO [mariadb-connector|task-0] Snapshot step 5 - Reading structure of captured tables (io.debezium.relational.RelationalSnapshotChangeEventSource:162)
[2025-01-03 10:24:30,694] INFO [mariadb-connector|task-0] All eligible tables schema should be captured, capturing: [leafy_factory.factories, leafy_factory.jobs, leafy_factory.jobs_machines, leafy_factory.machines, leafy_factory.product_cost, leafy_factory.production_lines, leafy_factory.products, leafy_factory.products_raw_materials, leafy_factory.raw_materials, leafy_factory.work_orders] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:314)
[2025-01-03 10:24:31,214] INFO [mariadb-connector|task-0] Reading structure of database 'leafy_factory' (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:348)
[2025-01-03 10:24:31,322] INFO [mariadb-connector|task-0] Snapshot step 6 - Persisting schema history (io.debezium.relational.RelationalSnapshotChangeEventSource:166)
[2025-01-03 10:24:31,362] INFO [mariadb-connector|task-0] Releasing global read lock to enable MySQL writes (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:497)
[2025-01-03 10:24:31,362] INFO [mariadb-connector|task-0] Writes to MySQL tables prevented for a total of 00:00:00.671 (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:501)
[2025-01-03 10:24:31,362] INFO [mariadb-connector|task-0] Snapshot step 7 - Snapshotting data (io.debezium.relational.RelationalSnapshotChangeEventSource:178)
[2025-01-03 10:24:31,363] INFO [mariadb-connector|task-0] Creating snapshot worker pool with 1 worker thread(s) (io.debezium.relational.RelationalSnapshotChangeEventSource:480)
[2025-01-03 10:24:31,363] INFO [mariadb-connector|task-0] For table 'leafy_factory.factories' using select statement: 'SELECT `id_factory`, `factory_name`, `factory_location`, `factory_timestamp` FROM `leafy_factory`.`factories`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:24:31,365] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.factories is OptionalLong[0] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:24:31,365] INFO [mariadb-connector|task-0] For table 'leafy_factory.jobs' using select statement: 'SELECT `id_job`, `target_output`, `nOk_products`, `quality_rate`, `job_status`, `creation_date`, `work_id` FROM `leafy_factory`.`jobs`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:24:31,366] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.jobs is OptionalLong[0] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:24:31,366] INFO [mariadb-connector|task-0] For table 'leafy_factory.jobs_machines' using select statement: 'SELECT `id_jobs_machines`, `job_id`, `machine_id` FROM `leafy_factory`.`jobs_machines`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:24:31,368] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.jobs_machines is OptionalLong[2] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:24:31,368] INFO [mariadb-connector|task-0] For table 'leafy_factory.machines' using select statement: 'SELECT `id_machine`, `machine_status`, `last_maintenance`, `operator`, `avg_output`, `reject_count`, `production_line_id` FROM `leafy_factory`.`machines`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:24:31,369] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.machines is OptionalLong[4] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:24:31,369] INFO [mariadb-connector|task-0] For table 'leafy_factory.product_cost' using select statement: 'SELECT `id_cost`, `raw_material_cost_per_product`, `overhead_per_product`, `total_cost_per_product`, `cost_ok_with_overhead`, `cost_nok_with_overhead`, `actual_total_cost`, `work_id` FROM `leafy_factory`.`product_cost`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:24:31,370] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.product_cost is OptionalLong[12] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:24:31,370] INFO [mariadb-connector|task-0] For table 'leafy_factory.production_lines' using select statement: 'SELECT `id_production_line`, `factory_id` FROM `leafy_factory`.`production_lines`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:24:31,371] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.production_lines is OptionalLong[2] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:24:31,371] INFO [mariadb-connector|task-0] For table 'leafy_factory.products' using select statement: 'SELECT `id_product`, `product_name`, `product_description` FROM `leafy_factory`.`products`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:24:31,373] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.products is OptionalLong[2] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:24:31,373] INFO [mariadb-connector|task-0] For table 'leafy_factory.products_raw_materials' using select statement: 'SELECT `id_products_raw_materials`, `product_id`, `raw_material_id` FROM `leafy_factory`.`products_raw_materials`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:24:31,374] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.products_raw_materials is OptionalLong[9] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:24:31,374] INFO [mariadb-connector|task-0] For table 'leafy_factory.raw_materials' using select statement: 'SELECT `id_raw_material`, `item_code`, `raw_material_name`, `raw_material_description`, `unit_measurement`, `raw_material_stock`, `raw_material_status`, `raw_material_currency`, `cost_per_part` FROM `leafy_factory`.`raw_materials`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:24:31,375] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.raw_materials is OptionalLong[8] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:24:31,375] INFO [mariadb-connector|task-0] For table 'leafy_factory.work_orders' using select statement: 'SELECT `id_work`, `planned_start_date`, `planned_end_date`, `actual_start_date`, `actual_end_date`, `quantity`, `wo_status`, `creation_date`, `product_id`, `nOk_products` FROM `leafy_factory`.`work_orders`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:24:31,377] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.work_orders is OptionalLong[12] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:24:31,378] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.factories' (1 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:24:31,389] INFO [mariadb-connector|task-0] 	 Finished exporting 1 records for table 'leafy_factory.factories' (1 of 10 tables); total duration '00:00:00.011' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:24:31,390] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.jobs' (2 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:24:31,391] INFO [mariadb-connector|task-0] 	 Finished exporting 1 records for table 'leafy_factory.jobs' (2 of 10 tables); total duration '00:00:00.001' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:24:31,391] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.jobs_machines' (3 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:24:31,392] INFO [mariadb-connector|task-0] 	 Finished exporting 2 records for table 'leafy_factory.jobs_machines' (3 of 10 tables); total duration '00:00:00.001' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:24:31,392] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.machines' (4 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:24:31,394] INFO [mariadb-connector|task-0] 	 Finished exporting 4 records for table 'leafy_factory.machines' (4 of 10 tables); total duration '00:00:00.002' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:24:31,394] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.product_cost' (5 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:24:31,397] INFO [mariadb-connector|task-0] 	 Finished exporting 20 records for table 'leafy_factory.product_cost' (5 of 10 tables); total duration '00:00:00.003' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:24:31,397] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.production_lines' (6 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:24:31,399] INFO [mariadb-connector|task-0] 	 Finished exporting 2 records for table 'leafy_factory.production_lines' (6 of 10 tables); total duration '00:00:00.002' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:24:31,399] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.products' (7 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:24:31,400] INFO [mariadb-connector|task-0] 	 Finished exporting 2 records for table 'leafy_factory.products' (7 of 10 tables); total duration '00:00:00.001' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:24:31,400] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.products_raw_materials' (8 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:24:31,402] INFO [mariadb-connector|task-0] 	 Finished exporting 8 records for table 'leafy_factory.products_raw_materials' (8 of 10 tables); total duration '00:00:00.002' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:24:31,402] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.raw_materials' (9 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:24:31,403] INFO [mariadb-connector|task-0] 	 Finished exporting 8 records for table 'leafy_factory.raw_materials' (9 of 10 tables); total duration '00:00:00.001' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:24:31,404] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.work_orders' (10 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:24:31,408] INFO [mariadb-connector|task-0] 	 Finished exporting 20 records for table 'leafy_factory.work_orders' (10 of 10 tables); total duration '00:00:00.004' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:24:31,409] INFO [mariadb-connector|task-0] Snapshot - Final stage (io.debezium.pipeline.source.AbstractSnapshotChangeEventSource:108)
[2025-01-03 10:24:31,409] INFO [mariadb-connector|task-0] Snapshot completed (io.debezium.pipeline.source.AbstractSnapshotChangeEventSource:112)
[2025-01-03 10:24:31,415] INFO [mariadb-connector|task-0] Snapshot ended with SnapshotResult [status=COMPLETED, offset=BinlogOffsetContext{sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=BinlogSourceInfo{currentGtid='null', currentBinlogFilename='mariadb-bin.000001', currentBinlogPosition=49121, currentRowNumber=0, serverId=0, sourceTime=2025-01-03T16:24:31Z, threadId=-1, currentQuery='null', tableIds=[leafy_factory.work_orders], databaseName='leafy_factory'}, snapshotCompleted=true, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet='null', currentGtidSet='null', restartBinlogFilename='mariadb-bin.000001', restartBinlogPosition=49121, restartRowsToSkip=0, restartEventsToSkip=0, currentEventLengthInBytes=0, inTransaction=false, transactionId='null', incrementalSnapshotContext=IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]}] (io.debezium.pipeline.ChangeEventSourceCoordinator:298)
[2025-01-03 10:24:31,417] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = binlog-client (io.debezium.util.Threads:270)
[2025-01-03 10:24:31,419] INFO [mariadb-connector|task-0] Enable ssl PREFERRED mode for connector db_ (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:1289)
[2025-01-03 10:24:31,422] INFO [mariadb-connector|task-0] SignalProcessor started. Scheduling it every 5000ms (io.debezium.pipeline.signal.SignalProcessor:105)
[2025-01-03 10:24:31,422] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-SignalProcessor (io.debezium.util.Threads:287)
[2025-01-03 10:24:31,422] INFO [mariadb-connector|task-0] Starting streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:323)
[2025-01-03 10:24:31,423] INFO [mariadb-connector|task-0] Skip 0 events on streaming start (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:278)
[2025-01-03 10:24:31,423] INFO [mariadb-connector|task-0] Skip 0 rows on streaming start (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:282)
[2025-01-03 10:24:31,424] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-binlog-client (io.debezium.util.Threads:287)
[2025-01-03 10:24:31,424] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-binlog-client (io.debezium.util.Threads:287)
[2025-01-03 10:24:31,448] INFO [mariadb-connector|task-0] Connected to binlog at localhost:3306, starting at BinlogOffsetContext{sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=BinlogSourceInfo{currentGtid='null', currentBinlogFilename='mariadb-bin.000001', currentBinlogPosition=49121, currentRowNumber=0, serverId=0, sourceTime=2025-01-03T16:24:31Z, threadId=-1, currentQuery='null', tableIds=[leafy_factory.work_orders], databaseName='leafy_factory'}, snapshotCompleted=true, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet='null', currentGtidSet='null', restartBinlogFilename='mariadb-bin.000001', restartBinlogPosition=49121, restartRowsToSkip=0, restartEventsToSkip=0, currentEventLengthInBytes=0, inTransaction=false, transactionId='null', incrementalSnapshotContext=IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]} (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:1232)
[2025-01-03 10:24:31,448] INFO [mariadb-connector|task-0] Waiting for keepalive thread to start (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:299)
[2025-01-03 10:24:31,448] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-binlog-client (io.debezium.util.Threads:287)
[2025-01-03 10:24:31,553] INFO [mariadb-connector|task-0] Keepalive thread is running (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:306)
[2025-01-03 10:24:31,648] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 4 : {db_.leafy_factory.factories=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:24:31,797] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 7 : {db_.leafy_factory.jobs=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:24:31,924] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 10 : {db_.leafy_factory.jobs_machines=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:24:32,063] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 13 : {db_.leafy_factory.machines=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:24:32,200] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 17 : {db_.leafy_factory.product_cost=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:24:32,331] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 23 : {db_.leafy_factory.production_lines=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:24:32,468] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 31 : {db_.leafy_factory.products=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:24:32,596] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 35 : {db_.leafy_factory.products_raw_materials=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:24:32,719] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 39 : {db_.leafy_factory.raw_materials=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:24:32,856] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 44 : {db_.leafy_factory.work_orders=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:24:40,409] INFO [mariadb-connector|task-0|offsets] WorkerSourceTask{id=mariadb-connector-0} Committing offsets for 68 acknowledged messages (org.apache.kafka.connect.runtime.WorkerSourceTask:236)
[2025-01-03 10:24:58,667] INFO [mariadb-connector|task-0] 74 records sent during previous 00:00:28.269, last recorded offset of {server=db_} partition is {ts_sec=1735921498, file=mariadb-bin.000001, pos=0, row=1, server_id=1} (io.debezium.connector.common.BaseSourceTask:351)
[2025-01-03 10:25:00,435] INFO [mariadb-connector|task-0|offsets] WorkerSourceTask{id=mariadb-connector-0} Committing offsets for 6 acknowledged messages (org.apache.kafka.connect.runtime.WorkerSourceTask:236)
[2025-01-03 10:29:48,879] ERROR Uncaught exception in REST call to /connectors (org.apache.kafka.connect.runtime.rest.errors.ConnectExceptionMapper:65)
org.apache.kafka.connect.errors.ConnectException: Failed to find any class that implements Connector and which name matches com.mongodb.kafka.connect.MongoSinkConnector, available connectors are: PluginDesc{klass=class io.debezium.connector.mongodb.MongoDbConnector, name='io.debezium.connector.mongodb.MongoDbConnector', version='3.0.5.Final', encodedVersion=3.0.5.Final, type=source, typeName='source', location='file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/'}, PluginDesc{klass=class io.debezium.connector.mongodb.MongoDbSinkConnector, name='io.debezium.connector.mongodb.MongoDbSinkConnector', version='3.0.5.Final', encodedVersion=3.0.5.Final, type=sink, typeName='sink', location='file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/'}, PluginDesc{klass=class io.debezium.connector.mysql.MySqlConnector, name='io.debezium.connector.mysql.MySqlConnector', version='3.0.5.Final', encodedVersion=3.0.5.Final, type=source, typeName='source', location='file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql/'}, PluginDesc{klass=class org.apache.kafka.connect.mirror.MirrorCheckpointConnector, name='org.apache.kafka.connect.mirror.MirrorCheckpointConnector', version='3.9.0', encodedVersion=3.9.0, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.mirror.MirrorHeartbeatConnector, name='org.apache.kafka.connect.mirror.MirrorHeartbeatConnector', version='3.9.0', encodedVersion=3.9.0, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.mirror.MirrorSourceConnector, name='org.apache.kafka.connect.mirror.MirrorSourceConnector', version='3.9.0', encodedVersion=3.9.0, type=source, typeName='source', location='classpath'}
	at org.apache.kafka.connect.runtime.isolation.Plugins.connectorClass(Plugins.java:321)
	at org.apache.kafka.connect.runtime.isolation.Plugins.newConnector(Plugins.java:292)
	at org.apache.kafka.connect.runtime.AbstractHerder.lambda$getConnector$7(AbstractHerder.java:954)
	at java.base/java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1713)
	at org.apache.kafka.connect.runtime.AbstractHerder.getConnector(AbstractHerder.java:954)
	at org.apache.kafka.connect.runtime.AbstractHerder.validateConnectorConfig(AbstractHerder.java:667)
	at org.apache.kafka.connect.runtime.AbstractHerder.lambda$validateConnectorConfig$3(AbstractHerder.java:579)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:29:48,887] INFO [0:0:0:0:0:0:0:1] - - [03/Jan/2025:16:29:48 +0000] "POST /connectors HTTP/1.1" 500 1944 "-" "curl/8.7.1" 38 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-01-03 10:37:14,809] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:87)
[2025-01-03 10:37:14,812] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:358)
[2025-01-03 10:37:14,822] INFO Stopped http_8083@2a5abd3c{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:383)
[2025-01-03 10:37:14,822] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:149)
[2025-01-03 10:37:14,829] INFO Stopped o.e.j.s.ServletContextHandler@9f9146d{/,null,STOPPED} (org.eclipse.jetty.server.handler.ContextHandler:1159)
[2025-01-03 10:37:14,830] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:387)
[2025-01-03 10:37:14,830] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder stopping (org.apache.kafka.connect.runtime.distributed.DistributedHerder:851)
[2025-01-03 10:37:14,830] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Stopping connectors and tasks that are still assigned to this worker. (org.apache.kafka.connect.runtime.distributed.DistributedHerder:808)
[2025-01-03 10:37:14,830] INFO [mariadb-connector|worker] Stopping connector mariadb-connector (org.apache.kafka.connect.runtime.Worker:452)
[2025-01-03 10:37:14,831] INFO [mariadb-connector|worker] Scheduled shutdown for WorkerConnector{id=mariadb-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2025-01-03 10:37:14,835] INFO [mariadb-connector|worker] Completed shutdown for WorkerConnector{id=mariadb-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2025-01-03 10:37:14,837] INFO [mariadb-connector|task-0] Stopping task mariadb-connector-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2025-01-03 10:37:15,072] INFO [mariadb-connector|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:434)
[2025-01-03 10:37:15,100] INFO [mariadb-connector|task-0] Finished streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:325)
[2025-01-03 10:37:15,100] INFO [mariadb-connector|task-0] Stopped reading binlog after 39 events, last recorded offset: {ts_sec=1735921498, file=mariadb-bin.000001, pos=52374, server_id=1, event=1} (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:1218)
[2025-01-03 10:37:15,101] INFO [mariadb-connector|task-0] SignalProcessor stopped (io.debezium.pipeline.signal.SignalProcessor:127)
[2025-01-03 10:37:15,103] INFO [mariadb-connector|task-0] Debezium ServiceRegistry stopped. (io.debezium.service.DefaultServiceRegistry:105)
[2025-01-03 10:37:15,106] INFO [mariadb-connector|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:983)
[2025-01-03 10:37:15,107] INFO [mariadb-connector|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:983)
[2025-01-03 10:37:15,108] INFO [mariadb-connector|task-0] [Producer clientId=db_-schemahistory] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 10:37:15,115] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:37:15,116] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:37:15,116] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:37:15,116] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:37:15,117] INFO [mariadb-connector|task-0] App info kafka.producer for db_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:37:15,118] INFO [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 10:37:15,121] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:37:15,121] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:37:15,122] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:37:15,122] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:37:15,122] INFO [mariadb-connector|task-0] App info kafka.producer for connector-producer-mariadb-connector-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:37:15,124] INFO Stopping KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:317)
[2025-01-03 10:37:15,125] INFO [Producer clientId=connect-cluster-statuses] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 10:37:15,128] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:37:15,128] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:37:15,128] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:37:15,128] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:37:15,129] INFO App info kafka.producer for connect-cluster-statuses unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:37:15,129] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 10:37:15,129] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 10:37:15,130] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Node 0 sent an invalid full fetch response with extraIds=(v9oeTD-8QxWCE3CtkK_dUg), response=() (org.apache.kafka.clients.FetchSessionHandler:556)
[2025-01-03 10:37:15,130] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:37:15,130] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:37:15,131] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:37:15,131] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:37:15,132] INFO App info kafka.consumer for connect-cluster-statuses unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:37:15,132] INFO Stopped KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:341)
[2025-01-03 10:37:15,133] INFO Closing KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:407)
[2025-01-03 10:37:15,133] INFO Stopping KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:317)
[2025-01-03 10:37:15,133] INFO [Producer clientId=connect-cluster-configs] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 10:37:15,135] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:37:15,135] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:37:15,135] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:37:15,135] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:37:15,136] INFO App info kafka.producer for connect-cluster-configs unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:37:15,136] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 10:37:15,136] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 10:37:15,248] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:37:15,248] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:37:15,248] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:37:15,249] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:37:15,250] INFO App info kafka.consumer for connect-cluster-configs unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:37:15,251] INFO Stopped KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:341)
[2025-01-03 10:37:15,251] INFO Closed KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:412)
[2025-01-03 10:37:15,251] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:250)
[2025-01-03 10:37:15,252] INFO Stopping KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:261)
[2025-01-03 10:37:15,252] INFO Stopping KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:317)
[2025-01-03 10:37:15,252] INFO [Producer clientId=connect-cluster-offsets] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 10:37:15,254] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:37:15,254] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:37:15,254] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:37:15,254] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:37:15,255] INFO App info kafka.producer for connect-cluster-offsets unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:37:15,255] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 10:37:15,255] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 10:37:15,445] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:37:15,446] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:37:15,446] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:37:15,446] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:37:15,449] INFO App info kafka.consumer for connect-cluster-offsets unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:37:15,449] INFO Stopped KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:341)
[2025-01-03 10:37:15,449] INFO Stopped KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:263)
[2025-01-03 10:37:15,449] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:37:15,449] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:37:15,449] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:37:15,450] INFO App info kafka.connect for 192.168.1.5:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:37:15,450] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:271)
[2025-01-03 10:37:15,451] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Member connect-192.168.1.5:8083-b90e671e-a8d1-4e20-b74d-99c309313aee sending LeaveGroup request to coordinator 192.168.1.5:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1174)
[2025-01-03 10:37:15,452] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1056)
[2025-01-03 10:37:15,453] WARN [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Close timed out with 1 pending requests to coordinator, terminating client connections (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1141)
[2025-01-03 10:37:15,453] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:37:15,453] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:37:15,453] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:37:15,455] INFO App info kafka.connect for connect-192.168.1.5:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:37:15,457] INFO App info kafka.admin.client for connect-cluster-shared-admin unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:37:15,459] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:37:15,459] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:37:15,459] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:37:15,459] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder stopped (org.apache.kafka.connect.runtime.distributed.DistributedHerder:394)
[2025-01-03 10:37:15,460] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder stopped (org.apache.kafka.connect.runtime.distributed.DistributedHerder:858)
[2025-01-03 10:37:15,460] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:92)
[2025-01-03 10:37:30,808] INFO Kafka Connect worker initializing ... (org.apache.kafka.connect.cli.AbstractConnectCli:114)
[2025-01-03 10:37:30,810] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote=true, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 23, 23+37-2369
	jvm.classpath = /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/activation-1.1.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/aopalliance-repackaged-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/argparse4j-0.7.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/audience-annotations-0.12.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/caffeine-2.9.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-beanutils-1.9.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-cli-1.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-collections-3.2.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-digester-2.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-io-2.14.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-lang3-3.12.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-logging-1.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-validator-1.7.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-basic-auth-extension-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-json-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-mirror-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-mirror-client-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-runtime-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-transforms-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/error_prone_annotations-2.10.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-api-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-locator-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-utils-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-annotations-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-core-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-databind-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-dataformat-csv-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-datatype-jdk8-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-jaxrs-base-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-jaxrs-json-provider-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-afterburner-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-jaxb-annotations-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-scala_2.13-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.activation-api-1.2.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.annotation-api-1.3.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.inject-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.validation-api-2.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.xml.bind-api-2.3.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javassist-3.29.2-GA.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.activation-api-1.2.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.annotation-api-1.3.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.servlet-api-3.1.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.ws.rs-api-2.1.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jaxb-api-2.3.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-client-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-common-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-container-servlet-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-container-servlet-core-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-hk2-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-server-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-client-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-continuation-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-http-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-io-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-security-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-server-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-servlet-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-servlets-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-util-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-util-ajax-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jline-3.25.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jopt-simple-5.0.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jose4j-0.9.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jsr305-3.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-clients-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-group-coordinator-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-group-coordinator-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-metadata-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-raft-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-server-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-server-common-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-shell-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-storage-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-storage-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-examples-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-scala_2.13-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-test-utils-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-tools-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-tools-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-transaction-coordinator-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka_2.13-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/lz4-java-1.8.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/maven-artifact-3.9.6.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/metrics-core-2.2.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/metrics-core-4.1.12.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-buffer-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-codec-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-common-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-handler-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-resolver-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-classes-epoll-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-native-epoll-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-native-unix-common-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/opentelemetry-proto-1.0.0-alpha.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/osgi-resource-locator-1.0.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/paranamer-2.8.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/pcollections-4.0.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/plexus-utils-3.5.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/protobuf-java-3.25.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/reflections-0.10.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/reload4j-1.2.25.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/rocksdbjni-7.9.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-collection-compat_2.13-2.10.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-java8-compat_2.13-1.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-library-2.13.14.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-logging_2.13-3.9.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-reflect-2.13.14.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/slf4j-api-1.7.36.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/slf4j-reload4j-1.7.36.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/snappy-java-1.1.10.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/swagger-annotations-2.2.8.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/trogdor-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zookeeper-3.8.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zookeeper-jute-3.8.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zstd-jni-1.5.6-4.jar
	os.spec = Mac OS X, aarch64, 15.0
	os.vcpus = 11
 (org.apache.kafka.connect.runtime.WorkerInfo:72)
[2025-01-03 10:37:30,811] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.AbstractConnectCli:120)
[2025-01-03 10:37:30,821] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:37:30,846] INFO Using up-to-date JsonConverter implementation (io.debezium.converters.CloudEventsConverter:120)
[2025-01-03 10:37:30,922] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:37:30,923] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1 (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:37:30,926] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:37:30,926] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:37:30,934] INFO Using up-to-date JsonConverter implementation (io.debezium.converters.CloudEventsConverter:120)
[2025-01-03 10:37:30,947] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:37:30,949] INFO Loading plugin from: classpath (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:37:30,952] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@5a07e868 (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:37:30,952] INFO Scanning plugins with ServiceLoaderScanner took 131 ms (org.apache.kafka.connect.runtime.isolation.PluginScanner:71)
[2025-01-03 10:37:30,953] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:37:31,091] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:37:31,091] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1 (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:37:31,150] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:37:31,150] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:37:31,294] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:37:31,294] INFO Loading plugin from: classpath (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:37:31,716] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@5a07e868 (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:37:31,716] INFO Scanning plugins with ReflectionScanner took 763 ms (org.apache.kafka.connect.runtime.isolation.PluginScanner:71)
[2025-01-03 10:37:31,717] WARN One or more plugins are missing ServiceLoader manifests may not be usable with plugin.discovery=service_load: [
file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1/	com.mongodb.kafka.connect.MongoSinkConnector	sink	1.14.1
file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1/	com.mongodb.kafka.connect.MongoSourceConnector	source	1.14.1
file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/	io.debezium.connector.mongodb.MongoDbSinkConnector	sink	3.0.5.Final
]
Read the documentation at https://kafka.apache.org/documentation.html#connect_plugindiscovery for instructions on migrating your plugins to take advantage of the performance improvements of service_load mode. To silence this warning, set plugin.discovery=only_scan in the worker config. (org.apache.kafka.connect.runtime.isolation.Plugins:123)
[2025-01-03 10:37:31,718] INFO Added plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,718] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,718] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,718] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,718] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,718] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,718] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,718] INFO Added plugin 'io.debezium.transforms.HeaderToValue' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,718] INFO Added plugin 'io.debezium.transforms.SchemaChangeEventFilter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,718] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,718] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,718] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,718] INFO Added plugin 'io.debezium.transforms.ExtractSchemaToNewRecord' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,718] INFO Added plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,718] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,718] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,718] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,718] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,718] INFO Added plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,719] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,719] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,719] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,719] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,719] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,719] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,719] INFO Added plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,719] INFO Added plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,719] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,719] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,719] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,719] INFO Added plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,719] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,719] INFO Added plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,719] INFO Added plugin 'io.debezium.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,719] INFO Added plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,719] INFO Added plugin 'io.debezium.transforms.partitions.PartitionRouting' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,719] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,719] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,719] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,719] INFO Added plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,719] INFO Added plugin 'io.debezium.transforms.TimezoneConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,719] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,719] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,719] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,719] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,719] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,719] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,720] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,720] INFO Added plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,720] INFO Added plugin 'io.debezium.transforms.ExtractChangedRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,720] INFO Added plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,720] INFO Added plugin 'io.debezium.connector.mongodb.transforms.outbox.MongoEventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,720] INFO Added plugin 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,720] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,720] INFO Added plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,720] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,720] INFO Added plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,720] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,720] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,720] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,720] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,720] INFO Added plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,720] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,720] INFO Added plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,720] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,720] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,720] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,720] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,720] INFO Added plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,720] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,720] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,720] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:37:31,721] INFO Added alias 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,721] INFO Added alias 'ExtractNewDocumentState' to plugin 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,721] INFO Added alias 'JsonConverter' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,721] INFO Added alias 'DebeziumMySql' to plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,721] INFO Added alias 'MongoSourceConnector' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,721] INFO Added alias 'CloudEventsConverter' to plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,721] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,721] INFO Added alias 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,721] INFO Added alias 'DebeziumMySqlConnectRestExtension' to plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,721] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,721] INFO Added alias 'MySql' to plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,721] INFO Added alias 'MongoDbSinkConnector' to plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,721] INFO Added alias 'EnvVar' to plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,721] INFO Added alias 'EnvVarConfigProvider' to plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,721] INFO Added alias 'MongoDb' to plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,722] INFO Added alias 'MirrorCheckpointConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,722] INFO Added alias 'Boolean' to plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,722] INFO Added alias 'MySqlConnector' to plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,722] INFO Added alias 'NoneConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,722] INFO Added alias 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,722] INFO Added alias 'HeaderToValue' to plugin 'io.debezium.transforms.HeaderToValue' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,722] INFO Added alias 'PartitionRouting' to plugin 'io.debezium.transforms.partitions.PartitionRouting' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,722] INFO Added alias 'MongoDbSink' to plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,722] INFO Added alias 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,722] INFO Added alias 'StringConverter' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,722] INFO Added alias 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,722] INFO Added alias 'ByLogicalTableRouter' to plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,722] INFO Added alias 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,722] INFO Added alias 'MongoDbConnector' to plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,722] INFO Added alias 'FileConfigProvider' to plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,722] INFO Added alias 'IntegerConverter' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,722] INFO Added alias 'EventRouter' to plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,722] INFO Added alias 'SchemaChangeEventFilter' to plugin 'io.debezium.transforms.SchemaChangeEventFilter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,722] INFO Added alias 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,722] INFO Added alias 'LongConverter' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,722] INFO Added alias 'DropHeaders' to plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,722] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,722] INFO Added alias 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,722] INFO Added alias 'ExtractSchemaToNewRecord' to plugin 'io.debezium.transforms.ExtractSchemaToNewRecord' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,722] INFO Added alias 'BinaryData' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,722] INFO Added alias 'File' to plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,722] INFO Added alias 'FloatConverter' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,722] INFO Added alias 'DirectoryConfigProvider' to plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,722] INFO Added alias 'ReadToInsertEvent' to plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,722] INFO Added alias 'ShortConverter' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,723] INFO Added alias 'CloudEvents' to plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,723] INFO Added alias 'ActivateTracingSpan' to plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,723] INFO Added alias 'TimezoneConverter' to plugin 'io.debezium.transforms.TimezoneConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,723] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,723] INFO Added alias 'DoubleConverter' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,723] INFO Added alias 'Simple' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,723] INFO Added alias 'BinaryDataConverter' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,723] INFO Added alias 'AllConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,723] INFO Added alias 'ExtractNewRecordState' to plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,723] INFO Added alias 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,723] INFO Added alias 'DebeziumMongoDb' to plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,723] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,723] INFO Added alias 'MirrorHeartbeatConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,723] INFO Added alias 'InsertHeader' to plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,723] INFO Added alias 'Directory' to plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,723] INFO Added alias 'MirrorSourceConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,723] INFO Added alias 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,723] INFO Added alias 'MongoSinkConnector' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,723] INFO Added alias 'PrincipalConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,723] INFO Added alias 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,723] INFO Added alias 'DebeziumMongoDbConnectRestExtension' to plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,723] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,723] INFO Added alias 'BooleanConverter' to plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,723] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,723] INFO Added alias 'MongoEventRouter' to plugin 'io.debezium.connector.mongodb.transforms.outbox.MongoEventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,723] INFO Added alias 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,723] INFO Added alias 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,723] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,723] INFO Added alias 'ExtractChangedRecordState' to plugin 'io.debezium.transforms.ExtractChangedRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,723] INFO Added alias 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,723] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,723] INFO Added alias 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:37:31,726] INFO StandaloneConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	config.providers = []
	connector.client.config.override.policy = All
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = [http://:8083]
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.file.filename = /tmp/connect.offsets
	plugin.discovery = hybrid_warn
	plugin.path = [/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins]
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.standalone.StandaloneConfig:371)
[2025-01-03 10:37:31,727] INFO Creating Kafka admin client (org.apache.kafka.connect.runtime.WorkerConfig:281)
[2025-01-03 10:37:31,728] INFO AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:371)
[2025-01-03 10:37:31,746] INFO These configurations '[offset.flush.interval.ms, key.converter.schemas.enable, offset.storage.file.filename, value.converter.schemas.enable, plugin.path, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2025-01-03 10:37:31,747] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:37:31,747] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:37:31,747] INFO Kafka startTimeMs: 1735922251746 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:37:31,856] INFO Kafka cluster ID: lG5B1M_nTOa7_oILZ1X4hw (org.apache.kafka.connect.runtime.WorkerConfig:298)
[2025-01-03 10:37:31,856] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:37:31,858] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:37:31,858] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:37:31,858] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:37:31,860] INFO PublicConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	listeners = [http://:8083]
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
 (org.apache.kafka.connect.runtime.rest.RestServerConfig$PublicConfig:371)
[2025-01-03 10:37:31,863] INFO Logging initialized @1293ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:170)
[2025-01-03 10:37:31,875] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:125)
[2025-01-03 10:37:31,875] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:196)
[2025-01-03 10:37:31,884] INFO jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 23+37-2369 (org.eclipse.jetty.server.Server:375)
[2025-01-03 10:37:31,891] INFO Started http_8083@498a4d62{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:333)
[2025-01-03 10:37:31,891] INFO Started @1321ms (org.eclipse.jetty.server.Server:415)
[2025-01-03 10:37:31,898] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 10:37:31,898] INFO REST server listening at http://192.168.1.5:8083/, advertising URL http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:216)
[2025-01-03 10:37:31,899] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 10:37:31,899] INFO REST admin endpoints at http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2025-01-03 10:37:31,899] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 10:37:31,899] INFO Setting up All Policy for ConnectorClientConfigOverride. This will allow all client configurations to be overridden (org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy:45)
[2025-01-03 10:37:31,899] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:37:31,903] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:37:31,904] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:37:31,904] INFO Kafka startTimeMs: 1735922251903 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:37:31,905] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:37:31,905] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:37:31,908] INFO Kafka Connect worker initialization took 1100ms (org.apache.kafka.connect.cli.AbstractConnectCli:141)
[2025-01-03 10:37:31,908] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:67)
[2025-01-03 10:37:31,909] INFO Herder starting (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:112)
[2025-01-03 10:37:31,909] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:233)
[2025-01-03 10:37:31,909] INFO Starting FileOffsetBackingStore with file /tmp/connect.offsets (org.apache.kafka.connect.storage.FileOffsetBackingStore:64)
[2025-01-03 10:37:31,910] INFO Worker started (org.apache.kafka.connect.runtime.Worker:243)
[2025-01-03 10:37:31,910] INFO Herder started (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:114)
[2025-01-03 10:37:31,910] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2025-01-03 10:37:31,922] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:238)
[2025-01-03 10:37:31,939] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:334)
[2025-01-03 10:37:31,940] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:339)
[2025-01-03 10:37:31,940] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session:132)
[2025-01-03 10:37:32,065] INFO Started o.e.j.s.ServletContextHandler@2166c48a{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:921)
[2025-01-03 10:37:32,065] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:303)
[2025-01-03 10:37:32,066] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:77)
[2025-01-03 10:37:38,522] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:87)
[2025-01-03 10:37:38,522] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:358)
[2025-01-03 10:37:38,529] INFO Stopped http_8083@498a4d62{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:383)
[2025-01-03 10:37:38,529] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:149)
[2025-01-03 10:37:38,534] INFO Stopped o.e.j.s.ServletContextHandler@2166c48a{/,null,STOPPED} (org.eclipse.jetty.server.handler.ContextHandler:1159)
[2025-01-03 10:37:38,534] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:387)
[2025-01-03 10:37:38,535] INFO Herder stopping (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:119)
[2025-01-03 10:37:38,535] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:250)
[2025-01-03 10:37:38,536] INFO Stopped FileOffsetBackingStore (org.apache.kafka.connect.storage.FileOffsetBackingStore:72)
[2025-01-03 10:37:38,536] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:37:38,536] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:37:38,536] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:37:38,536] INFO App info kafka.connect for 192.168.1.5:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:37:38,536] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:271)
[2025-01-03 10:37:38,538] INFO Herder stopped (org.apache.kafka.connect.runtime.standalone.StandaloneHerder:130)
[2025-01-03 10:37:38,538] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:92)
[2025-01-03 10:38:05,190] INFO Kafka Connect worker initializing ... (org.apache.kafka.connect.cli.AbstractConnectCli:114)
[2025-01-03 10:38:05,192] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote=true, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 23, 23+37-2369
	jvm.classpath = /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/activation-1.1.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/aopalliance-repackaged-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/argparse4j-0.7.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/audience-annotations-0.12.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/caffeine-2.9.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-beanutils-1.9.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-cli-1.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-collections-3.2.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-digester-2.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-io-2.14.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-lang3-3.12.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-logging-1.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-validator-1.7.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-basic-auth-extension-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-json-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-mirror-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-mirror-client-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-runtime-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-transforms-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/error_prone_annotations-2.10.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-api-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-locator-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-utils-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-annotations-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-core-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-databind-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-dataformat-csv-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-datatype-jdk8-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-jaxrs-base-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-jaxrs-json-provider-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-afterburner-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-jaxb-annotations-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-scala_2.13-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.activation-api-1.2.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.annotation-api-1.3.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.inject-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.validation-api-2.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.xml.bind-api-2.3.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javassist-3.29.2-GA.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.activation-api-1.2.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.annotation-api-1.3.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.servlet-api-3.1.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.ws.rs-api-2.1.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jaxb-api-2.3.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-client-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-common-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-container-servlet-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-container-servlet-core-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-hk2-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-server-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-client-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-continuation-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-http-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-io-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-security-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-server-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-servlet-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-servlets-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-util-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-util-ajax-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jline-3.25.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jopt-simple-5.0.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jose4j-0.9.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jsr305-3.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-clients-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-group-coordinator-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-group-coordinator-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-metadata-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-raft-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-server-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-server-common-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-shell-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-storage-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-storage-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-examples-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-scala_2.13-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-test-utils-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-tools-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-tools-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-transaction-coordinator-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka_2.13-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/lz4-java-1.8.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/maven-artifact-3.9.6.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/metrics-core-2.2.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/metrics-core-4.1.12.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-buffer-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-codec-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-common-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-handler-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-resolver-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-classes-epoll-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-native-epoll-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-native-unix-common-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/opentelemetry-proto-1.0.0-alpha.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/osgi-resource-locator-1.0.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/paranamer-2.8.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/pcollections-4.0.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/plexus-utils-3.5.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/protobuf-java-3.25.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/reflections-0.10.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/reload4j-1.2.25.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/rocksdbjni-7.9.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-collection-compat_2.13-2.10.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-java8-compat_2.13-1.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-library-2.13.14.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-logging_2.13-3.9.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-reflect-2.13.14.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/slf4j-api-1.7.36.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/slf4j-reload4j-1.7.36.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/snappy-java-1.1.10.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/swagger-annotations-2.2.8.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/trogdor-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zookeeper-3.8.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zookeeper-jute-3.8.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zstd-jni-1.5.6-4.jar
	os.spec = Mac OS X, aarch64, 15.0
	os.vcpus = 11
 (org.apache.kafka.connect.runtime.WorkerInfo:72)
[2025-01-03 10:38:05,193] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.AbstractConnectCli:120)
[2025-01-03 10:38:05,203] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:38:05,233] INFO Using up-to-date JsonConverter implementation (io.debezium.converters.CloudEventsConverter:120)
[2025-01-03 10:38:05,311] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:38:05,312] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1 (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:38:05,315] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:38:05,315] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:38:05,326] INFO Using up-to-date JsonConverter implementation (io.debezium.converters.CloudEventsConverter:120)
[2025-01-03 10:38:05,340] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:38:05,342] INFO Loading plugin from: classpath (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:38:05,344] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@5a07e868 (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:38:05,345] INFO Scanning plugins with ServiceLoaderScanner took 142 ms (org.apache.kafka.connect.runtime.isolation.PluginScanner:71)
[2025-01-03 10:38:05,345] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:38:05,469] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:38:05,469] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1 (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:38:05,529] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:38:05,530] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:38:05,689] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:38:05,689] INFO Loading plugin from: classpath (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:38:06,121] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@5a07e868 (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:38:06,121] INFO Scanning plugins with ReflectionScanner took 776 ms (org.apache.kafka.connect.runtime.isolation.PluginScanner:71)
[2025-01-03 10:38:06,122] WARN One or more plugins are missing ServiceLoader manifests may not be usable with plugin.discovery=service_load: [
file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1/	com.mongodb.kafka.connect.MongoSinkConnector	sink	1.14.1
file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1/	com.mongodb.kafka.connect.MongoSourceConnector	source	1.14.1
file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/	io.debezium.connector.mongodb.MongoDbSinkConnector	sink	3.0.5.Final
]
Read the documentation at https://kafka.apache.org/documentation.html#connect_plugindiscovery for instructions on migrating your plugins to take advantage of the performance improvements of service_load mode. To silence this warning, set plugin.discovery=only_scan in the worker config. (org.apache.kafka.connect.runtime.isolation.Plugins:123)
[2025-01-03 10:38:06,123] INFO Added plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,123] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,123] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,123] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,123] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,123] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,123] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,123] INFO Added plugin 'io.debezium.transforms.HeaderToValue' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,123] INFO Added plugin 'io.debezium.transforms.SchemaChangeEventFilter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,123] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,123] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,123] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,123] INFO Added plugin 'io.debezium.transforms.ExtractSchemaToNewRecord' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,123] INFO Added plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,123] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,123] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,123] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,123] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,123] INFO Added plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,123] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,123] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,123] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,123] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,124] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,124] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,124] INFO Added plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,124] INFO Added plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,124] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,124] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,124] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,124] INFO Added plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,124] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,124] INFO Added plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,124] INFO Added plugin 'io.debezium.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,124] INFO Added plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,124] INFO Added plugin 'io.debezium.transforms.partitions.PartitionRouting' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,124] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,124] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,124] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,124] INFO Added plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,124] INFO Added plugin 'io.debezium.transforms.TimezoneConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,124] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,124] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,124] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,124] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,124] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,124] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,124] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,124] INFO Added plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,124] INFO Added plugin 'io.debezium.transforms.ExtractChangedRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,124] INFO Added plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,124] INFO Added plugin 'io.debezium.connector.mongodb.transforms.outbox.MongoEventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,124] INFO Added plugin 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,125] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,125] INFO Added plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,125] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,125] INFO Added plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,125] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,125] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,125] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,125] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,125] INFO Added plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,125] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,125] INFO Added plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,125] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,125] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,125] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,125] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,125] INFO Added plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,125] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,125] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,125] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:38:06,126] INFO Added alias 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,126] INFO Added alias 'ExtractNewDocumentState' to plugin 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,126] INFO Added alias 'JsonConverter' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,126] INFO Added alias 'DebeziumMySql' to plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,126] INFO Added alias 'MongoSourceConnector' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,126] INFO Added alias 'CloudEventsConverter' to plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,126] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,126] INFO Added alias 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,126] INFO Added alias 'DebeziumMySqlConnectRestExtension' to plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,126] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,126] INFO Added alias 'MySql' to plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,126] INFO Added alias 'MongoDbSinkConnector' to plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,126] INFO Added alias 'EnvVar' to plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,126] INFO Added alias 'EnvVarConfigProvider' to plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,126] INFO Added alias 'MongoDb' to plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,126] INFO Added alias 'MirrorCheckpointConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,126] INFO Added alias 'Boolean' to plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,126] INFO Added alias 'MySqlConnector' to plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,126] INFO Added alias 'NoneConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,126] INFO Added alias 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,126] INFO Added alias 'HeaderToValue' to plugin 'io.debezium.transforms.HeaderToValue' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,126] INFO Added alias 'PartitionRouting' to plugin 'io.debezium.transforms.partitions.PartitionRouting' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,126] INFO Added alias 'MongoDbSink' to plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,126] INFO Added alias 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,126] INFO Added alias 'StringConverter' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,126] INFO Added alias 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,126] INFO Added alias 'ByLogicalTableRouter' to plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,127] INFO Added alias 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,127] INFO Added alias 'MongoDbConnector' to plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,127] INFO Added alias 'FileConfigProvider' to plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,127] INFO Added alias 'IntegerConverter' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,127] INFO Added alias 'EventRouter' to plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,127] INFO Added alias 'SchemaChangeEventFilter' to plugin 'io.debezium.transforms.SchemaChangeEventFilter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,127] INFO Added alias 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,127] INFO Added alias 'LongConverter' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,127] INFO Added alias 'DropHeaders' to plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,127] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,127] INFO Added alias 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,127] INFO Added alias 'ExtractSchemaToNewRecord' to plugin 'io.debezium.transforms.ExtractSchemaToNewRecord' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,127] INFO Added alias 'BinaryData' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,127] INFO Added alias 'File' to plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,127] INFO Added alias 'FloatConverter' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,127] INFO Added alias 'DirectoryConfigProvider' to plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,127] INFO Added alias 'ReadToInsertEvent' to plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,127] INFO Added alias 'ShortConverter' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,127] INFO Added alias 'CloudEvents' to plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,127] INFO Added alias 'ActivateTracingSpan' to plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,127] INFO Added alias 'TimezoneConverter' to plugin 'io.debezium.transforms.TimezoneConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,127] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,127] INFO Added alias 'DoubleConverter' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,127] INFO Added alias 'Simple' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,127] INFO Added alias 'BinaryDataConverter' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,127] INFO Added alias 'AllConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,127] INFO Added alias 'ExtractNewRecordState' to plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,127] INFO Added alias 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,127] INFO Added alias 'DebeziumMongoDb' to plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,127] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,127] INFO Added alias 'MirrorHeartbeatConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,128] INFO Added alias 'InsertHeader' to plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,128] INFO Added alias 'Directory' to plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,128] INFO Added alias 'MirrorSourceConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,128] INFO Added alias 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,128] INFO Added alias 'MongoSinkConnector' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,128] INFO Added alias 'PrincipalConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,128] INFO Added alias 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,128] INFO Added alias 'DebeziumMongoDbConnectRestExtension' to plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,128] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,128] INFO Added alias 'BooleanConverter' to plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,128] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,128] INFO Added alias 'MongoEventRouter' to plugin 'io.debezium.connector.mongodb.transforms.outbox.MongoEventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,128] INFO Added alias 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,128] INFO Added alias 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,128] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,128] INFO Added alias 'ExtractChangedRecordState' to plugin 'io.debezium.transforms.ExtractChangedRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,128] INFO Added alias 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,128] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,128] INFO Added alias 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:38:06,144] INFO DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	config.providers = []
	config.storage.replication.factor = 1
	config.storage.topic = connect-configs
	connect.protocol = sessioned
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = All
	exactly.once.source.support = disabled
	group.id = connect-cluster
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	heartbeat.interval.ms = 3000
	inter.worker.key.generation.algorithm = HmacSHA256
	inter.worker.key.size = null
	inter.worker.key.ttl.ms = 3600000
	inter.worker.signature.algorithm = HmacSHA256
	inter.worker.verification.algorithms = [HmacSHA256]
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = [http://:8083]
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 1
	offset.storage.topic = connect-offsets
	plugin.discovery = hybrid_warn
	plugin.path = [/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins]
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	status.storage.partitions = 5
	status.storage.replication.factor = 1
	status.storage.topic = connect-status
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000
 (org.apache.kafka.connect.runtime.distributed.DistributedConfig:371)
[2025-01-03 10:38:06,144] INFO Creating Kafka admin client (org.apache.kafka.connect.runtime.WorkerConfig:281)
[2025-01-03 10:38:06,145] INFO AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:371)
[2025-01-03 10:38:06,164] INFO These configurations '[config.storage.topic, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2025-01-03 10:38:06,165] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:38:06,165] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:38:06,165] INFO Kafka startTimeMs: 1735922286165 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:38:06,278] INFO Kafka cluster ID: lG5B1M_nTOa7_oILZ1X4hw (org.apache.kafka.connect.runtime.WorkerConfig:298)
[2025-01-03 10:38:06,278] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:38:06,280] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:38:06,280] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:38:06,281] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:38:06,282] INFO PublicConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	listeners = [http://:8083]
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
 (org.apache.kafka.connect.runtime.rest.RestServerConfig$PublicConfig:371)
[2025-01-03 10:38:06,286] INFO Logging initialized @1328ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:170)
[2025-01-03 10:38:06,298] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:125)
[2025-01-03 10:38:06,298] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:196)
[2025-01-03 10:38:06,307] INFO jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 23+37-2369 (org.eclipse.jetty.server.Server:375)
[2025-01-03 10:38:06,314] INFO Started http_8083@1a7e799e{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:333)
[2025-01-03 10:38:06,315] INFO Started @1357ms (org.eclipse.jetty.server.Server:415)
[2025-01-03 10:38:06,322] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 10:38:06,322] INFO REST server listening at http://192.168.1.5:8083/, advertising URL http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:216)
[2025-01-03 10:38:06,322] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 10:38:06,322] INFO REST admin endpoints at http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2025-01-03 10:38:06,322] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 10:38:06,322] INFO Setting up All Policy for ConnectorClientConfigOverride. This will allow all client configurations to be overridden (org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy:45)
[2025-01-03 10:38:06,324] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:38:06,331] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:38:06,331] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:38:06,331] INFO Kafka startTimeMs: 1735922286331 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:38:06,333] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:38:06,333] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:38:06,340] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 10:38:06,352] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:38:06,352] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:38:06,352] INFO Kafka startTimeMs: 1735922286352 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:38:06,353] INFO Kafka Connect worker initialization took 1162ms (org.apache.kafka.connect.cli.AbstractConnectCli:141)
[2025-01-03 10:38:06,353] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:67)
[2025-01-03 10:38:06,354] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2025-01-03 10:38:06,354] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder starting (org.apache.kafka.connect.runtime.distributed.DistributedHerder:375)
[2025-01-03 10:38:06,354] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:233)
[2025-01-03 10:38:06,354] INFO Starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:232)
[2025-01-03 10:38:06,355] INFO Starting KafkaBasedLog with topic connect-offsets reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog:254)
[2025-01-03 10:38:06,355] INFO AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-shared-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:371)
[2025-01-03 10:38:06,356] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2025-01-03 10:38:06,356] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:38:06,356] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:38:06,356] INFO Kafka startTimeMs: 1735922286356 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:38:06,365] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:238)
[2025-01-03 10:38:06,375] INFO ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-offsets
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 10:38:06,378] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:334)
[2025-01-03 10:38:06,378] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:339)
[2025-01-03 10:38:06,378] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session:132)
[2025-01-03 10:38:06,383] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:38:06,390] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 10:38:06,390] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:38:06,390] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:38:06,390] INFO Kafka startTimeMs: 1735922286390 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:38:06,392] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-offsets
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 10:38:06,392] INFO [Producer clientId=connect-cluster-offsets] Cluster ID: lG5B1M_nTOa7_oILZ1X4hw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:38:06,396] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:38:06,407] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2025-01-03 10:38:06,407] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:38:06,407] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:38:06,407] INFO Kafka startTimeMs: 1735922286407 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:38:06,410] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Cluster ID: lG5B1M_nTOa7_oILZ1X4hw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:38:06,411] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Assigned to partition(s): connect-offsets-0, connect-offsets-5, connect-offsets-10, connect-offsets-20, connect-offsets-15, connect-offsets-9, connect-offsets-11, connect-offsets-4, connect-offsets-16, connect-offsets-17, connect-offsets-3, connect-offsets-24, connect-offsets-23, connect-offsets-13, connect-offsets-18, connect-offsets-22, connect-offsets-8, connect-offsets-2, connect-offsets-12, connect-offsets-19, connect-offsets-14, connect-offsets-1, connect-offsets-6, connect-offsets-7, connect-offsets-21 (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:579)
[2025-01-03 10:38:06,413] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:38:06,413] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-5 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:38:06,413] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-10 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:38:06,413] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-20 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:38:06,413] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-15 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:38:06,413] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-9 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:38:06,413] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-11 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:38:06,413] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:38:06,413] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-16 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:38:06,413] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-17 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:38:06,413] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:38:06,413] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-24 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:38:06,413] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-23 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:38:06,413] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-13 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:38:06,413] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-18 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:38:06,413] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-22 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:38:06,413] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-8 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:38:06,413] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:38:06,413] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-12 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:38:06,413] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-19 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:38:06,413] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-14 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:38:06,413] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:38:06,413] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-6 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:38:06,413] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-7 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:38:06,413] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-21 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:38:06,436] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:38:06,437] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:38:06,437] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:38:06,437] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:38:06,437] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:38:06,437] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:38:06,437] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:38:06,437] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:38:06,437] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:38:06,437] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:38:06,437] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:38:06,437] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:38:06,437] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:38:06,437] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:38:06,437] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:38:06,437] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:38:06,437] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:38:06,437] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:38:06,437] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:38:06,437] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:38:06,437] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:38:06,437] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:38:06,437] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:38:06,437] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:38:06,437] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:38:06,460] INFO Finished reading KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:311)
[2025-01-03 10:38:06,460] INFO Started KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:313)
[2025-01-03 10:38:06,460] INFO Finished reading offsets topic and starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:249)
[2025-01-03 10:38:06,461] INFO Worker started (org.apache.kafka.connect.runtime.Worker:243)
[2025-01-03 10:38:06,461] INFO Starting KafkaBasedLog with topic connect-status reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog:254)
[2025-01-03 10:38:06,464] INFO ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-statuses
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 10:38:06,464] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:38:06,466] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 10:38:06,466] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:38:06,466] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:38:06,466] INFO Kafka startTimeMs: 1735922286466 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:38:06,466] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-statuses
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 10:38:06,467] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:38:06,468] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2025-01-03 10:38:06,468] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:38:06,468] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:38:06,468] INFO Kafka startTimeMs: 1735922286468 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:38:06,469] INFO [Producer clientId=connect-cluster-statuses] Cluster ID: lG5B1M_nTOa7_oILZ1X4hw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:38:06,471] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Cluster ID: lG5B1M_nTOa7_oILZ1X4hw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:38:06,472] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Assigned to partition(s): connect-status-0, connect-status-4, connect-status-1, connect-status-2, connect-status-3 (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:579)
[2025-01-03 10:38:06,472] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:38:06,472] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:38:06,472] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:38:06,472] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:38:06,472] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:38:06,479] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:38:06,479] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:38:06,479] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:38:06,479] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:38:06,479] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:38:06,483] INFO Finished reading KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:311)
[2025-01-03 10:38:06,483] INFO Started KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:313)
[2025-01-03 10:38:06,484] INFO Starting KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:378)
[2025-01-03 10:38:06,484] INFO Starting KafkaBasedLog with topic connect-configs reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog:254)
[2025-01-03 10:38:06,489] INFO ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-configs
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 10:38:06,489] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:38:06,490] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 10:38:06,490] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:38:06,490] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:38:06,490] INFO Kafka startTimeMs: 1735922286490 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:38:06,491] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-configs
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 10:38:06,491] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:38:06,492] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2025-01-03 10:38:06,492] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:38:06,492] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:38:06,492] INFO Kafka startTimeMs: 1735922286492 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:38:06,493] INFO [Producer clientId=connect-cluster-configs] Cluster ID: lG5B1M_nTOa7_oILZ1X4hw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:38:06,495] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Cluster ID: lG5B1M_nTOa7_oILZ1X4hw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:38:06,495] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Assigned to partition(s): connect-configs-0 (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:579)
[2025-01-03 10:38:06,495] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Seeking to earliest offset of partition connect-configs-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:38:06,501] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Resetting offset for partition connect-configs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:38:06,503] INFO Finished reading KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:311)
[2025-01-03 10:38:06,503] INFO Started KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:313)
[2025-01-03 10:38:06,503] INFO Started KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:402)
[2025-01-03 10:38:06,506] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Cluster ID: lG5B1M_nTOa7_oILZ1X4hw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:38:06,507] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Discovered group coordinator 192.168.1.5:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:937)
[2025-01-03 10:38:06,508] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2025-01-03 10:38:06,508] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 10:38:06,511] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 10:38:06,512] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=5, memberId='connect-192.168.1.5:8083-d34d4154-a563-45ec-a8c7-a450376e9962', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2025-01-03 10:38:06,519] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=5, memberId='connect-192.168.1.5:8083-d34d4154-a563-45ec-a8c7-a450376e9962', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2025-01-03 10:38:06,519] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Joined group at generation 5 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.1.5:8083-d34d4154-a563-45ec-a8c7-a450376e9962', leaderUrl='http://192.168.1.5:8083/', offset=4, connectorIds=[mariadb-connector], taskIds=[mariadb-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2025-01-03 10:38:06,520] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder started (org.apache.kafka.connect.runtime.distributed.DistributedHerder:387)
[2025-01-03 10:38:06,520] WARN [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Catching up to assignment's config offset. (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1811)
[2025-01-03 10:38:06,520] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Current config state offset -1 is behind group assignment 4, reading to end of config log (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1884)
[2025-01-03 10:38:06,521] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished reading to end of log and updated config snapshot, new config log offset: 4 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1911)
[2025-01-03 10:38:06,521] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 4 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2025-01-03 10:38:06,522] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connector mariadb-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2025-01-03 10:38:06,522] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting task mariadb-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2025-01-03 10:38:06,523] INFO [mariadb-connector|worker] Creating connector mariadb-connector of type io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:313)
[2025-01-03 10:38:06,523] INFO Started o.e.j.s.ServletContextHandler@5402612e{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:921)
[2025-01-03 10:38:06,523] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:303)
[2025-01-03 10:38:06,523] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:77)
[2025-01-03 10:38:06,524] INFO [mariadb-connector|task-0] Creating task mariadb-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2025-01-03 10:38:06,524] INFO [mariadb-connector|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2025-01-03 10:38:06,524] INFO [mariadb-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mariadb-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2025-01-03 10:38:06,524] INFO [mariadb-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mariadb-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 10:38:06,524] INFO [mariadb-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 10:38:06,526] INFO [mariadb-connector|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.mysql.MySqlConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2025-01-03 10:38:06,526] INFO [mariadb-connector|worker] Instantiated connector mariadb-connector with version 3.0.5.Final of type class io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:335)
[2025-01-03 10:38:06,526] INFO [mariadb-connector|worker] Finished creating connector mariadb-connector (org.apache.kafka.connect.runtime.Worker:356)
[2025-01-03 10:38:06,527] INFO [mariadb-connector|task-0] Instantiated task mariadb-connector-0 with version 3.0.5.Final of type io.debezium.connector.mysql.MySqlConnectorTask (org.apache.kafka.connect.runtime.Worker:665)
[2025-01-03 10:38:06,527] INFO [mariadb-connector|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:38:06,527] INFO [mariadb-connector|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mariadb-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:678)
[2025-01-03 10:38:06,527] INFO [mariadb-connector|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:38:06,527] INFO [mariadb-connector|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mariadb-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:684)
[2025-01-03 10:38:06,527] INFO [mariadb-connector|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mariadb-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2025-01-03 10:38:06,528] INFO [mariadb-connector|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:1795)
[2025-01-03 10:38:06,528] INFO [mariadb-connector|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2025-01-03 10:38:06,528] INFO [mariadb-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 10:38:06,529] INFO [mariadb-connector|task-0] ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-mariadb-connector-0
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 10:38:06,529] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:38:06,530] INFO [mariadb-connector|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 10:38:06,530] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:38:06,530] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:38:06,530] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735922286530 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:38:06,533] INFO [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] Cluster ID: lG5B1M_nTOa7_oILZ1X4hw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:38:06,534] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2025-01-03 10:38:06,535] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2025-01-03 10:38:06,535] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 10:38:06,543] INFO [mariadb-connector|task-0] Starting MySqlConnectorTask with configuration:
   connector.class = io.debezium.connector.mysql.MySqlConnector
   database.user = root
   database.server.id = 184054
   database.history.kafka.bootstrap.servers = localhost:9092
   database.history.kafka.topic = db.history.leafy_factory
   database.server.name = leafy_factory
   schema.history.internal.kafka.bootstrap.servers = localhost:9092
   database.port = 3306
   include.schema.changes = false
   topic.prefix = db_
   schema.history.internal.kafka.topic = db.history.internal
   task.class = io.debezium.connector.mysql.MySqlConnectorTask
   database.hostname = localhost
   database.password = ********
   name = mariadb-connector
   database.include.list = leafy_factory
 (io.debezium.connector.common.BaseSourceTask:250)
[2025-01-03 10:38:06,545] INFO [mariadb-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mysql.MySqlSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2025-01-03 10:38:06,551] INFO [mariadb-connector|task-0] Loading the custom topic naming strategy plugin: io.debezium.schema.DefaultTopicNamingStrategy (io.debezium.config.CommonConnectorConfig:1401)
[2025-01-03 10:38:06,959] INFO [mariadb-connector|task-0] Using 'SHOW MASTER STATUS' to get binary log status (io.debezium.connector.mysql.jdbc.MySqlConnection:41)
[2025-01-03 10:38:06,963] INFO [mariadb-connector|task-0] Found previous partition offset BinlogPartition{serverName='db_'} io.debezium.connector.mysql.MySqlPartition@183c0: {file=mariadb-bin.000001, pos=0, row=1} (io.debezium.connector.common.BaseSourceTask:529)
[2025-01-03 10:38:06,988] INFO [mariadb-connector|task-0] KafkaSchemaHistory Consumer config: {key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, enable.auto.commit=false, group.id=db_-schemahistory, bootstrap.servers=localhost:9092, fetch.min.bytes=1, session.timeout.ms=10000, auto.offset.reset=earliest, client.id=db_-schemahistory} (io.debezium.storage.kafka.history.KafkaSchemaHistory:245)
[2025-01-03 10:38:06,988] INFO [mariadb-connector|task-0] KafkaSchemaHistory Producer config: {retries=1, value.serializer=org.apache.kafka.common.serialization.StringSerializer, acks=1, batch.size=32768, max.block.ms=10000, bootstrap.servers=localhost:9092, buffer.memory=1048576, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=db_-schemahistory, linger.ms=0} (io.debezium.storage.kafka.history.KafkaSchemaHistory:246)
[2025-01-03 10:38:06,989] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = db-history-config-check (io.debezium.util.Threads:270)
[2025-01-03 10:38:06,990] INFO [mariadb-connector|task-0] Idempotence will be disabled because acks is set to 1, not set to 'all'. (org.apache.kafka.clients.producer.ProducerConfig:587)
[2025-01-03 10:38:06,990] INFO [mariadb-connector|task-0] ProducerConfig values: 
	acks = 1
	auto.include.jmx.reporter = true
	batch.size = 32768
	bootstrap.servers = [localhost:9092]
	buffer.memory = 1048576
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 10:38:06,990] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:38:06,992] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:38:06,992] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:38:06,992] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735922286992 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:38:06,994] INFO [mariadb-connector|task-0] [Producer clientId=db_-schemahistory] Cluster ID: lG5B1M_nTOa7_oILZ1X4hw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:38:07,057] INFO [mariadb-connector|task-0] Using 'SHOW MASTER STATUS' to get binary log status (io.debezium.connector.mysql.jdbc.MySqlConnection:41)
[2025-01-03 10:38:07,065] INFO [mariadb-connector|task-0] Closing connection before starting schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:123)
[2025-01-03 10:38:07,066] INFO [mariadb-connector|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:983)
[2025-01-03 10:38:07,067] INFO [mariadb-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = db_-schemahistory
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 10:38:07,067] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:38:07,069] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:38:07,069] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:38:07,069] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735922287069 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:38:07,070] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Cluster ID: lG5B1M_nTOa7_oILZ1X4hw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:38:07,072] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 10:38:07,072] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 10:38:07,072] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:38:07,073] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:38:07,073] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:38:07,073] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:38:07,074] INFO [mariadb-connector|task-0] App info kafka.consumer for db_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:38:07,074] INFO [mariadb-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = db_-schemahistory
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 10:38:07,074] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:38:07,075] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:38:07,075] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:38:07,075] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735922287075 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:38:07,076] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-db-history-config-check (io.debezium.util.Threads:287)
[2025-01-03 10:38:07,076] INFO [mariadb-connector|task-0] AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory-topic-check
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:371)
[2025-01-03 10:38:07,076] INFO [mariadb-connector|task-0] These configurations '[value.serializer, acks, batch.size, max.block.ms, buffer.memory, key.serializer, linger.ms]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2025-01-03 10:38:07,076] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:38:07,077] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:38:07,077] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735922287076 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:38:07,078] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Cluster ID: lG5B1M_nTOa7_oILZ1X4hw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:38:07,084] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 10:38:07,084] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 10:38:07,084] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:38:07,084] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:38:07,084] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:38:07,084] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:38:07,085] INFO [mariadb-connector|task-0] App info kafka.consumer for db_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:38:07,086] INFO [mariadb-connector|task-0] Get all known binlogs (io.debezium.connector.binlog.jdbc.BinlogConnectorConnection:157)
[2025-01-03 10:38:07,091] INFO [mariadb-connector|task-0] Database schema history topic 'db.history.internal' has correct settings (io.debezium.storage.kafka.history.KafkaSchemaHistory:473)
[2025-01-03 10:38:07,091] INFO [mariadb-connector|task-0] App info kafka.admin.client for db_-schemahistory-topic-check unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:38:07,092] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:38:07,092] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:38:07,092] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:38:07,117] INFO [mariadb-connector|task-0] Server has the binlog file 'mariadb-bin.000001' required by the connector (io.debezium.connector.binlog.jdbc.BinlogConnectorConnection:399)
[2025-01-03 10:38:07,118] INFO [mariadb-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = db_-schemahistory
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 10:38:07,118] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:38:07,119] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:38:07,119] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:38:07,119] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735922287119 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:38:07,121] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Cluster ID: lG5B1M_nTOa7_oILZ1X4hw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:38:07,121] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 10:38:07,121] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 10:38:07,122] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:38:07,122] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:38:07,122] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:38:07,122] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:38:07,122] INFO [mariadb-connector|task-0] App info kafka.consumer for db_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:38:07,122] INFO [mariadb-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = db_-schemahistory
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 10:38:07,123] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:38:07,124] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:38:07,124] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:38:07,124] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735922287124 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:38:07,125] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Cluster ID: lG5B1M_nTOa7_oILZ1X4hw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:38:07,128] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 10:38:07,128] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 10:38:07,129] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:38:07,129] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:38:07,129] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:38:07,129] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:38:07,129] INFO [mariadb-connector|task-0] App info kafka.consumer for db_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:38:07,130] INFO [mariadb-connector|task-0] Started database schema history recovery (io.debezium.relational.history.SchemaHistoryMetrics:115)
[2025-01-03 10:38:07,132] INFO [mariadb-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = db_-schemahistory
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 10:38:07,132] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:38:07,133] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:38:07,133] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:38:07,133] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735922287133 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:38:07,133] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Subscribed to topic(s): db.history.internal (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:481)
[2025-01-03 10:38:07,135] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Cluster ID: lG5B1M_nTOa7_oILZ1X4hw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:38:07,138] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Discovered group coordinator 192.168.1.5:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:937)
[2025-01-03 10:38:07,139] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:605)
[2025-01-03 10:38:07,141] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Request joining group due to: need to re-join with the given member-id: db_-schemahistory-9b40f660-6f06-4866-8c8e-ede00428c3e6 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 10:38:07,142] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:605)
[2025-01-03 10:38:07,142] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Successfully joined group with generation Generation{generationId=1, memberId='db_-schemahistory-9b40f660-6f06-4866-8c8e-ede00428c3e6', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:666)
[2025-01-03 10:38:07,144] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Finished assignment for group at generation 1: {db_-schemahistory-9b40f660-6f06-4866-8c8e-ede00428c3e6=Assignment(partitions=[db.history.internal-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:664)
[2025-01-03 10:38:07,146] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Successfully synced group in generation Generation{generationId=1, memberId='db_-schemahistory-9b40f660-6f06-4866-8c8e-ede00428c3e6', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:843)
[2025-01-03 10:38:07,146] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Notifying assignor about the new Assignment(partitions=[db.history.internal-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:324)
[2025-01-03 10:38:07,146] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Adding newly assigned partitions: db.history.internal-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:58)
[2025-01-03 10:38:07,147] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Found no committed offset for partition db.history.internal-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1508)
[2025-01-03 10:38:07,148] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Resetting offset for partition db.history.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:38:07,155] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Revoke previously assigned partitions db.history.internal-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:80)
[2025-01-03 10:38:07,156] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Member db_-schemahistory-9b40f660-6f06-4866-8c8e-ede00428c3e6 sending LeaveGroup request to coordinator 192.168.1.5:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1174)
[2025-01-03 10:38:07,156] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 10:38:07,156] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 10:38:07,654] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:38:07,654] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:38:07,654] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:38:07,654] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:38:07,658] INFO [mariadb-connector|task-0] App info kafka.consumer for db_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:38:07,658] INFO [mariadb-connector|task-0] Finished database schema history recovery of 24 change(s) in 528 ms (io.debezium.relational.history.SchemaHistoryMetrics:121)
[2025-01-03 10:38:07,660] INFO [mariadb-connector|task-0] Reconnecting after finishing schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:136)
[2025-01-03 10:38:07,684] INFO [mariadb-connector|task-0] Found previous offset BinlogOffsetContext{sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=BinlogSourceInfo{currentGtid='null', currentBinlogFilename='mariadb-bin.000001', currentBinlogPosition=0, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery='null', tableIds=[], databaseName='null'}, snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet='null', currentGtidSet='null', restartBinlogFilename='mariadb-bin.000001', restartBinlogPosition=0, restartRowsToSkip=1, restartEventsToSkip=0, currentEventLengthInBytes=0, inTransaction=false, transactionId='null', incrementalSnapshotContext=IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]} (io.debezium.connector.mysql.MySqlConnectorTask:150)
[2025-01-03 10:38:07,700] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = SignalProcessor (io.debezium.util.Threads:270)
[2025-01-03 10:38:07,711] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = change-event-source-coordinator (io.debezium.util.Threads:270)
[2025-01-03 10:38:07,711] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = blocking-snapshot (io.debezium.util.Threads:270)
[2025-01-03 10:38:07,713] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-change-event-source-coordinator (io.debezium.util.Threads:287)
[2025-01-03 10:38:07,713] INFO [mariadb-connector|task-0] WorkerSourceTask{id=mariadb-connector-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.AbstractWorkerSourceTask:280)
[2025-01-03 10:38:07,715] INFO [mariadb-connector|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:137)
[2025-01-03 10:38:07,715] INFO [mariadb-connector|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:140)
[2025-01-03 10:38:07,719] INFO [mariadb-connector|task-0] A previous offset indicating a completed snapshot has been found. (io.debezium.relational.RelationalSnapshotChangeEventSource:275)
[2025-01-03 10:38:07,721] INFO [mariadb-connector|task-0] Snapshot ended with SnapshotResult [status=SKIPPED, offset=BinlogOffsetContext{sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=BinlogSourceInfo{currentGtid='null', currentBinlogFilename='mariadb-bin.000001', currentBinlogPosition=0, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery='null', tableIds=[], databaseName='null'}, snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet='null', currentGtidSet='null', restartBinlogFilename='mariadb-bin.000001', restartBinlogPosition=0, restartRowsToSkip=1, restartEventsToSkip=0, currentEventLengthInBytes=0, inTransaction=false, transactionId='null', incrementalSnapshotContext=IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]}] (io.debezium.pipeline.ChangeEventSourceCoordinator:298)
[2025-01-03 10:38:07,724] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = binlog-client (io.debezium.util.Threads:270)
[2025-01-03 10:38:07,727] INFO [mariadb-connector|task-0] Enable ssl PREFERRED mode for connector db_ (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:1289)
[2025-01-03 10:38:07,731] INFO [mariadb-connector|task-0] SignalProcessor started. Scheduling it every 5000ms (io.debezium.pipeline.signal.SignalProcessor:105)
[2025-01-03 10:38:07,732] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-SignalProcessor (io.debezium.util.Threads:287)
[2025-01-03 10:38:07,732] INFO [mariadb-connector|task-0] Starting streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:323)
[2025-01-03 10:38:07,732] WARN [mariadb-connector|task-0] After applying the include/exclude list filters, no changes will be captured. Please check your configuration! (io.debezium.relational.RelationalDatabaseSchema:72)
[2025-01-03 10:38:07,733] INFO [mariadb-connector|task-0] Skip 0 events on streaming start (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:278)
[2025-01-03 10:38:07,733] INFO [mariadb-connector|task-0] Skip 1 rows on streaming start (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:282)
[2025-01-03 10:38:07,734] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-binlog-client (io.debezium.util.Threads:287)
[2025-01-03 10:38:07,735] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-binlog-client (io.debezium.util.Threads:287)
[2025-01-03 10:38:07,771] INFO [mariadb-connector|task-0] Connected to binlog at localhost:3306, starting at BinlogOffsetContext{sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=BinlogSourceInfo{currentGtid='null', currentBinlogFilename='mariadb-bin.000001', currentBinlogPosition=0, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery='null', tableIds=[], databaseName='null'}, snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet='null', currentGtidSet='null', restartBinlogFilename='mariadb-bin.000001', restartBinlogPosition=0, restartRowsToSkip=1, restartEventsToSkip=0, currentEventLengthInBytes=0, inTransaction=false, transactionId='null', incrementalSnapshotContext=IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]} (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:1232)
[2025-01-03 10:38:07,771] INFO [mariadb-connector|task-0] Waiting for keepalive thread to start (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:299)
[2025-01-03 10:38:07,771] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-binlog-client (io.debezium.util.Threads:287)
[2025-01-03 10:38:07,876] INFO [mariadb-connector|task-0] Keepalive thread is running (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:306)
[2025-01-03 10:38:08,010] ERROR [mariadb-connector|task-0] Encountered change event 'Event{header=EventHeaderV4{timestamp=1735846965000, eventType=TABLE_MAP, serverId=1, headerLength=19, dataLength=69, nextPosition=0, flags=0}, data=TableMapEventData{tableId=26, database='leafy_factory', table='raw_materials', columnTypes=3, 15, 15, 15, 15, 3, 15, 15, -10, columnMetadata=0, 400, 400, 400, 400, 0, 400, 400, 522, columnNullability={}, eventMetadata=null}}' at offset {file=mariadb-bin.000001, pos=0, server_id=1} for table leafy_factory.raw_materials whose schema isn't known to this connector. One possible cause is an incomplete database schema history topic. Take a new snapshot in this case.
Use the mysqlbinlog tool to view the problematic event: mysqlbinlog --start-position=-88 --stop-position=0 --verbose mariadb-bin.000001 (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:991)
[2025-01-03 10:38:08,013] ERROR [mariadb-connector|task-0] Error during binlog processing. Last offset stored = {file=mariadb-bin.000001, pos=372, server_id=1, event=1}, binlog reader near position = mariadb-bin.000001/372 (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:1157)
[2025-01-03 10:38:08,014] ERROR [mariadb-connector|task-0] Producer failure (io.debezium.pipeline.ErrorHandler:52)
io.debezium.DebeziumException: Error processing binlog event
	at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.handleEvent(BinlogStreamingChangeEventSource.java:591)
	at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.lambda$execute$17(BinlogStreamingChangeEventSource.java:209)
	at com.github.shyiko.mysql.binlog.BinaryLogClient.notifyEventListeners(BinaryLogClient.java:1281)
	at com.github.shyiko.mysql.binlog.BinaryLogClient.listenForEventPackets(BinaryLogClient.java:1103)
	at com.github.shyiko.mysql.binlog.BinaryLogClient.connect(BinaryLogClient.java:657)
	at com.github.shyiko.mysql.binlog.BinaryLogClient$7.run(BinaryLogClient.java:959)
	at java.base/java.lang.Thread.run(Thread.java:1575)
Caused by: io.debezium.DebeziumException: Encountered change event for table leafy_factory.raw_materials whose schema isn't known to this connector
	at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.informAboutUnknownTableIfRequired(BinlogStreamingChangeEventSource.java:996)
	at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.informAboutUnknownTableIfRequired(BinlogStreamingChangeEventSource.java:1048)
	at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.handleUpdateTableMetadata(BinlogStreamingChangeEventSource.java:797)
	at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.lambda$execute$4(BinlogStreamingChangeEventSource.java:178)
	at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.handleEvent(BinlogStreamingChangeEventSource.java:571)
	... 6 more
[2025-01-03 10:38:08,017] INFO [mariadb-connector|task-0] Error processing binlog event, and propagating to Kafka Connect so it stops this connector. Future binlog events read before connector is shutdown will be ignored. (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:596)
[2025-01-03 10:38:08,223] ERROR [mariadb-connector|task-0] WorkerSourceTask{id=mariadb-connector-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:234)
org.apache.kafka.connect.errors.ConnectException: An exception occurred in the change event producer. This connector will be stopped.
	at io.debezium.pipeline.ErrorHandler.setProducerThrowable(ErrorHandler.java:67)
	at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.handleEvent(BinlogStreamingChangeEventSource.java:591)
	at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.lambda$execute$17(BinlogStreamingChangeEventSource.java:209)
	at com.github.shyiko.mysql.binlog.BinaryLogClient.notifyEventListeners(BinaryLogClient.java:1281)
	at com.github.shyiko.mysql.binlog.BinaryLogClient.listenForEventPackets(BinaryLogClient.java:1103)
	at com.github.shyiko.mysql.binlog.BinaryLogClient.connect(BinaryLogClient.java:657)
	at com.github.shyiko.mysql.binlog.BinaryLogClient$7.run(BinaryLogClient.java:959)
	at java.base/java.lang.Thread.run(Thread.java:1575)
Caused by: io.debezium.DebeziumException: Error processing binlog event
	... 7 more
Caused by: io.debezium.DebeziumException: Encountered change event for table leafy_factory.raw_materials whose schema isn't known to this connector
	at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.informAboutUnknownTableIfRequired(BinlogStreamingChangeEventSource.java:996)
	at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.informAboutUnknownTableIfRequired(BinlogStreamingChangeEventSource.java:1048)
	at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.handleUpdateTableMetadata(BinlogStreamingChangeEventSource.java:797)
	at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.lambda$execute$4(BinlogStreamingChangeEventSource.java:178)
	at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.handleEvent(BinlogStreamingChangeEventSource.java:571)
	... 6 more
[2025-01-03 10:38:08,224] INFO [mariadb-connector|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:434)
[2025-01-03 10:38:08,287] INFO [mariadb-connector|task-0] Finished streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:325)
[2025-01-03 10:38:08,288] INFO [mariadb-connector|task-0] Stopped reading binlog after 356 events, last recorded offset: {file=mariadb-bin.000001, pos=52374, server_id=1, event=1} (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:1218)
[2025-01-03 10:38:08,288] INFO [mariadb-connector|task-0] SignalProcessor stopped (io.debezium.pipeline.signal.SignalProcessor:127)
[2025-01-03 10:38:08,289] INFO [mariadb-connector|task-0] Debezium ServiceRegistry stopped. (io.debezium.service.DefaultServiceRegistry:105)
[2025-01-03 10:38:08,290] INFO [mariadb-connector|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:983)
[2025-01-03 10:38:08,291] INFO [mariadb-connector|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:983)
[2025-01-03 10:38:08,291] INFO [mariadb-connector|task-0] [Producer clientId=db_-schemahistory] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 10:38:08,293] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:38:08,293] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:38:08,293] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:38:08,293] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:38:08,294] INFO [mariadb-connector|task-0] App info kafka.producer for db_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:38:08,294] INFO [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 10:38:08,296] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:38:08,296] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:38:08,296] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:38:08,296] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:38:08,296] INFO [mariadb-connector|task-0] App info kafka.producer for connector-producer-mariadb-connector-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:38:30,467] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:87)
[2025-01-03 10:38:30,467] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:358)
[2025-01-03 10:38:30,477] INFO Stopped http_8083@1a7e799e{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:383)
[2025-01-03 10:38:30,478] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:149)
[2025-01-03 10:38:30,479] INFO Stopped o.e.j.s.ServletContextHandler@5402612e{/,null,STOPPED} (org.eclipse.jetty.server.handler.ContextHandler:1159)
[2025-01-03 10:38:30,480] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:387)
[2025-01-03 10:38:30,480] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder stopping (org.apache.kafka.connect.runtime.distributed.DistributedHerder:851)
[2025-01-03 10:38:30,480] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Stopping connectors and tasks that are still assigned to this worker. (org.apache.kafka.connect.runtime.distributed.DistributedHerder:808)
[2025-01-03 10:38:30,481] INFO [mariadb-connector|worker] Stopping connector mariadb-connector (org.apache.kafka.connect.runtime.Worker:452)
[2025-01-03 10:38:30,481] INFO [mariadb-connector|worker] Scheduled shutdown for WorkerConnector{id=mariadb-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2025-01-03 10:38:30,482] INFO [mariadb-connector|worker] Completed shutdown for WorkerConnector{id=mariadb-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2025-01-03 10:38:30,483] INFO [mariadb-connector|task-0] Stopping task mariadb-connector-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2025-01-03 10:38:30,485] INFO Stopping KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:317)
[2025-01-03 10:38:30,486] INFO [Producer clientId=connect-cluster-statuses] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 10:38:30,488] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:38:30,489] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:38:30,489] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:38:30,489] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:38:30,489] INFO App info kafka.producer for connect-cluster-statuses unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:38:30,490] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 10:38:30,490] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 10:38:30,988] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:38:30,988] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:38:30,988] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:38:30,988] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:38:30,989] INFO App info kafka.consumer for connect-cluster-statuses unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:38:30,989] INFO Stopped KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:341)
[2025-01-03 10:38:30,989] INFO Closing KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:407)
[2025-01-03 10:38:30,990] INFO Stopping KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:317)
[2025-01-03 10:38:30,990] INFO [Producer clientId=connect-cluster-configs] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 10:38:30,991] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:38:30,992] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:38:30,992] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:38:30,992] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:38:30,992] INFO App info kafka.producer for connect-cluster-configs unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:38:30,992] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 10:38:30,992] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 10:38:31,222] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:38:31,222] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:38:31,222] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:38:31,222] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:38:31,224] INFO App info kafka.consumer for connect-cluster-configs unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:38:31,224] INFO Stopped KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:341)
[2025-01-03 10:38:31,224] INFO Closed KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:412)
[2025-01-03 10:38:31,224] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:250)
[2025-01-03 10:38:31,225] INFO Stopping KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:261)
[2025-01-03 10:38:31,225] INFO Stopping KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:317)
[2025-01-03 10:38:31,225] INFO [Producer clientId=connect-cluster-offsets] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 10:38:31,226] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:38:31,226] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:38:31,227] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:38:31,227] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:38:31,227] INFO App info kafka.producer for connect-cluster-offsets unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:38:31,227] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 10:38:31,227] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 10:38:31,688] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:38:31,689] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:38:31,689] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:38:31,689] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:38:31,692] INFO App info kafka.consumer for connect-cluster-offsets unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:38:31,692] INFO Stopped KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:341)
[2025-01-03 10:38:31,692] INFO Stopped KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:263)
[2025-01-03 10:38:31,693] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:38:31,693] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:38:31,693] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:38:31,693] INFO App info kafka.connect for 192.168.1.5:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:38:31,693] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:271)
[2025-01-03 10:38:31,695] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Member connect-192.168.1.5:8083-d34d4154-a563-45ec-a8c7-a450376e9962 sending LeaveGroup request to coordinator 192.168.1.5:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1174)
[2025-01-03 10:38:31,696] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1056)
[2025-01-03 10:38:31,696] WARN [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Close timed out with 1 pending requests to coordinator, terminating client connections (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1141)
[2025-01-03 10:38:31,696] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:38:31,696] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:38:31,697] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:38:31,698] INFO App info kafka.connect for connect-192.168.1.5:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:38:31,699] INFO App info kafka.admin.client for connect-cluster-shared-admin unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:38:31,701] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:38:31,701] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:38:31,701] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:38:31,702] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder stopped (org.apache.kafka.connect.runtime.distributed.DistributedHerder:394)
[2025-01-03 10:38:31,702] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder stopped (org.apache.kafka.connect.runtime.distributed.DistributedHerder:858)
[2025-01-03 10:38:31,702] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:92)
[2025-01-03 10:40:01,921] INFO Kafka Connect worker initializing ... (org.apache.kafka.connect.cli.AbstractConnectCli:114)
[2025-01-03 10:40:01,923] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote=true, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 23, 23+37-2369
	jvm.classpath = /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/activation-1.1.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/aopalliance-repackaged-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/argparse4j-0.7.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/audience-annotations-0.12.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/caffeine-2.9.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-beanutils-1.9.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-cli-1.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-collections-3.2.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-digester-2.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-io-2.14.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-lang3-3.12.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-logging-1.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-validator-1.7.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-basic-auth-extension-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-json-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-mirror-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-mirror-client-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-runtime-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-transforms-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/error_prone_annotations-2.10.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-api-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-locator-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-utils-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-annotations-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-core-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-databind-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-dataformat-csv-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-datatype-jdk8-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-jaxrs-base-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-jaxrs-json-provider-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-afterburner-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-jaxb-annotations-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-scala_2.13-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.activation-api-1.2.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.annotation-api-1.3.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.inject-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.validation-api-2.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.xml.bind-api-2.3.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javassist-3.29.2-GA.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.activation-api-1.2.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.annotation-api-1.3.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.servlet-api-3.1.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.ws.rs-api-2.1.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jaxb-api-2.3.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-client-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-common-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-container-servlet-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-container-servlet-core-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-hk2-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-server-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-client-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-continuation-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-http-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-io-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-security-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-server-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-servlet-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-servlets-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-util-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-util-ajax-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jline-3.25.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jopt-simple-5.0.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jose4j-0.9.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jsr305-3.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-clients-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-group-coordinator-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-group-coordinator-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-metadata-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-raft-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-server-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-server-common-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-shell-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-storage-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-storage-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-examples-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-scala_2.13-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-test-utils-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-tools-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-tools-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-transaction-coordinator-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka_2.13-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/lz4-java-1.8.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/maven-artifact-3.9.6.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/metrics-core-2.2.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/metrics-core-4.1.12.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-buffer-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-codec-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-common-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-handler-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-resolver-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-classes-epoll-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-native-epoll-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-native-unix-common-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/opentelemetry-proto-1.0.0-alpha.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/osgi-resource-locator-1.0.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/paranamer-2.8.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/pcollections-4.0.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/plexus-utils-3.5.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/protobuf-java-3.25.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/reflections-0.10.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/reload4j-1.2.25.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/rocksdbjni-7.9.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-collection-compat_2.13-2.10.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-java8-compat_2.13-1.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-library-2.13.14.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-logging_2.13-3.9.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-reflect-2.13.14.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/slf4j-api-1.7.36.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/slf4j-reload4j-1.7.36.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/snappy-java-1.1.10.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/swagger-annotations-2.2.8.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/trogdor-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zookeeper-3.8.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zookeeper-jute-3.8.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zstd-jni-1.5.6-4.jar
	os.spec = Mac OS X, aarch64, 15.0
	os.vcpus = 11
 (org.apache.kafka.connect.runtime.WorkerInfo:72)
[2025-01-03 10:40:01,924] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.AbstractConnectCli:120)
[2025-01-03 10:40:01,936] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:40:01,967] INFO Using up-to-date JsonConverter implementation (io.debezium.converters.CloudEventsConverter:120)
[2025-01-03 10:40:02,054] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:40:02,055] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1 (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:40:02,058] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:40:02,059] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:40:02,070] INFO Using up-to-date JsonConverter implementation (io.debezium.converters.CloudEventsConverter:120)
[2025-01-03 10:40:02,084] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:40:02,086] INFO Loading plugin from: classpath (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:40:02,089] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@5a07e868 (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:40:02,090] INFO Scanning plugins with ServiceLoaderScanner took 155 ms (org.apache.kafka.connect.runtime.isolation.PluginScanner:71)
[2025-01-03 10:40:02,091] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:40:02,215] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:40:02,215] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1 (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:40:02,273] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:40:02,273] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:40:02,421] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:40:02,421] INFO Loading plugin from: classpath (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:40:02,895] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@5a07e868 (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:40:02,895] INFO Scanning plugins with ReflectionScanner took 804 ms (org.apache.kafka.connect.runtime.isolation.PluginScanner:71)
[2025-01-03 10:40:02,896] WARN One or more plugins are missing ServiceLoader manifests may not be usable with plugin.discovery=service_load: [
file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1/	com.mongodb.kafka.connect.MongoSinkConnector	sink	1.14.1
file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1/	com.mongodb.kafka.connect.MongoSourceConnector	source	1.14.1
file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/	io.debezium.connector.mongodb.MongoDbSinkConnector	sink	3.0.5.Final
]
Read the documentation at https://kafka.apache.org/documentation.html#connect_plugindiscovery for instructions on migrating your plugins to take advantage of the performance improvements of service_load mode. To silence this warning, set plugin.discovery=only_scan in the worker config. (org.apache.kafka.connect.runtime.isolation.Plugins:123)
[2025-01-03 10:40:02,897] INFO Added plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,897] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,897] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,897] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,897] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,897] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,897] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,897] INFO Added plugin 'io.debezium.transforms.HeaderToValue' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,897] INFO Added plugin 'io.debezium.transforms.SchemaChangeEventFilter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,897] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,897] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,897] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,897] INFO Added plugin 'io.debezium.transforms.ExtractSchemaToNewRecord' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,897] INFO Added plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,897] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,897] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,897] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,897] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,897] INFO Added plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,898] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,898] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,898] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,898] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,898] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,898] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,898] INFO Added plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,898] INFO Added plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,898] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,898] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,898] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,898] INFO Added plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,898] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,898] INFO Added plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,898] INFO Added plugin 'io.debezium.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,898] INFO Added plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,898] INFO Added plugin 'io.debezium.transforms.partitions.PartitionRouting' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,898] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,898] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,898] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,898] INFO Added plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,898] INFO Added plugin 'io.debezium.transforms.TimezoneConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,899] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,899] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,899] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,899] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,899] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,899] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,899] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,899] INFO Added plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,899] INFO Added plugin 'io.debezium.transforms.ExtractChangedRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,899] INFO Added plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,899] INFO Added plugin 'io.debezium.connector.mongodb.transforms.outbox.MongoEventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,899] INFO Added plugin 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,899] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,899] INFO Added plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,899] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,899] INFO Added plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,899] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,899] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,899] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,899] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,899] INFO Added plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,899] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,899] INFO Added plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,899] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,899] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,899] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,899] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,899] INFO Added plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,899] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,900] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,900] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:40:02,900] INFO Added alias 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,900] INFO Added alias 'ExtractNewDocumentState' to plugin 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,900] INFO Added alias 'JsonConverter' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,900] INFO Added alias 'DebeziumMySql' to plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,900] INFO Added alias 'MongoSourceConnector' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,900] INFO Added alias 'CloudEventsConverter' to plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,900] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,900] INFO Added alias 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,901] INFO Added alias 'DebeziumMySqlConnectRestExtension' to plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,901] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,901] INFO Added alias 'MySql' to plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,901] INFO Added alias 'MongoDbSinkConnector' to plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,901] INFO Added alias 'EnvVar' to plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,901] INFO Added alias 'EnvVarConfigProvider' to plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,901] INFO Added alias 'MongoDb' to plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,901] INFO Added alias 'MirrorCheckpointConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,901] INFO Added alias 'Boolean' to plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,901] INFO Added alias 'MySqlConnector' to plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,901] INFO Added alias 'NoneConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,901] INFO Added alias 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,901] INFO Added alias 'HeaderToValue' to plugin 'io.debezium.transforms.HeaderToValue' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,901] INFO Added alias 'PartitionRouting' to plugin 'io.debezium.transforms.partitions.PartitionRouting' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,901] INFO Added alias 'MongoDbSink' to plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,901] INFO Added alias 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,901] INFO Added alias 'StringConverter' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,901] INFO Added alias 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,901] INFO Added alias 'ByLogicalTableRouter' to plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,901] INFO Added alias 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,901] INFO Added alias 'MongoDbConnector' to plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,901] INFO Added alias 'FileConfigProvider' to plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,901] INFO Added alias 'IntegerConverter' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,901] INFO Added alias 'EventRouter' to plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,901] INFO Added alias 'SchemaChangeEventFilter' to plugin 'io.debezium.transforms.SchemaChangeEventFilter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,901] INFO Added alias 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,901] INFO Added alias 'LongConverter' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,901] INFO Added alias 'DropHeaders' to plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,901] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,902] INFO Added alias 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,902] INFO Added alias 'ExtractSchemaToNewRecord' to plugin 'io.debezium.transforms.ExtractSchemaToNewRecord' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,902] INFO Added alias 'BinaryData' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,902] INFO Added alias 'File' to plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,902] INFO Added alias 'FloatConverter' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,902] INFO Added alias 'DirectoryConfigProvider' to plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,902] INFO Added alias 'ReadToInsertEvent' to plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,902] INFO Added alias 'ShortConverter' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,902] INFO Added alias 'CloudEvents' to plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,902] INFO Added alias 'ActivateTracingSpan' to plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,902] INFO Added alias 'TimezoneConverter' to plugin 'io.debezium.transforms.TimezoneConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,902] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,902] INFO Added alias 'DoubleConverter' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,902] INFO Added alias 'Simple' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,902] INFO Added alias 'BinaryDataConverter' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,902] INFO Added alias 'AllConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,902] INFO Added alias 'ExtractNewRecordState' to plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,902] INFO Added alias 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,902] INFO Added alias 'DebeziumMongoDb' to plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,902] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,902] INFO Added alias 'MirrorHeartbeatConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,902] INFO Added alias 'InsertHeader' to plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,902] INFO Added alias 'Directory' to plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,902] INFO Added alias 'MirrorSourceConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,902] INFO Added alias 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,902] INFO Added alias 'MongoSinkConnector' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,902] INFO Added alias 'PrincipalConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,902] INFO Added alias 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,902] INFO Added alias 'DebeziumMongoDbConnectRestExtension' to plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,902] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,902] INFO Added alias 'BooleanConverter' to plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,902] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,903] INFO Added alias 'MongoEventRouter' to plugin 'io.debezium.connector.mongodb.transforms.outbox.MongoEventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,903] INFO Added alias 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,903] INFO Added alias 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,903] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,903] INFO Added alias 'ExtractChangedRecordState' to plugin 'io.debezium.transforms.ExtractChangedRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,903] INFO Added alias 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,903] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,903] INFO Added alias 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:40:02,918] INFO DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	config.providers = []
	config.storage.replication.factor = 1
	config.storage.topic = connect-configs
	connect.protocol = sessioned
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = All
	exactly.once.source.support = disabled
	group.id = connect-cluster
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	heartbeat.interval.ms = 3000
	inter.worker.key.generation.algorithm = HmacSHA256
	inter.worker.key.size = null
	inter.worker.key.ttl.ms = 3600000
	inter.worker.signature.algorithm = HmacSHA256
	inter.worker.verification.algorithms = [HmacSHA256]
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = [http://:8083]
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 1
	offset.storage.topic = connect-offsets
	plugin.discovery = hybrid_warn
	plugin.path = [/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins]
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	status.storage.partitions = 5
	status.storage.replication.factor = 1
	status.storage.topic = connect-status
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000
 (org.apache.kafka.connect.runtime.distributed.DistributedConfig:371)
[2025-01-03 10:40:02,919] INFO Creating Kafka admin client (org.apache.kafka.connect.runtime.WorkerConfig:281)
[2025-01-03 10:40:02,921] INFO AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:371)
[2025-01-03 10:40:02,940] INFO These configurations '[config.storage.topic, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2025-01-03 10:40:02,940] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:40:02,940] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:40:02,940] INFO Kafka startTimeMs: 1735922402940 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:40:03,048] INFO Kafka cluster ID: gMBptw3AQ7-7-73laciaMw (org.apache.kafka.connect.runtime.WorkerConfig:298)
[2025-01-03 10:40:03,049] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:40:03,051] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:40:03,051] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:40:03,051] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:40:03,053] INFO PublicConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	listeners = [http://:8083]
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
 (org.apache.kafka.connect.runtime.rest.RestServerConfig$PublicConfig:371)
[2025-01-03 10:40:03,056] INFO Logging initialized @1367ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:170)
[2025-01-03 10:40:03,069] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:125)
[2025-01-03 10:40:03,069] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:196)
[2025-01-03 10:40:03,077] INFO jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 23+37-2369 (org.eclipse.jetty.server.Server:375)
[2025-01-03 10:40:03,084] INFO Started http_8083@521a506c{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:333)
[2025-01-03 10:40:03,085] INFO Started @1395ms (org.eclipse.jetty.server.Server:415)
[2025-01-03 10:40:03,091] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 10:40:03,091] INFO REST server listening at http://192.168.1.5:8083/, advertising URL http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:216)
[2025-01-03 10:40:03,091] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 10:40:03,091] INFO REST admin endpoints at http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2025-01-03 10:40:03,091] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 10:40:03,092] INFO Setting up All Policy for ConnectorClientConfigOverride. This will allow all client configurations to be overridden (org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy:45)
[2025-01-03 10:40:03,093] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:40:03,100] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:40:03,100] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:40:03,100] INFO Kafka startTimeMs: 1735922403100 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:40:03,102] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:40:03,102] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:40:03,110] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 10:40:03,122] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:40:03,122] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:40:03,123] INFO Kafka startTimeMs: 1735922403122 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:40:03,124] INFO Kafka Connect worker initialization took 1201ms (org.apache.kafka.connect.cli.AbstractConnectCli:141)
[2025-01-03 10:40:03,124] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:67)
[2025-01-03 10:40:03,124] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2025-01-03 10:40:03,125] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder starting (org.apache.kafka.connect.runtime.distributed.DistributedHerder:375)
[2025-01-03 10:40:03,125] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:233)
[2025-01-03 10:40:03,125] INFO Starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:232)
[2025-01-03 10:40:03,125] INFO Starting KafkaBasedLog with topic connect-offsets reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog:254)
[2025-01-03 10:40:03,125] INFO AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-shared-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:371)
[2025-01-03 10:40:03,126] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2025-01-03 10:40:03,127] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:40:03,127] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:40:03,127] INFO Kafka startTimeMs: 1735922403126 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:40:03,136] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:238)
[2025-01-03 10:40:03,150] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:334)
[2025-01-03 10:40:03,151] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:339)
[2025-01-03 10:40:03,152] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session:132)
[2025-01-03 10:40:03,312] INFO Started o.e.j.s.ServletContextHandler@19924f15{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:921)
[2025-01-03 10:40:03,312] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:303)
[2025-01-03 10:40:03,312] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:77)
[2025-01-03 10:40:03,464] INFO Created topic (name=connect-offsets, numPartitions=25, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:9092 (org.apache.kafka.connect.util.TopicAdmin:416)
[2025-01-03 10:40:03,467] INFO ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-offsets
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 10:40:03,475] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:40:03,481] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 10:40:03,481] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:40:03,481] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:40:03,481] INFO Kafka startTimeMs: 1735922403481 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:40:03,484] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-offsets
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 10:40:03,484] INFO [Producer clientId=connect-cluster-offsets] Cluster ID: gMBptw3AQ7-7-73laciaMw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:40:03,488] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:40:03,497] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2025-01-03 10:40:03,497] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:40:03,497] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:40:03,497] INFO Kafka startTimeMs: 1735922403497 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:40:03,499] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Cluster ID: gMBptw3AQ7-7-73laciaMw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:40:03,503] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Assigned to partition(s): connect-offsets-0, connect-offsets-5, connect-offsets-10, connect-offsets-20, connect-offsets-15, connect-offsets-9, connect-offsets-11, connect-offsets-4, connect-offsets-16, connect-offsets-17, connect-offsets-3, connect-offsets-24, connect-offsets-23, connect-offsets-13, connect-offsets-18, connect-offsets-22, connect-offsets-8, connect-offsets-2, connect-offsets-12, connect-offsets-19, connect-offsets-14, connect-offsets-1, connect-offsets-6, connect-offsets-7, connect-offsets-21 (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:579)
[2025-01-03 10:40:03,504] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:40:03,504] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-5 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:40:03,504] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-10 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:40:03,504] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-20 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:40:03,504] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-15 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:40:03,504] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-9 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:40:03,504] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-11 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:40:03,504] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:40:03,504] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-16 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:40:03,504] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-17 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:40:03,504] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:40:03,504] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-24 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:40:03,504] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-23 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:40:03,504] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-13 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:40:03,504] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-18 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:40:03,504] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-22 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:40:03,504] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-8 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:40:03,504] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:40:03,504] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-12 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:40:03,504] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-19 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:40:03,504] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-14 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:40:03,504] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:40:03,504] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-6 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:40:03,505] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-7 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:40:03,505] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-21 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:40:03,521] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:40:03,521] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:40:03,521] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:40:03,521] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:40:03,521] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:40:03,521] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:40:03,521] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:40:03,521] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:40:03,521] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:40:03,521] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:40:03,521] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:40:03,521] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:40:03,521] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:40:03,521] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:40:03,521] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:40:03,521] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:40:03,521] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:40:03,521] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:40:03,521] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:40:03,522] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:40:03,522] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:40:03,522] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:40:03,522] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:40:03,522] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:40:03,522] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:40:03,522] INFO Finished reading KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:311)
[2025-01-03 10:40:03,522] INFO Started KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:313)
[2025-01-03 10:40:03,522] INFO Finished reading offsets topic and starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:249)
[2025-01-03 10:40:03,522] INFO Worker started (org.apache.kafka.connect.runtime.Worker:243)
[2025-01-03 10:40:03,523] INFO Starting KafkaBasedLog with topic connect-status reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog:254)
[2025-01-03 10:40:03,600] INFO Created topic (name=connect-status, numPartitions=5, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:9092 (org.apache.kafka.connect.util.TopicAdmin:416)
[2025-01-03 10:40:03,601] INFO ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-statuses
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 10:40:03,601] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:40:03,603] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 10:40:03,603] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:40:03,603] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:40:03,603] INFO Kafka startTimeMs: 1735922403603 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:40:03,604] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-statuses
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 10:40:03,605] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:40:03,606] INFO [Producer clientId=connect-cluster-statuses] Cluster ID: gMBptw3AQ7-7-73laciaMw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:40:03,607] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2025-01-03 10:40:03,607] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:40:03,607] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:40:03,607] INFO Kafka startTimeMs: 1735922403607 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:40:03,610] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Cluster ID: gMBptw3AQ7-7-73laciaMw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:40:03,611] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Assigned to partition(s): connect-status-0, connect-status-4, connect-status-1, connect-status-2, connect-status-3 (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:579)
[2025-01-03 10:40:03,611] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:40:03,611] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:40:03,611] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:40:03,611] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:40:03,611] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:40:03,617] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:40:03,617] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:40:03,617] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:40:03,617] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:40:03,617] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:40:03,617] INFO Finished reading KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:311)
[2025-01-03 10:40:03,617] INFO Started KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:313)
[2025-01-03 10:40:03,618] INFO Starting KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:378)
[2025-01-03 10:40:03,618] INFO Starting KafkaBasedLog with topic connect-configs reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog:254)
[2025-01-03 10:40:03,651] INFO Created topic (name=connect-configs, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:9092 (org.apache.kafka.connect.util.TopicAdmin:416)
[2025-01-03 10:40:03,652] INFO ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-configs
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 10:40:03,652] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:40:03,653] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 10:40:03,653] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:40:03,653] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:40:03,653] INFO Kafka startTimeMs: 1735922403653 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:40:03,654] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-configs
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 10:40:03,654] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:40:03,655] INFO [Producer clientId=connect-cluster-configs] Cluster ID: gMBptw3AQ7-7-73laciaMw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:40:03,655] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2025-01-03 10:40:03,655] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:40:03,655] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:40:03,655] INFO Kafka startTimeMs: 1735922403655 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:40:03,657] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Cluster ID: gMBptw3AQ7-7-73laciaMw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:40:03,657] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Assigned to partition(s): connect-configs-0 (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:579)
[2025-01-03 10:40:03,657] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Seeking to earliest offset of partition connect-configs-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:40:03,664] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Resetting offset for partition connect-configs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:40:03,665] INFO Finished reading KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:311)
[2025-01-03 10:40:03,665] INFO Started KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:313)
[2025-01-03 10:40:03,665] INFO Started KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:402)
[2025-01-03 10:40:03,667] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Cluster ID: gMBptw3AQ7-7-73laciaMw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:40:04,466] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Discovered group coordinator 192.168.1.5:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:937)
[2025-01-03 10:40:04,470] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2025-01-03 10:40:04,470] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 10:40:04,489] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 10:40:04,499] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=1, memberId='connect-192.168.1.5:8083-50ebba96-5ef2-4278-939c-c372fff74be9', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2025-01-03 10:40:04,528] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=1, memberId='connect-192.168.1.5:8083-50ebba96-5ef2-4278-939c-c372fff74be9', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2025-01-03 10:40:04,528] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Joined group at generation 1 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.1.5:8083-50ebba96-5ef2-4278-939c-c372fff74be9', leaderUrl='http://192.168.1.5:8083/', offset=-1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2025-01-03 10:40:04,529] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder started (org.apache.kafka.connect.runtime.distributed.DistributedHerder:387)
[2025-01-03 10:40:04,529] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connectors and tasks using config offset -1 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2025-01-03 10:40:04,529] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2025-01-03 10:40:04,571] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Session key updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2510)
[2025-01-03 10:40:17,543] INFO [0:0:0:0:0:0:0:1] - - [03/Jan/2025:16:40:17 +0000] "POST /status HTTP/1.1" 404 49 "-" "curl/8.7.1" 43 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-01-03 10:40:25,603] INFO [0:0:0:0:0:0:0:1] - - [03/Jan/2025:16:40:25 +0000] "GET /connectors/mariadb-connector/status HTTP/1.1" 404 78 "-" "curl/8.7.1" 11 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-01-03 10:40:41,959] INFO MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = ${topic}
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = test
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 10:40:41,995] INFO MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync", "version": "4.7.2"}, "os": {"type": "Darwin", "name": "Mac OS X", "architecture": "aarch64", "version": "15.0"}, "platform": "Java/Oracle Corporation/23+37-2369", "application": {"name": "IST-Shared"}} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=majority, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=MongoCredential{mechanism=null, userName='leafy_factory', source='admin', password=<hidden>, mechanismProperties=<hidden>}, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@3c1f132f]}, clusterSettings={hosts=[127.0.0.1:27017], srvHost=ist-shared.n0kts.mongodb.net, srvServiceName=mongodb, mode=MULTIPLE, requiredClusterType=REPLICA_SET, requiredReplicaSetName='atlas-133bp7-shard-0', serverSelector='null', clusterListeners='[com.mongodb.kafka.connect.util.ConnectionValidator$1@2c209b1c]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=true, invalidHostNameAllowed=false, context=null}, applicationName='IST-Shared', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null} (org.mongodb.driver.client:71)
[2025-01-03 10:40:42,228] INFO Adding discovered server ist-shared-shard-00-01.n0kts.mongodb.net:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2025-01-03 10:40:42,246] INFO Adding discovered server ist-shared-shard-00-02.n0kts.mongodb.net:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2025-01-03 10:40:42,248] INFO Adding discovered server ist-shared-shard-00-00.n0kts.mongodb.net:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2025-01-03 10:40:43,512] INFO Opened connection [connectionId{localValue:6, serverValue:1157497}] to ist-shared-shard-00-00.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 10:40:43,514] INFO Opened connection [connectionId{localValue:4, serverValue:698140}] to ist-shared-shard-00-02.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 10:40:43,515] INFO Opened connection [connectionId{localValue:2, serverValue:698141}] to ist-shared-shard-00-02.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 10:40:43,519] INFO Opened connection [connectionId{localValue:5, serverValue:1157496}] to ist-shared-shard-00-00.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 10:40:43,521] INFO Monitor thread successfully connected to server with description ServerDescription{address=ist-shared-shard-00-02.n0kts.mongodb.net:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=510698208, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-02.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az4'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=null, setVersion=71, topologyVersion=TopologyVersion{processId=67694a2575e55e11df78dcd1, counter=14}, lastWriteDate=Fri Jan 03 10:40:43 CST 2025, lastUpdateTimeNanos=45494034220541} (org.mongodb.driver.cluster:71)
[2025-01-03 10:40:43,521] INFO Monitor thread successfully connected to server with description ServerDescription{address=ist-shared-shard-00-00.n0kts.mongodb.net:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=493910666, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-00.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az1'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=null, setVersion=71, topologyVersion=TopologyVersion{processId=675b24d1d1431a587be694a8, counter=19}, lastWriteDate=Fri Jan 03 10:40:43 CST 2025, lastUpdateTimeNanos=45494030996833} (org.mongodb.driver.cluster:71)
[2025-01-03 10:40:43,533] INFO No server chosen by ReadPreferenceServerSelector{readPreference=primary} from cluster description ClusterDescription{type=REPLICA_SET, connectionMode=MULTIPLE, serverDescriptions=[ServerDescription{address=ist-shared-shard-00-02.n0kts.mongodb.net:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=510698208, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-02.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az4'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=null, setVersion=71, topologyVersion=TopologyVersion{processId=67694a2575e55e11df78dcd1, counter=14}, lastWriteDate=Fri Jan 03 10:40:43 CST 2025, lastUpdateTimeNanos=45494034220541}, ServerDescription{address=ist-shared-shard-00-01.n0kts.mongodb.net:27017, type=UNKNOWN, state=CONNECTING}, ServerDescription{address=ist-shared-shard-00-00.n0kts.mongodb.net:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=493910666, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-00.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az1'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=null, setVersion=71, topologyVersion=TopologyVersion{processId=675b24d1d1431a587be694a8, counter=19}, lastWriteDate=Fri Jan 03 10:40:43 CST 2025, lastUpdateTimeNanos=45494030996833}]}. Waiting for 30000 ms before timing out (org.mongodb.driver.cluster:71)
[2025-01-03 10:40:43,553] INFO Opened connection [connectionId{localValue:1, serverValue:1292665}] to ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 10:40:43,553] INFO Monitor thread successfully connected to server with description ServerDescription{address=ist-shared-shard-00-01.n0kts.mongodb.net:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=538727167, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-01.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az2'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=7fffffff000000000000010a, setVersion=71, topologyVersion=TopologyVersion{processId=67694b4e6118cd58a4791747, counter=41}, lastWriteDate=Fri Jan 03 10:40:43 CST 2025, lastUpdateTimeNanos=45494072751333} (org.mongodb.driver.cluster:71)
[2025-01-03 10:40:43,554] INFO Setting max election id to 7fffffff000000000000010a from replica set primary ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.cluster:71)
[2025-01-03 10:40:43,554] INFO Setting max set version to 71 from replica set primary ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.cluster:71)
[2025-01-03 10:40:43,554] INFO Discovered replica set primary ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.cluster:71)
[2025-01-03 10:40:43,584] INFO Opened connection [connectionId{localValue:3, serverValue:1292666}] to ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 10:40:44,197] INFO Opened connection [connectionId{localValue:7, serverValue:1292667}] to ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 10:40:44,325] INFO MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = ${topic}
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = __default
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 10:40:44,462] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:371)
[2025-01-03 10:40:44,474] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Connector mongodb-sink config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2025-01-03 10:40:44,475] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2025-01-03 10:40:44,475] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 10:40:44,480] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=2, memberId='connect-192.168.1.5:8083-50ebba96-5ef2-4278-939c-c372fff74be9', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2025-01-03 10:40:44,486] INFO [0:0:0:0:0:0:0:1] - - [03/Jan/2025:16:40:41 +0000] "POST /connectors HTTP/1.1" 201 757 "-" "curl/8.7.1" 2880 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-01-03 10:40:44,487] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=2, memberId='connect-192.168.1.5:8083-50ebba96-5ef2-4278-939c-c372fff74be9', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2025-01-03 10:40:44,488] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Joined group at generation 2 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.1.5:8083-50ebba96-5ef2-4278-939c-c372fff74be9', leaderUrl='http://192.168.1.5:8083/', offset=2, connectorIds=[mongodb-sink], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2025-01-03 10:40:44,488] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 2 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2025-01-03 10:40:44,489] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connector mongodb-sink (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2025-01-03 10:40:44,490] INFO [mongodb-sink|worker] Creating connector mongodb-sink of type com.mongodb.kafka.connect.MongoSinkConnector (org.apache.kafka.connect.runtime.Worker:313)
[2025-01-03 10:40:44,491] INFO [mongodb-sink|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = true
	errors.log.include.messages = true
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = all
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongodb-sink
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = []
	topics.regex = db_\.leafy_factory\..*
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2025-01-03 10:40:44,492] INFO [mongodb-sink|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = true
	errors.log.include.messages = true
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = all
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongodb-sink
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = []
	topics.regex = db_\.leafy_factory\..*
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 10:40:44,494] INFO [mongodb-sink|worker] Instantiated connector mongodb-sink with version 1.14.1 of type class com.mongodb.kafka.connect.MongoSinkConnector (org.apache.kafka.connect.runtime.Worker:335)
[2025-01-03 10:40:44,494] INFO [mongodb-sink|worker] Finished creating connector mongodb-sink (org.apache.kafka.connect.runtime.Worker:356)
[2025-01-03 10:40:44,494] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2025-01-03 10:40:44,499] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = true
	errors.log.include.messages = true
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = all
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongodb-sink
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = []
	topics.regex = db_\.leafy_factory\..*
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2025-01-03 10:40:44,500] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = true
	errors.log.include.messages = true
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = all
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongodb-sink
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = []
	topics.regex = db_\.leafy_factory\..*
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 10:40:44,513] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Tasks [mongodb-sink-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2025-01-03 10:40:44,515] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2025-01-03 10:40:44,515] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 10:40:44,517] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=3, memberId='connect-192.168.1.5:8083-50ebba96-5ef2-4278-939c-c372fff74be9', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2025-01-03 10:40:44,522] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=3, memberId='connect-192.168.1.5:8083-50ebba96-5ef2-4278-939c-c372fff74be9', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2025-01-03 10:40:44,522] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Joined group at generation 3 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.1.5:8083-50ebba96-5ef2-4278-939c-c372fff74be9', leaderUrl='http://192.168.1.5:8083/', offset=4, connectorIds=[mongodb-sink], taskIds=[mongodb-sink-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2025-01-03 10:40:44,522] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 4 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2025-01-03 10:40:44,523] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting task mongodb-sink-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2025-01-03 10:40:44,525] INFO [mongodb-sink|task-0] Creating task mongodb-sink-0 (org.apache.kafka.connect.runtime.Worker:646)
[2025-01-03 10:40:44,526] INFO [mongodb-sink|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = true
	errors.log.include.messages = true
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = all
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongodb-sink
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2025-01-03 10:40:44,526] INFO [mongodb-sink|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = true
	errors.log.include.messages = true
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = all
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongodb-sink
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 10:40:44,527] INFO [mongodb-sink|task-0] TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.sink.MongoSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2025-01-03 10:40:44,527] INFO [mongodb-sink|task-0] Instantiated task mongodb-sink-0 with version 1.14.1 of type com.mongodb.kafka.connect.sink.MongoSinkTask (org.apache.kafka.connect.runtime.Worker:665)
[2025-01-03 10:40:44,527] INFO [mongodb-sink|task-0] StringConverterConfig values: 
	converter.encoding = UTF-8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:371)
[2025-01-03 10:40:44,527] INFO [mongodb-sink|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:40:44,527] INFO [mongodb-sink|task-0] Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task mongodb-sink-0 using the connector config (org.apache.kafka.connect.runtime.Worker:680)
[2025-01-03 10:40:44,527] INFO [mongodb-sink|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongodb-sink-0 using the connector config (org.apache.kafka.connect.runtime.Worker:686)
[2025-01-03 10:40:44,527] INFO [mongodb-sink|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongodb-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2025-01-03 10:40:44,528] INFO [mongodb-sink|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:1795)
[2025-01-03 10:40:44,529] INFO [mongodb-sink|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = true
	errors.log.include.messages = true
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = all
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongodb-sink
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = []
	topics.regex = db_\.leafy_factory\..*
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2025-01-03 10:40:44,529] INFO [mongodb-sink|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = true
	errors.log.include.messages = true
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = all
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongodb-sink
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = []
	topics.regex = db_\.leafy_factory\..*
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 10:40:44,530] INFO [mongodb-sink|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mongodb-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mongodb-sink
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 10:40:44,530] INFO [mongodb-sink|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:40:44,533] INFO [mongodb-sink|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2025-01-03 10:40:44,533] INFO [mongodb-sink|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:40:44,533] INFO [mongodb-sink|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:40:44,533] INFO [mongodb-sink|task-0] Kafka startTimeMs: 1735922444533 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:40:44,537] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Subscribed to pattern: 'db_\.leafy_factory\..*' (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:534)
[2025-01-03 10:40:44,537] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2025-01-03 10:40:44,537] INFO [mongodb-sink|task-0] Starting MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:66)
[2025-01-03 10:40:44,555] INFO [mongodb-sink|task-0] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|mongo-kafka|sink", "version": "4.7.2|1.14.1"}, "os": {"type": "Darwin", "name": "Mac OS X", "architecture": "aarch64", "version": "15.0"}, "platform": "Java/Oracle Corporation/23+37-2369", "application": {"name": "IST-Shared"}} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=majority, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=MongoCredential{mechanism=null, userName='leafy_factory', source='admin', password=<hidden>, mechanismProperties=<hidden>}, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@3c1f132f]}, clusterSettings={hosts=[127.0.0.1:27017], srvHost=ist-shared.n0kts.mongodb.net, srvServiceName=mongodb, mode=MULTIPLE, requiredClusterType=REPLICA_SET, requiredReplicaSetName='atlas-133bp7-shard-0', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=true, invalidHostNameAllowed=false, context=null}, applicationName='IST-Shared', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null} (org.mongodb.driver.client:71)
[2025-01-03 10:40:44,560] INFO [mongodb-sink|task-0] Adding discovered server ist-shared-shard-00-01.n0kts.mongodb.net:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2025-01-03 10:40:44,560] INFO [mongodb-sink|task-0] WorkerSinkTask{id=mongodb-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:325)
[2025-01-03 10:40:44,560] INFO [mongodb-sink|task-0] Adding discovered server ist-shared-shard-00-02.n0kts.mongodb.net:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2025-01-03 10:40:44,560] INFO [mongodb-sink|task-0] WorkerSinkTask{id=mongodb-sink-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:211)
[2025-01-03 10:40:44,561] INFO [mongodb-sink|task-0] Adding discovered server ist-shared-shard-00-00.n0kts.mongodb.net:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2025-01-03 10:40:44,564] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Cluster ID: gMBptw3AQ7-7-73laciaMw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:40:44,564] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Discovered group coordinator 192.168.1.5:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:937)
[2025-01-03 10:40:44,565] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:605)
[2025-01-03 10:40:44,569] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Request joining group due to: need to re-join with the given member-id: connector-consumer-mongodb-sink-0-db949108-291d-4c08-87d7-ecdf87a8dc34 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 10:40:44,569] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:605)
[2025-01-03 10:40:44,573] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Successfully joined group with generation Generation{generationId=1, memberId='connector-consumer-mongodb-sink-0-db949108-291d-4c08-87d7-ecdf87a8dc34', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:666)
[2025-01-03 10:40:44,575] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Finished assignment for group at generation 1: {connector-consumer-mongodb-sink-0-db949108-291d-4c08-87d7-ecdf87a8dc34=Assignment(partitions=[])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:664)
[2025-01-03 10:40:44,578] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Successfully synced group in generation Generation{generationId=1, memberId='connector-consumer-mongodb-sink-0-db949108-291d-4c08-87d7-ecdf87a8dc34', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:843)
[2025-01-03 10:40:44,579] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:324)
[2025-01-03 10:40:44,579] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:58)
[2025-01-03 10:40:45,006] INFO [mongodb-sink|task-0] Opened connection [connectionId{localValue:12, serverValue:1157500}] to ist-shared-shard-00-00.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 10:40:45,006] INFO [mongodb-sink|task-0] Opened connection [connectionId{localValue:11, serverValue:698144}] to ist-shared-shard-00-02.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 10:40:45,006] INFO [mongodb-sink|task-0] Opened connection [connectionId{localValue:13, serverValue:1157501}] to ist-shared-shard-00-00.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 10:40:45,006] INFO [mongodb-sink|task-0] Opened connection [connectionId{localValue:10, serverValue:698145}] to ist-shared-shard-00-02.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 10:40:45,006] INFO [mongodb-sink|task-0] Monitor thread successfully connected to server with description ServerDescription{address=ist-shared-shard-00-00.n0kts.mongodb.net:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=349136041, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-00.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az1'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=null, setVersion=71, topologyVersion=TopologyVersion{processId=675b24d1d1431a587be694a8, counter=19}, lastWriteDate=Fri Jan 03 10:40:44 CST 2025, lastUpdateTimeNanos=45495525134041} (org.mongodb.driver.cluster:71)
[2025-01-03 10:40:45,007] INFO [mongodb-sink|task-0] Monitor thread successfully connected to server with description ServerDescription{address=ist-shared-shard-00-02.n0kts.mongodb.net:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=349685708, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-02.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az4'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=null, setVersion=71, topologyVersion=TopologyVersion{processId=67694a2575e55e11df78dcd1, counter=14}, lastWriteDate=Fri Jan 03 10:40:44 CST 2025, lastUpdateTimeNanos=45495525174083} (org.mongodb.driver.cluster:71)
[2025-01-03 10:40:45,013] INFO [mongodb-sink|task-0] Opened connection [connectionId{localValue:8, serverValue:1292672}] to ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 10:40:45,015] INFO [mongodb-sink|task-0] Monitor thread successfully connected to server with description ServerDescription{address=ist-shared-shard-00-01.n0kts.mongodb.net:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=332899459, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-01.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az2'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=7fffffff000000000000010a, setVersion=71, topologyVersion=TopologyVersion{processId=67694b4e6118cd58a4791747, counter=41}, lastWriteDate=Fri Jan 03 10:40:44 CST 2025, lastUpdateTimeNanos=45495532760541} (org.mongodb.driver.cluster:71)
[2025-01-03 10:40:45,015] INFO [mongodb-sink|task-0] Setting max election id to 7fffffff000000000000010a from replica set primary ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.cluster:71)
[2025-01-03 10:40:45,016] INFO [mongodb-sink|task-0] Setting max set version to 71 from replica set primary ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.cluster:71)
[2025-01-03 10:40:45,016] INFO [mongodb-sink|task-0] Discovered replica set primary ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.cluster:71)
[2025-01-03 10:40:45,017] INFO [mongodb-sink|task-0] Opened connection [connectionId{localValue:9, serverValue:1292671}] to ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 10:41:18,582] INFO Loading the custom source info struct maker plugin: io.debezium.connector.mysql.MySqlSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2025-01-03 10:41:19,034] INFO Using 'SHOW MASTER STATUS' to get binary log status (io.debezium.connector.mysql.jdbc.MySqlConnection:41)
[2025-01-03 10:41:19,037] INFO Successfully tested connection for jdbc:mysql://localhost:3306/?useInformationSchema=true&nullCatalogMeansCurrent=false&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8&zeroDateTimeBehavior=CONVERT_TO_NULL&connectTimeout=30000 with user 'root' (io.debezium.connector.binlog.BinlogConnector:66)
[2025-01-03 10:41:19,039] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection:983)
[2025-01-03 10:41:19,040] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:371)
[2025-01-03 10:41:19,045] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Connector mariadb-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2025-01-03 10:41:19,046] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2025-01-03 10:41:19,046] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 10:41:19,047] INFO [0:0:0:0:0:0:0:1] - - [03/Jan/2025:16:41:18 +0000] "POST /connectors HTTP/1.1" 201 681 "-" "curl/8.7.1" 504 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-01-03 10:41:19,048] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=4, memberId='connect-192.168.1.5:8083-50ebba96-5ef2-4278-939c-c372fff74be9', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2025-01-03 10:41:19,050] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=4, memberId='connect-192.168.1.5:8083-50ebba96-5ef2-4278-939c-c372fff74be9', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2025-01-03 10:41:19,050] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Joined group at generation 4 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.1.5:8083-50ebba96-5ef2-4278-939c-c372fff74be9', leaderUrl='http://192.168.1.5:8083/', offset=5, connectorIds=[mariadb-connector, mongodb-sink], taskIds=[mongodb-sink-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2025-01-03 10:41:19,050] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 5 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2025-01-03 10:41:19,050] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connector mariadb-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2025-01-03 10:41:19,051] INFO [mariadb-connector|worker] Creating connector mariadb-connector of type io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:313)
[2025-01-03 10:41:19,051] INFO [mariadb-connector|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2025-01-03 10:41:19,051] INFO [mariadb-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 10:41:19,053] INFO [mariadb-connector|worker] Instantiated connector mariadb-connector with version 3.0.5.Final of type class io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:335)
[2025-01-03 10:41:19,053] INFO [mariadb-connector|worker] Finished creating connector mariadb-connector (org.apache.kafka.connect.runtime.Worker:356)
[2025-01-03 10:41:19,053] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2025-01-03 10:41:19,054] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2025-01-03 10:41:19,054] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 10:41:19,062] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Tasks [mariadb-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2025-01-03 10:41:19,062] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2025-01-03 10:41:19,062] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 10:41:19,063] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=5, memberId='connect-192.168.1.5:8083-50ebba96-5ef2-4278-939c-c372fff74be9', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2025-01-03 10:41:19,064] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=5, memberId='connect-192.168.1.5:8083-50ebba96-5ef2-4278-939c-c372fff74be9', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2025-01-03 10:41:19,064] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Joined group at generation 5 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.1.5:8083-50ebba96-5ef2-4278-939c-c372fff74be9', leaderUrl='http://192.168.1.5:8083/', offset=7, connectorIds=[mariadb-connector, mongodb-sink], taskIds=[mariadb-connector-0, mongodb-sink-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2025-01-03 10:41:19,065] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 7 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2025-01-03 10:41:19,065] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting task mariadb-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2025-01-03 10:41:19,065] INFO [mariadb-connector|task-0] Creating task mariadb-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2025-01-03 10:41:19,065] INFO [mariadb-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mariadb-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2025-01-03 10:41:19,065] INFO [mariadb-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mariadb-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 10:41:19,066] INFO [mariadb-connector|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.mysql.MySqlConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2025-01-03 10:41:19,067] INFO [mariadb-connector|task-0] Instantiated task mariadb-connector-0 with version 3.0.5.Final of type io.debezium.connector.mysql.MySqlConnectorTask (org.apache.kafka.connect.runtime.Worker:665)
[2025-01-03 10:41:19,068] INFO [mariadb-connector|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:41:19,068] INFO [mariadb-connector|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mariadb-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:678)
[2025-01-03 10:41:19,068] INFO [mariadb-connector|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:41:19,068] INFO [mariadb-connector|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mariadb-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:684)
[2025-01-03 10:41:19,068] INFO [mariadb-connector|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mariadb-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2025-01-03 10:41:19,068] INFO [mariadb-connector|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:1795)
[2025-01-03 10:41:19,069] INFO [mariadb-connector|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2025-01-03 10:41:19,069] INFO [mariadb-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 10:41:19,069] INFO [mariadb-connector|task-0] ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-mariadb-connector-0
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 10:41:19,069] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:41:19,072] INFO [mariadb-connector|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 10:41:19,072] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:41:19,072] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:41:19,072] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735922479072 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:41:19,075] INFO [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] Cluster ID: gMBptw3AQ7-7-73laciaMw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:41:19,076] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2025-01-03 10:41:19,077] INFO [mariadb-connector|task-0] Starting MySqlConnectorTask with configuration:
   connector.class = io.debezium.connector.mysql.MySqlConnector
   database.user = root
   database.server.id = 184054
   database.history.kafka.bootstrap.servers = localhost:9092
   database.history.kafka.topic = db.history.leafy_factory
   database.server.name = leafy_factory
   schema.history.internal.kafka.bootstrap.servers = localhost:9092
   database.port = 3306
   include.schema.changes = false
   topic.prefix = db_
   schema.history.internal.kafka.topic = db.history.internal
   task.class = io.debezium.connector.mysql.MySqlConnectorTask
   database.hostname = localhost
   database.password = ********
   name = mariadb-connector
   database.include.list = leafy_factory
 (io.debezium.connector.common.BaseSourceTask:250)
[2025-01-03 10:41:19,077] INFO [mariadb-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mysql.MySqlSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2025-01-03 10:41:19,078] INFO [mariadb-connector|task-0] Loading the custom topic naming strategy plugin: io.debezium.schema.DefaultTopicNamingStrategy (io.debezium.config.CommonConnectorConfig:1401)
[2025-01-03 10:41:19,113] INFO [mariadb-connector|task-0] Using 'SHOW MASTER STATUS' to get binary log status (io.debezium.connector.mysql.jdbc.MySqlConnection:41)
[2025-01-03 10:41:19,117] INFO [mariadb-connector|task-0] No previous offsets found (io.debezium.connector.common.BaseSourceTask:536)
[2025-01-03 10:41:19,134] INFO [mariadb-connector|task-0] KafkaSchemaHistory Consumer config: {key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, enable.auto.commit=false, group.id=db_-schemahistory, bootstrap.servers=localhost:9092, fetch.min.bytes=1, session.timeout.ms=10000, auto.offset.reset=earliest, client.id=db_-schemahistory} (io.debezium.storage.kafka.history.KafkaSchemaHistory:245)
[2025-01-03 10:41:19,134] INFO [mariadb-connector|task-0] KafkaSchemaHistory Producer config: {retries=1, value.serializer=org.apache.kafka.common.serialization.StringSerializer, acks=1, batch.size=32768, max.block.ms=10000, bootstrap.servers=localhost:9092, buffer.memory=1048576, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=db_-schemahistory, linger.ms=0} (io.debezium.storage.kafka.history.KafkaSchemaHistory:246)
[2025-01-03 10:41:19,134] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = db-history-config-check (io.debezium.util.Threads:270)
[2025-01-03 10:41:19,135] INFO [mariadb-connector|task-0] Idempotence will be disabled because acks is set to 1, not set to 'all'. (org.apache.kafka.clients.producer.ProducerConfig:587)
[2025-01-03 10:41:19,135] INFO [mariadb-connector|task-0] ProducerConfig values: 
	acks = 1
	auto.include.jmx.reporter = true
	batch.size = 32768
	bootstrap.servers = [localhost:9092]
	buffer.memory = 1048576
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 10:41:19,135] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:41:19,136] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:41:19,136] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:41:19,136] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735922479136 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:41:19,138] INFO [mariadb-connector|task-0] [Producer clientId=db_-schemahistory] Cluster ID: gMBptw3AQ7-7-73laciaMw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:41:19,188] INFO [mariadb-connector|task-0] Using 'SHOW MASTER STATUS' to get binary log status (io.debezium.connector.mysql.jdbc.MySqlConnection:41)
[2025-01-03 10:41:19,195] INFO [mariadb-connector|task-0] Closing connection before starting schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:123)
[2025-01-03 10:41:19,196] INFO [mariadb-connector|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:983)
[2025-01-03 10:41:19,196] INFO [mariadb-connector|task-0] Connector started for the first time. (io.debezium.connector.common.BaseSourceTask:89)
[2025-01-03 10:41:19,197] INFO [mariadb-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = db_-schemahistory
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 10:41:19,197] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:41:19,202] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:41:19,202] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:41:19,202] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735922479202 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:41:19,203] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Cluster ID: gMBptw3AQ7-7-73laciaMw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:41:19,205] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 10:41:19,205] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 10:41:19,207] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:41:19,207] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:41:19,207] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:41:19,207] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:41:19,208] INFO [mariadb-connector|task-0] App info kafka.consumer for db_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:41:19,208] INFO [mariadb-connector|task-0] AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:371)
[2025-01-03 10:41:19,209] INFO [mariadb-connector|task-0] These configurations '[value.serializer, acks, batch.size, max.block.ms, buffer.memory, key.serializer, linger.ms]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2025-01-03 10:41:19,209] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:41:19,209] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:41:19,210] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735922479209 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:41:19,262] INFO [mariadb-connector|task-0] Database schema history topic '(name=db.history.internal, numPartitions=1, replicationFactor=default, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807, retention.bytes=-1})' created (io.debezium.storage.kafka.history.KafkaSchemaHistory:555)
[2025-01-03 10:41:19,263] INFO [mariadb-connector|task-0] App info kafka.admin.client for db_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:41:19,263] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:41:19,263] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:41:19,263] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:41:19,263] INFO [mariadb-connector|task-0] Reconnecting after finishing schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:136)
[2025-01-03 10:41:19,306] INFO [mariadb-connector|task-0] No previous offset found (io.debezium.connector.mysql.MySqlConnectorTask:147)
[2025-01-03 10:41:19,315] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = SignalProcessor (io.debezium.util.Threads:270)
[2025-01-03 10:41:19,323] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = change-event-source-coordinator (io.debezium.util.Threads:270)
[2025-01-03 10:41:19,323] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = blocking-snapshot (io.debezium.util.Threads:270)
[2025-01-03 10:41:19,323] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-change-event-source-coordinator (io.debezium.util.Threads:287)
[2025-01-03 10:41:19,324] INFO [mariadb-connector|task-0] WorkerSourceTask{id=mariadb-connector-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.AbstractWorkerSourceTask:280)
[2025-01-03 10:41:19,324] INFO [mariadb-connector|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:137)
[2025-01-03 10:41:19,325] INFO [mariadb-connector|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:140)
[2025-01-03 10:41:19,327] INFO [mariadb-connector|task-0] According to the connector configuration both schema and data will be snapshot. (io.debezium.relational.RelationalSnapshotChangeEventSource:282)
[2025-01-03 10:41:19,328] INFO [mariadb-connector|task-0] Snapshot step 1 - Preparing (io.debezium.relational.RelationalSnapshotChangeEventSource:135)
[2025-01-03 10:41:19,329] INFO [mariadb-connector|task-0] Snapshot step 2 - Determining captured tables (io.debezium.relational.RelationalSnapshotChangeEventSource:144)
[2025-01-03 10:41:19,329] INFO [mariadb-connector|task-0] Read list of available databases (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:116)
[2025-01-03 10:41:19,330] INFO [mariadb-connector|task-0] 	 list of available databases is: [information_schema, leafy_factory, mysql, performance_schema, sys, test] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:118)
[2025-01-03 10:41:19,330] INFO [mariadb-connector|task-0] Read list of available tables in each database (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:126)
[2025-01-03 10:41:19,365] INFO [mariadb-connector|task-0] 	snapshot continuing with database(s): [leafy_factory] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:147)
[2025-01-03 10:41:19,365] INFO [mariadb-connector|task-0] Adding table leafy_factory.jobs_machines to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:41:19,365] INFO [mariadb-connector|task-0] Adding table leafy_factory.product_cost to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:41:19,365] INFO [mariadb-connector|task-0] Adding table leafy_factory.production_lines to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:41:19,365] INFO [mariadb-connector|task-0] Adding table leafy_factory.products_raw_materials to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:41:19,365] INFO [mariadb-connector|task-0] Adding table leafy_factory.machines to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:41:19,365] INFO [mariadb-connector|task-0] Adding table leafy_factory.jobs to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:41:19,365] INFO [mariadb-connector|task-0] Adding table leafy_factory.raw_materials to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:41:19,366] INFO [mariadb-connector|task-0] Adding table leafy_factory.work_orders to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:41:19,366] INFO [mariadb-connector|task-0] Adding table leafy_factory.products to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:41:19,366] INFO [mariadb-connector|task-0] Adding table leafy_factory.factories to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:41:19,367] INFO [mariadb-connector|task-0] Created connection pool with 1 threads (io.debezium.relational.RelationalSnapshotChangeEventSource:236)
[2025-01-03 10:41:19,367] INFO [mariadb-connector|task-0] Snapshot step 3 - Locking captured tables [leafy_factory.factories, leafy_factory.jobs, leafy_factory.jobs_machines, leafy_factory.machines, leafy_factory.product_cost, leafy_factory.production_lines, leafy_factory.products, leafy_factory.products_raw_materials, leafy_factory.raw_materials, leafy_factory.work_orders] (io.debezium.relational.RelationalSnapshotChangeEventSource:153)
[2025-01-03 10:41:19,368] INFO [mariadb-connector|task-0] Flush and obtain global read lock to prevent writes to database (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:488)
[2025-01-03 10:41:19,369] INFO [mariadb-connector|task-0] Snapshot step 4 - Determining snapshot offset (io.debezium.relational.RelationalSnapshotChangeEventSource:159)
[2025-01-03 10:41:19,370] INFO [mariadb-connector|task-0] Read binlog position of MySQL primary server (io.debezium.connector.mysql.MySqlSnapshotChangeEventSource:58)
[2025-01-03 10:41:19,372] INFO [mariadb-connector|task-0] Snapshot step 5 - Reading structure of captured tables (io.debezium.relational.RelationalSnapshotChangeEventSource:162)
[2025-01-03 10:41:19,372] INFO [mariadb-connector|task-0] All eligible tables schema should be captured, capturing: [leafy_factory.factories, leafy_factory.jobs, leafy_factory.jobs_machines, leafy_factory.machines, leafy_factory.product_cost, leafy_factory.production_lines, leafy_factory.products, leafy_factory.products_raw_materials, leafy_factory.raw_materials, leafy_factory.work_orders] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:314)
[2025-01-03 10:41:19,923] INFO [mariadb-connector|task-0] Reading structure of database 'leafy_factory' (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:348)
[2025-01-03 10:41:20,013] INFO [mariadb-connector|task-0] Snapshot step 6 - Persisting schema history (io.debezium.relational.RelationalSnapshotChangeEventSource:166)
[2025-01-03 10:41:20,053] INFO [mariadb-connector|task-0] Releasing global read lock to enable MySQL writes (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:497)
[2025-01-03 10:41:20,054] INFO [mariadb-connector|task-0] Writes to MySQL tables prevented for a total of 00:00:00.685 (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:501)
[2025-01-03 10:41:20,054] INFO [mariadb-connector|task-0] Snapshot step 7 - Snapshotting data (io.debezium.relational.RelationalSnapshotChangeEventSource:178)
[2025-01-03 10:41:20,054] INFO [mariadb-connector|task-0] Creating snapshot worker pool with 1 worker thread(s) (io.debezium.relational.RelationalSnapshotChangeEventSource:480)
[2025-01-03 10:41:20,055] INFO [mariadb-connector|task-0] For table 'leafy_factory.factories' using select statement: 'SELECT `id_factory`, `factory_name`, `factory_location`, `factory_timestamp` FROM `leafy_factory`.`factories`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:41:20,058] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.factories is OptionalLong[0] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:41:20,058] INFO [mariadb-connector|task-0] For table 'leafy_factory.jobs' using select statement: 'SELECT `id_job`, `target_output`, `nOk_products`, `quality_rate`, `job_status`, `creation_date`, `work_id` FROM `leafy_factory`.`jobs`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:41:20,059] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.jobs is OptionalLong[0] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:41:20,059] INFO [mariadb-connector|task-0] For table 'leafy_factory.jobs_machines' using select statement: 'SELECT `id_jobs_machines`, `job_id`, `machine_id` FROM `leafy_factory`.`jobs_machines`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:41:20,060] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.jobs_machines is OptionalLong[2] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:41:20,060] INFO [mariadb-connector|task-0] For table 'leafy_factory.machines' using select statement: 'SELECT `id_machine`, `machine_status`, `last_maintenance`, `operator`, `avg_output`, `reject_count`, `production_line_id` FROM `leafy_factory`.`machines`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:41:20,061] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.machines is OptionalLong[4] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:41:20,061] INFO [mariadb-connector|task-0] For table 'leafy_factory.product_cost' using select statement: 'SELECT `id_cost`, `raw_material_cost_per_product`, `overhead_per_product`, `total_cost_per_product`, `cost_ok_with_overhead`, `cost_nok_with_overhead`, `actual_total_cost`, `work_id` FROM `leafy_factory`.`product_cost`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:41:20,063] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.product_cost is OptionalLong[12] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:41:20,063] INFO [mariadb-connector|task-0] For table 'leafy_factory.production_lines' using select statement: 'SELECT `id_production_line`, `factory_id` FROM `leafy_factory`.`production_lines`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:41:20,064] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.production_lines is OptionalLong[2] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:41:20,064] INFO [mariadb-connector|task-0] For table 'leafy_factory.products' using select statement: 'SELECT `id_product`, `product_name`, `product_description` FROM `leafy_factory`.`products`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:41:20,065] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.products is OptionalLong[2] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:41:20,065] INFO [mariadb-connector|task-0] For table 'leafy_factory.products_raw_materials' using select statement: 'SELECT `id_products_raw_materials`, `product_id`, `raw_material_id` FROM `leafy_factory`.`products_raw_materials`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:41:20,066] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.products_raw_materials is OptionalLong[9] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:41:20,066] INFO [mariadb-connector|task-0] For table 'leafy_factory.raw_materials' using select statement: 'SELECT `id_raw_material`, `item_code`, `raw_material_name`, `raw_material_description`, `unit_measurement`, `raw_material_stock`, `raw_material_status`, `raw_material_currency`, `cost_per_part` FROM `leafy_factory`.`raw_materials`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:41:20,067] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.raw_materials is OptionalLong[8] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:41:20,067] INFO [mariadb-connector|task-0] For table 'leafy_factory.work_orders' using select statement: 'SELECT `id_work`, `planned_start_date`, `planned_end_date`, `actual_start_date`, `actual_end_date`, `quantity`, `wo_status`, `creation_date`, `product_id`, `nOk_products` FROM `leafy_factory`.`work_orders`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:41:20,068] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.work_orders is OptionalLong[12] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:41:20,069] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.factories' (1 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:41:20,081] INFO [mariadb-connector|task-0] 	 Finished exporting 1 records for table 'leafy_factory.factories' (1 of 10 tables); total duration '00:00:00.012' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:41:20,082] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.jobs' (2 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:41:20,084] INFO [mariadb-connector|task-0] 	 Finished exporting 1 records for table 'leafy_factory.jobs' (2 of 10 tables); total duration '00:00:00.002' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:41:20,085] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.jobs_machines' (3 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:41:20,086] INFO [mariadb-connector|task-0] 	 Finished exporting 2 records for table 'leafy_factory.jobs_machines' (3 of 10 tables); total duration '00:00:00.001' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:41:20,086] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.machines' (4 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:41:20,088] INFO [mariadb-connector|task-0] 	 Finished exporting 4 records for table 'leafy_factory.machines' (4 of 10 tables); total duration '00:00:00.002' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:41:20,088] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.product_cost' (5 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:41:20,090] INFO [mariadb-connector|task-0] 	 Finished exporting 21 records for table 'leafy_factory.product_cost' (5 of 10 tables); total duration '00:00:00.002' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:41:20,091] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.production_lines' (6 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:41:20,092] INFO [mariadb-connector|task-0] 	 Finished exporting 2 records for table 'leafy_factory.production_lines' (6 of 10 tables); total duration '00:00:00.001' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:41:20,092] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.products' (7 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:41:20,093] INFO [mariadb-connector|task-0] 	 Finished exporting 2 records for table 'leafy_factory.products' (7 of 10 tables); total duration '00:00:00.001' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:41:20,093] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.products_raw_materials' (8 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:41:20,095] INFO [mariadb-connector|task-0] 	 Finished exporting 8 records for table 'leafy_factory.products_raw_materials' (8 of 10 tables); total duration '00:00:00.002' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:41:20,095] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.raw_materials' (9 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:41:20,097] INFO [mariadb-connector|task-0] 	 Finished exporting 8 records for table 'leafy_factory.raw_materials' (9 of 10 tables); total duration '00:00:00.002' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:41:20,097] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.work_orders' (10 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:41:20,103] INFO [mariadb-connector|task-0] 	 Finished exporting 21 records for table 'leafy_factory.work_orders' (10 of 10 tables); total duration '00:00:00.006' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:41:20,105] INFO [mariadb-connector|task-0] Snapshot - Final stage (io.debezium.pipeline.source.AbstractSnapshotChangeEventSource:108)
[2025-01-03 10:41:20,105] INFO [mariadb-connector|task-0] Snapshot completed (io.debezium.pipeline.source.AbstractSnapshotChangeEventSource:112)
[2025-01-03 10:41:20,112] INFO [mariadb-connector|task-0] Snapshot ended with SnapshotResult [status=COMPLETED, offset=BinlogOffsetContext{sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=BinlogSourceInfo{currentGtid='null', currentBinlogFilename='mariadb-bin.000001', currentBinlogPosition=52374, currentRowNumber=0, serverId=0, sourceTime=2025-01-03T16:41:20Z, threadId=-1, currentQuery='null', tableIds=[leafy_factory.work_orders], databaseName='leafy_factory'}, snapshotCompleted=true, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet='null', currentGtidSet='null', restartBinlogFilename='mariadb-bin.000001', restartBinlogPosition=52374, restartRowsToSkip=0, restartEventsToSkip=0, currentEventLengthInBytes=0, inTransaction=false, transactionId='null', incrementalSnapshotContext=IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]}] (io.debezium.pipeline.ChangeEventSourceCoordinator:298)
[2025-01-03 10:41:20,114] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = binlog-client (io.debezium.util.Threads:270)
[2025-01-03 10:41:20,116] INFO [mariadb-connector|task-0] Enable ssl PREFERRED mode for connector db_ (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:1289)
[2025-01-03 10:41:20,119] INFO [mariadb-connector|task-0] SignalProcessor started. Scheduling it every 5000ms (io.debezium.pipeline.signal.SignalProcessor:105)
[2025-01-03 10:41:20,120] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-SignalProcessor (io.debezium.util.Threads:287)
[2025-01-03 10:41:20,120] INFO [mariadb-connector|task-0] Starting streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:323)
[2025-01-03 10:41:20,121] INFO [mariadb-connector|task-0] Skip 0 events on streaming start (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:278)
[2025-01-03 10:41:20,121] INFO [mariadb-connector|task-0] Skip 0 rows on streaming start (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:282)
[2025-01-03 10:41:20,121] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-binlog-client (io.debezium.util.Threads:287)
[2025-01-03 10:41:20,122] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-binlog-client (io.debezium.util.Threads:287)
[2025-01-03 10:41:20,146] INFO [mariadb-connector|task-0] Connected to binlog at localhost:3306, starting at BinlogOffsetContext{sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=BinlogSourceInfo{currentGtid='null', currentBinlogFilename='mariadb-bin.000001', currentBinlogPosition=52374, currentRowNumber=0, serverId=0, sourceTime=2025-01-03T16:41:20Z, threadId=-1, currentQuery='null', tableIds=[leafy_factory.work_orders], databaseName='leafy_factory'}, snapshotCompleted=true, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet='null', currentGtidSet='null', restartBinlogFilename='mariadb-bin.000001', restartBinlogPosition=52374, restartRowsToSkip=0, restartEventsToSkip=0, currentEventLengthInBytes=0, inTransaction=false, transactionId='null', incrementalSnapshotContext=IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]} (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:1232)
[2025-01-03 10:41:20,147] INFO [mariadb-connector|task-0] Waiting for keepalive thread to start (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:299)
[2025-01-03 10:41:20,147] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-binlog-client (io.debezium.util.Threads:287)
[2025-01-03 10:41:20,252] INFO [mariadb-connector|task-0] Keepalive thread is running (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:306)
[2025-01-03 10:41:20,347] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 4 : {db_.leafy_factory.factories=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:41:20,485] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 7 : {db_.leafy_factory.jobs=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:41:20,609] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 10 : {db_.leafy_factory.jobs_machines=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:41:20,740] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 13 : {db_.leafy_factory.machines=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:41:20,882] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 17 : {db_.leafy_factory.product_cost=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:41:21,028] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 22 : {db_.leafy_factory.production_lines=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:41:21,158] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 31 : {db_.leafy_factory.products=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:41:21,302] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 35 : {db_.leafy_factory.products_raw_materials=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:41:21,434] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 39 : {db_.leafy_factory.raw_materials=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:41:21,577] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 44 : {db_.leafy_factory.work_orders=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:41:29,081] INFO [mariadb-connector|task-0|offsets] WorkerSourceTask{id=mariadb-connector-0} Committing offsets for 70 acknowledged messages (org.apache.kafka.connect.runtime.WorkerSourceTask:236)
[2025-01-03 10:41:38,241] INFO [0:0:0:0:0:0:0:1] - - [03/Jan/2025:16:41:38 +0000] "POST /mariadb-connector/status HTTP/1.1" 404 49 "-" "curl/8.7.1" 3 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-01-03 10:41:45,727] INFO [0:0:0:0:0:0:0:1] - - [03/Jan/2025:16:41:45 +0000] "GET /connectors/mariadb-connector/status HTTP/1.1" 200 175 "-" "curl/8.7.1" 11 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-01-03 10:42:12,182] INFO [0:0:0:0:0:0:0:1] - - [03/Jan/2025:16:42:12 +0000] "GET /connectors/mongodb-sink/status HTTP/1.1" 200 168 "-" "curl/8.7.1" 4 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-01-03 10:43:40,289] INFO [mariadb-connector|task-0] 76 records sent during previous 00:02:21.221, last recorded offset of {server=db_} partition is {ts_sec=1735922620, file=mariadb-bin.000001, pos=0, row=1, server_id=1} (io.debezium.connector.common.BaseSourceTask:351)
[2025-01-03 10:43:49,187] INFO [mariadb-connector|task-0|offsets] WorkerSourceTask{id=mariadb-connector-0} Committing offsets for 6 acknowledged messages (org.apache.kafka.connect.runtime.WorkerSourceTask:236)
[2025-01-03 10:45:44,576] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Request joining group due to: cached metadata has changed from (version2: {}) at the beginning of the rebalance to (version3: {db_.leafy_factory.factories=[NO_RACKS], db_.leafy_factory.production_lines=[NO_RACKS], db_.leafy_factory.machines=[NO_RACKS], db_.leafy_factory.jobs_machines=[NO_RACKS], db_.leafy_factory.work_orders=[NO_RACKS], db_.leafy_factory.jobs=[NO_RACKS], db_.leafy_factory.products_raw_materials=[NO_RACKS], db_.leafy_factory.raw_materials=[NO_RACKS], db_.leafy_factory.product_cost=[NO_RACKS], db_.leafy_factory.products=[NO_RACKS]}) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 10:45:44,582] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:80)
[2025-01-03 10:45:44,584] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:605)
[2025-01-03 10:45:44,591] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Successfully joined group with generation Generation{generationId=2, memberId='connector-consumer-mongodb-sink-0-db949108-291d-4c08-87d7-ecdf87a8dc34', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:666)
[2025-01-03 10:45:44,596] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Finished assignment for group at generation 2: {connector-consumer-mongodb-sink-0-db949108-291d-4c08-87d7-ecdf87a8dc34=Assignment(partitions=[db_.leafy_factory.factories-0, db_.leafy_factory.machines-0, db_.leafy_factory.production_lines-0, db_.leafy_factory.jobs_machines-0, db_.leafy_factory.work_orders-0, db_.leafy_factory.jobs-0, db_.leafy_factory.products_raw_materials-0, db_.leafy_factory.raw_materials-0, db_.leafy_factory.product_cost-0, db_.leafy_factory.products-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:664)
[2025-01-03 10:45:44,600] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Successfully synced group in generation Generation{generationId=2, memberId='connector-consumer-mongodb-sink-0-db949108-291d-4c08-87d7-ecdf87a8dc34', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:843)
[2025-01-03 10:45:44,600] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Notifying assignor about the new Assignment(partitions=[db_.leafy_factory.factories-0, db_.leafy_factory.machines-0, db_.leafy_factory.production_lines-0, db_.leafy_factory.jobs_machines-0, db_.leafy_factory.work_orders-0, db_.leafy_factory.jobs-0, db_.leafy_factory.products_raw_materials-0, db_.leafy_factory.raw_materials-0, db_.leafy_factory.product_cost-0, db_.leafy_factory.products-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:324)
[2025-01-03 10:45:44,601] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Adding newly assigned partitions: db_.leafy_factory.factories-0, db_.leafy_factory.jobs-0, db_.leafy_factory.jobs_machines-0, db_.leafy_factory.machines-0, db_.leafy_factory.product_cost-0, db_.leafy_factory.production_lines-0, db_.leafy_factory.products-0, db_.leafy_factory.products_raw_materials-0, db_.leafy_factory.raw_materials-0, db_.leafy_factory.work_orders-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:58)
[2025-01-03 10:45:44,604] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Found no committed offset for partition db_.leafy_factory.machines-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1508)
[2025-01-03 10:45:44,604] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Found no committed offset for partition db_.leafy_factory.production_lines-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1508)
[2025-01-03 10:45:44,604] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Found no committed offset for partition db_.leafy_factory.products_raw_materials-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1508)
[2025-01-03 10:45:44,604] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Found no committed offset for partition db_.leafy_factory.factories-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1508)
[2025-01-03 10:45:44,605] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Found no committed offset for partition db_.leafy_factory.jobs_machines-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1508)
[2025-01-03 10:45:44,605] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Found no committed offset for partition db_.leafy_factory.work_orders-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1508)
[2025-01-03 10:45:44,605] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Found no committed offset for partition db_.leafy_factory.raw_materials-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1508)
[2025-01-03 10:45:44,605] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Found no committed offset for partition db_.leafy_factory.products-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1508)
[2025-01-03 10:45:44,605] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Found no committed offset for partition db_.leafy_factory.product_cost-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1508)
[2025-01-03 10:45:44,605] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Found no committed offset for partition db_.leafy_factory.jobs-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1508)
[2025-01-03 10:45:44,606] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Resetting offset for partition db_.leafy_factory.machines-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:45:44,607] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Resetting offset for partition db_.leafy_factory.production_lines-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:45:44,607] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Resetting offset for partition db_.leafy_factory.factories-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:45:44,607] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Resetting offset for partition db_.leafy_factory.products_raw_materials-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:45:44,607] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Resetting offset for partition db_.leafy_factory.work_orders-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:45:44,607] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Resetting offset for partition db_.leafy_factory.jobs_machines-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:45:44,607] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Resetting offset for partition db_.leafy_factory.raw_materials-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:45:44,607] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Resetting offset for partition db_.leafy_factory.product_cost-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:45:44,607] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Resetting offset for partition db_.leafy_factory.products-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:45:44,608] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Resetting offset for partition db_.leafy_factory.jobs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:45:44,708] INFO [mongodb-sink|task-0] MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = ${topic}
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = db_.leafy_factory.machines
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 10:45:44,711] INFO [mongodb-sink|task-0] MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = ${topic}
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = db_.leafy_factory.production_lines
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 10:45:44,712] INFO [mongodb-sink|task-0] MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = ${topic}
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = db_.leafy_factory.factories
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 10:45:44,712] INFO [mongodb-sink|task-0] MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = ${topic}
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = db_.leafy_factory.products_raw_materials
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 10:45:44,712] INFO [mongodb-sink|task-0] MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = ${topic}
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = db_.leafy_factory.jobs_machines
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 10:45:44,713] INFO [mongodb-sink|task-0] MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = ${topic}
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = db_.leafy_factory.work_orders
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 10:45:44,713] INFO [mongodb-sink|task-0] MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = ${topic}
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = db_.leafy_factory.raw_materials
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 10:45:44,714] INFO [mongodb-sink|task-0] MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = ${topic}
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = db_.leafy_factory.product_cost
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 10:45:44,715] INFO [mongodb-sink|task-0] MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = ${topic}
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = db_.leafy_factory.products
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 10:45:44,715] INFO [mongodb-sink|task-0] MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = ${topic}
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = db_.leafy_factory.jobs
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 10:45:46,259] INFO [mongodb-sink|task-0] Opened connection [connectionId{localValue:14, serverValue:1293069}] to ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 10:45:46,513] ERROR [mongodb-sink|task-0] Failed to put into the sink the following records: [SinkRecord{kafkaOffset=0, timestampType=CreateTime, originalTopic=db_.leafy_factory.machines, originalKafkaPartition=0, originalKafkaOffset=0} ConnectRecord{topic='db_.leafy_factory.machines', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_machine"}],"optional":false,"name":"db_.leafy_factory.machines.Key"},"payload":{"id_machine":1}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_machine=1,machine_status=Running,last_maintenance=1730384700000,operator=Ada Lovelace,avg_output=3000.00,reject_count=25.00,production_line_id=1},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=first_in_data_collection,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=machines,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480087,ts_us=1735922480087619,ts_ns=1735922480087619000}, valueSchema=Schema{db_.leafy_factory.machines.Envelope:STRUCT}, timestamp=1735922480863, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=1, timestampType=CreateTime, originalTopic=db_.leafy_factory.machines, originalKafkaPartition=0, originalKafkaOffset=1} ConnectRecord{topic='db_.leafy_factory.machines', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_machine"}],"optional":false,"name":"db_.leafy_factory.machines.Key"},"payload":{"id_machine":2}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_machine=2,machine_status=Running,last_maintenance=1730384700000,operator=Claude Jones,avg_output=3000.00,reject_count=25.00,production_line_id=1},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=machines,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480087,ts_us=1735922480087757,ts_ns=1735922480087757000}, valueSchema=Schema{db_.leafy_factory.machines.Envelope:STRUCT}, timestamp=1735922480864, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=2, timestampType=CreateTime, originalTopic=db_.leafy_factory.machines, originalKafkaPartition=0, originalKafkaOffset=2} ConnectRecord{topic='db_.leafy_factory.machines', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_machine"}],"optional":false,"name":"db_.leafy_factory.machines.Key"},"payload":{"id_machine":3}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_machine=3,machine_status=Available,last_maintenance=1730384700000,operator=Grace Conway,avg_output=3000.00,reject_count=25.00,production_line_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=machines,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480087,ts_us=1735922480087887,ts_ns=1735922480087887000}, valueSchema=Schema{db_.leafy_factory.machines.Envelope:STRUCT}, timestamp=1735922480864, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=3, timestampType=CreateTime, originalTopic=db_.leafy_factory.machines, originalKafkaPartition=0, originalKafkaOffset=3} ConnectRecord{topic='db_.leafy_factory.machines', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_machine"}],"optional":false,"name":"db_.leafy_factory.machines.Key"},"payload":{"id_machine":4}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_machine=4,machine_status=Available,last_maintenance=1730384700000,operator=Frida Sidik,avg_output=3000.00,reject_count=25.00,production_line_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=last_in_data_collection,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=machines,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480088,ts_us=1735922480088077,ts_ns=1735922480088077000}, valueSchema=Schema{db_.leafy_factory.machines.Envelope:STRUCT}, timestamp=1735922480865, headers=ConnectHeaders(headers=)}] (com.mongodb.kafka.connect.sink.MongoSinkTask:247)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922746, "i": 5}}, "signature": {"hash": {"$binary": {"base64": "P5OIYUf3F2qf/ZMwsVYZiHPYNJ0=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922746, "i": 5}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:46,522] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.machines', partition=0, offset=0, timestamp=1735922480863, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922746, "i": 5}}, "signature": {"hash": {"$binary": {"base64": "P5OIYUf3F2qf/ZMwsVYZiHPYNJ0=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922746, "i": 5}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:46,524] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.machines', partition=0, offset=1, timestamp=1735922480864, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922746, "i": 5}}, "signature": {"hash": {"$binary": {"base64": "P5OIYUf3F2qf/ZMwsVYZiHPYNJ0=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922746, "i": 5}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:46,524] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.machines', partition=0, offset=2, timestamp=1735922480864, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922746, "i": 5}}, "signature": {"hash": {"$binary": {"base64": "P5OIYUf3F2qf/ZMwsVYZiHPYNJ0=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922746, "i": 5}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:46,525] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.machines', partition=0, offset=3, timestamp=1735922480865, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922746, "i": 5}}, "signature": {"hash": {"$binary": {"base64": "P5OIYUf3F2qf/ZMwsVYZiHPYNJ0=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922746, "i": 5}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:46,665] ERROR [mongodb-sink|task-0] Failed to put into the sink the following records: [SinkRecord{kafkaOffset=0, timestampType=CreateTime, originalTopic=db_.leafy_factory.production_lines, originalKafkaPartition=0, originalKafkaOffset=0} ConnectRecord{topic='db_.leafy_factory.production_lines', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_production_line"}],"optional":false,"name":"db_.leafy_factory.production_lines.Key"},"payload":{"id_production_line":1}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_production_line=1,factory_id=1},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=first_in_data_collection,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=production_lines,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480092,ts_us=1735922480092079,ts_ns=1735922480092079000}, valueSchema=Schema{db_.leafy_factory.production_lines.Envelope:STRUCT}, timestamp=1735922481134, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=1, timestampType=CreateTime, originalTopic=db_.leafy_factory.production_lines, originalKafkaPartition=0, originalKafkaOffset=1} ConnectRecord{topic='db_.leafy_factory.production_lines', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_production_line"}],"optional":false,"name":"db_.leafy_factory.production_lines.Key"},"payload":{"id_production_line":2}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_production_line=2,factory_id=1},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=last_in_data_collection,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=production_lines,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480092,ts_us=1735922480092144,ts_ns=1735922480092144000}, valueSchema=Schema{db_.leafy_factory.production_lines.Envelope:STRUCT}, timestamp=1735922481135, headers=ConnectHeaders(headers=)}] (com.mongodb.kafka.connect.sink.MongoSinkTask:247)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922746, "i": 6}}, "signature": {"hash": {"$binary": {"base64": "P5OIYUf3F2qf/ZMwsVYZiHPYNJ0=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922746, "i": 6}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:46,667] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.production_lines', partition=0, offset=0, timestamp=1735922481134, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922746, "i": 6}}, "signature": {"hash": {"$binary": {"base64": "P5OIYUf3F2qf/ZMwsVYZiHPYNJ0=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922746, "i": 6}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:46,668] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.production_lines', partition=0, offset=1, timestamp=1735922481135, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922746, "i": 6}}, "signature": {"hash": {"$binary": {"base64": "P5OIYUf3F2qf/ZMwsVYZiHPYNJ0=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922746, "i": 6}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:46,827] ERROR [mongodb-sink|task-0] Failed to put into the sink the following records: [SinkRecord{kafkaOffset=0, timestampType=CreateTime, originalTopic=db_.leafy_factory.factories, originalKafkaPartition=0, originalKafkaOffset=0} ConnectRecord{topic='db_.leafy_factory.factories', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_factory"}],"optional":false,"name":"db_.leafy_factory.factories.Key"},"payload":{"id_factory":1}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_factory=1,factory_name=qro_fact_1,factory_location=Plant A,factory_timestamp=1730384700000},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=first,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=factories,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480080,ts_us=1735922480080425,ts_ns=1735922480080425000}, valueSchema=Schema{db_.leafy_factory.factories.Envelope:STRUCT}, timestamp=1735922480468, headers=ConnectHeaders(headers=)}] (com.mongodb.kafka.connect.sink.MongoSinkTask:247)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922746, "i": 6}}, "signature": {"hash": {"$binary": {"base64": "P5OIYUf3F2qf/ZMwsVYZiHPYNJ0=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922746, "i": 6}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:46,829] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.factories', partition=0, offset=0, timestamp=1735922480468, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922746, "i": 6}}, "signature": {"hash": {"$binary": {"base64": "P5OIYUf3F2qf/ZMwsVYZiHPYNJ0=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922746, "i": 6}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,063] ERROR [mongodb-sink|task-0] Failed to put into the sink the following records: [SinkRecord{kafkaOffset=0, timestampType=CreateTime, originalTopic=db_.leafy_factory.products_raw_materials, originalKafkaPartition=0, originalKafkaOffset=0} ConnectRecord{topic='db_.leafy_factory.products_raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_products_raw_materials"}],"optional":false,"name":"db_.leafy_factory.products_raw_materials.Key"},"payload":{"id_products_raw_materials":1}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_products_raw_materials=1,product_id=1,raw_material_id=1},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=first_in_data_collection,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=products_raw_materials,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480094,ts_us=1735922480094902,ts_ns=1735922480094902000}, valueSchema=Schema{db_.leafy_factory.products_raw_materials.Envelope:STRUCT}, timestamp=1735922481414, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=1, timestampType=CreateTime, originalTopic=db_.leafy_factory.products_raw_materials, originalKafkaPartition=0, originalKafkaOffset=1} ConnectRecord{topic='db_.leafy_factory.products_raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_products_raw_materials"}],"optional":false,"name":"db_.leafy_factory.products_raw_materials.Key"},"payload":{"id_products_raw_materials":2}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_products_raw_materials=2,product_id=1,raw_material_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=products_raw_materials,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480094,ts_us=1735922480094958,ts_ns=1735922480094958000}, valueSchema=Schema{db_.leafy_factory.products_raw_materials.Envelope:STRUCT}, timestamp=1735922481414, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=2, timestampType=CreateTime, originalTopic=db_.leafy_factory.products_raw_materials, originalKafkaPartition=0, originalKafkaOffset=2} ConnectRecord{topic='db_.leafy_factory.products_raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_products_raw_materials"}],"optional":false,"name":"db_.leafy_factory.products_raw_materials.Key"},"payload":{"id_products_raw_materials":3}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_products_raw_materials=3,product_id=1,raw_material_id=3},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=products_raw_materials,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480095,ts_us=1735922480095000,ts_ns=1735922480095000000}, valueSchema=Schema{db_.leafy_factory.products_raw_materials.Envelope:STRUCT}, timestamp=1735922481415, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=3, timestampType=CreateTime, originalTopic=db_.leafy_factory.products_raw_materials, originalKafkaPartition=0, originalKafkaOffset=3} ConnectRecord{topic='db_.leafy_factory.products_raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_products_raw_materials"}],"optional":false,"name":"db_.leafy_factory.products_raw_materials.Key"},"payload":{"id_products_raw_materials":4}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_products_raw_materials=4,product_id=1,raw_material_id=4},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=products_raw_materials,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480095,ts_us=1735922480095047,ts_ns=1735922480095047000}, valueSchema=Schema{db_.leafy_factory.products_raw_materials.Envelope:STRUCT}, timestamp=1735922481415, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=4, timestampType=CreateTime, originalTopic=db_.leafy_factory.products_raw_materials, originalKafkaPartition=0, originalKafkaOffset=4} ConnectRecord{topic='db_.leafy_factory.products_raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_products_raw_materials"}],"optional":false,"name":"db_.leafy_factory.products_raw_materials.Key"},"payload":{"id_products_raw_materials":5}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_products_raw_materials=5,product_id=2,raw_material_id=5},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=products_raw_materials,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480095,ts_us=1735922480095100,ts_ns=1735922480095100000}, valueSchema=Schema{db_.leafy_factory.products_raw_materials.Envelope:STRUCT}, timestamp=1735922481415, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=5, timestampType=CreateTime, originalTopic=db_.leafy_factory.products_raw_materials, originalKafkaPartition=0, originalKafkaOffset=5} ConnectRecord{topic='db_.leafy_factory.products_raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_products_raw_materials"}],"optional":false,"name":"db_.leafy_factory.products_raw_materials.Key"},"payload":{"id_products_raw_materials":6}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_products_raw_materials=6,product_id=2,raw_material_id=6},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=products_raw_materials,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480095,ts_us=1735922480095141,ts_ns=1735922480095141000}, valueSchema=Schema{db_.leafy_factory.products_raw_materials.Envelope:STRUCT}, timestamp=1735922481415, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=6, timestampType=CreateTime, originalTopic=db_.leafy_factory.products_raw_materials, originalKafkaPartition=0, originalKafkaOffset=6} ConnectRecord{topic='db_.leafy_factory.products_raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_products_raw_materials"}],"optional":false,"name":"db_.leafy_factory.products_raw_materials.Key"},"payload":{"id_products_raw_materials":7}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_products_raw_materials=7,product_id=2,raw_material_id=7},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=products_raw_materials,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480095,ts_us=1735922480095183,ts_ns=1735922480095183000}, valueSchema=Schema{db_.leafy_factory.products_raw_materials.Envelope:STRUCT}, timestamp=1735922481415, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=7, timestampType=CreateTime, originalTopic=db_.leafy_factory.products_raw_materials, originalKafkaPartition=0, originalKafkaOffset=7} ConnectRecord{topic='db_.leafy_factory.products_raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_products_raw_materials"}],"optional":false,"name":"db_.leafy_factory.products_raw_materials.Key"},"payload":{"id_products_raw_materials":8}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_products_raw_materials=8,product_id=2,raw_material_id=8},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=last_in_data_collection,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=products_raw_materials,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480095,ts_us=1735922480095227,ts_ns=1735922480095227000}, valueSchema=Schema{db_.leafy_factory.products_raw_materials.Envelope:STRUCT}, timestamp=1735922481415, headers=ConnectHeaders(headers=)}] (com.mongodb.kafka.connect.sink.MongoSinkTask:247)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,065] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.products_raw_materials', partition=0, offset=0, timestamp=1735922481414, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,066] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.products_raw_materials', partition=0, offset=1, timestamp=1735922481414, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,066] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.products_raw_materials', partition=0, offset=2, timestamp=1735922481415, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,067] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.products_raw_materials', partition=0, offset=3, timestamp=1735922481415, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,068] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.products_raw_materials', partition=0, offset=4, timestamp=1735922481415, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,069] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.products_raw_materials', partition=0, offset=5, timestamp=1735922481415, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,069] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.products_raw_materials', partition=0, offset=6, timestamp=1735922481415, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,070] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.products_raw_materials', partition=0, offset=7, timestamp=1735922481415, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,225] ERROR [mongodb-sink|task-0] Failed to put into the sink the following records: [SinkRecord{kafkaOffset=0, timestampType=CreateTime, originalTopic=db_.leafy_factory.jobs_machines, originalKafkaPartition=0, originalKafkaOffset=0} ConnectRecord{topic='db_.leafy_factory.jobs_machines', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_jobs_machines"}],"optional":false,"name":"db_.leafy_factory.jobs_machines.Key"},"payload":{"id_jobs_machines":1}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_jobs_machines=1,job_id=1,machine_id=1},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=first_in_data_collection,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=jobs_machines,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480086,ts_us=1735922480086153,ts_ns=1735922480086153000}, valueSchema=Schema{db_.leafy_factory.jobs_machines.Envelope:STRUCT}, timestamp=1735922480721, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=1, timestampType=CreateTime, originalTopic=db_.leafy_factory.jobs_machines, originalKafkaPartition=0, originalKafkaOffset=1} ConnectRecord{topic='db_.leafy_factory.jobs_machines', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_jobs_machines"}],"optional":false,"name":"db_.leafy_factory.jobs_machines.Key"},"payload":{"id_jobs_machines":2}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_jobs_machines=2,job_id=1,machine_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=last_in_data_collection,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=jobs_machines,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480086,ts_us=1735922480086255,ts_ns=1735922480086255000}, valueSchema=Schema{db_.leafy_factory.jobs_machines.Envelope:STRUCT}, timestamp=1735922480721, headers=ConnectHeaders(headers=)}] (com.mongodb.kafka.connect.sink.MongoSinkTask:247)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,227] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.jobs_machines', partition=0, offset=0, timestamp=1735922480721, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,228] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.jobs_machines', partition=0, offset=1, timestamp=1735922480721, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,459] ERROR [mongodb-sink|task-0] Failed to put into the sink the following records: [SinkRecord{kafkaOffset=0, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=0} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":1}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=1,planned_start_date=1733961600000,planned_end_date=1733961600000,actual_start_date=1735568659000,quantity=10,wo_status=In Progress,creation_date=1735568647000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=first_in_data_collection,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480099,ts_us=1735922480099496,ts_ns=1735922480099496000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735922481686, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=1, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=1} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":2}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=2,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735821435000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480099,ts_us=1735922480099838,ts_ns=1735922480099838000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735922481687, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=2, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=2} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":3}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=3,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735821471000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480100,ts_us=1735922480100017,ts_ns=1735922480100017000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735922481687, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=3, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=3} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":4}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=4,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735821640000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480100,ts_us=1735922480100192,ts_ns=1735922480100192000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735922481687, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=4, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=4} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":5}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=5,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735822841000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480100,ts_us=1735922480100367,ts_ns=1735922480100367000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735922481687, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=5, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=5} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":6}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=6,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735825365000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480100,ts_us=1735922480100541,ts_ns=1735922480100541000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735922481687, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=6, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=6} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":7}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=7,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735825423000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480100,ts_us=1735922480100731,ts_ns=1735922480100731000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735922481688, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=7, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=7} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":8}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=8,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735825590000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480100,ts_us=1735922480100899,ts_ns=1735922480100899000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735922481688, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=8, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=8} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":9}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=9,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735827154000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480101,ts_us=1735922480101083,ts_ns=1735922480101083000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735922481688, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=9, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=9} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":10}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=10,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735827191000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480101,ts_us=1735922480101324,ts_ns=1735922480101324000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735922481688, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=10, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=10} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":11}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=11,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735827213000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480101,ts_us=1735922480101521,ts_ns=1735922480101521000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735922481689, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=11, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=11} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":12}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=12,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735827239000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480101,ts_us=1735922480101844,ts_ns=1735922480101844000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735922481689, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=12, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=12} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":13}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=13,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735827365000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480102,ts_us=1735922480102017,ts_ns=1735922480102017000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735922481689, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=13, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=13} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":14}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=14,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735827367000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480102,ts_us=1735922480102282,ts_ns=1735922480102282000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735922481689, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=14, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=14} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":15}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=15,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735839623000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480102,ts_us=1735922480102455,ts_ns=1735922480102455000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735922481689, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=15, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=15} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":16}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=16,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735839629000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480102,ts_us=1735922480102690,ts_ns=1735922480102690000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735922481689, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=16, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=16} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":17}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=17,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735840703000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480102,ts_us=1735922480102862,ts_ns=1735922480102862000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735922481689, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=17, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=17} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":18}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=18,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735841700000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480103,ts_us=1735922480103027,ts_ns=1735922480103027000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735922481690, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=18, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=18} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":19}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=19,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735842324000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480103,ts_us=1735922480103198,ts_ns=1735922480103198000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735922481690, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=19, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=19} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":20}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=20,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735842960000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480103,ts_us=1735922480103359,ts_ns=1735922480103359000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735922481690, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=20, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=20} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":21}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=21,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735899898000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=last,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480103,ts_us=1735922480103532,ts_ns=1735922480103532000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735922481690, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=21, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=21} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":22}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=22,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735901020000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922620000,snapshot=false,db=leafy_factory,ts_us=1735922620000000,ts_ns=1735922620000000000,table=work_orders,server_id=1,file=mariadb-bin.000001,pos=-72,row=0},op=c,ts_ms=1735922620092,ts_us=1735922620092755,ts_ns=1735922620092755000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735922620293, headers=ConnectHeaders(headers=)}] (com.mongodb.kafka.connect.sink.MongoSinkTask:247)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 3}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 3}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,462] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=0, timestamp=1735922481686, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 3}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 3}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,463] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=1, timestamp=1735922481687, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 3}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 3}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,464] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=2, timestamp=1735922481687, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 3}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 3}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,464] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=3, timestamp=1735922481687, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 3}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 3}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,465] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=4, timestamp=1735922481687, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 3}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 3}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,465] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=5, timestamp=1735922481687, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 3}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 3}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,466] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=6, timestamp=1735922481688, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 3}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 3}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,466] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=7, timestamp=1735922481688, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 3}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 3}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,467] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=8, timestamp=1735922481688, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 3}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 3}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,468] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=9, timestamp=1735922481688, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 3}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 3}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,468] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=10, timestamp=1735922481689, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 3}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 3}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,468] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=11, timestamp=1735922481689, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 3}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 3}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,469] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=12, timestamp=1735922481689, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 3}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 3}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,469] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=13, timestamp=1735922481689, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 3}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 3}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,469] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=14, timestamp=1735922481689, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 3}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 3}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,470] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=15, timestamp=1735922481689, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 3}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 3}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,470] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=16, timestamp=1735922481689, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 3}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 3}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,471] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=17, timestamp=1735922481690, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 3}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 3}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,471] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=18, timestamp=1735922481690, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 3}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 3}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,471] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=19, timestamp=1735922481690, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 3}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 3}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,472] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=20, timestamp=1735922481690, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 3}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 3}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,472] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=21, timestamp=1735922620293, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 3}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 3}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,799] ERROR [mongodb-sink|task-0] Failed to put into the sink the following records: [SinkRecord{kafkaOffset=0, timestampType=CreateTime, originalTopic=db_.leafy_factory.raw_materials, originalKafkaPartition=0, originalKafkaOffset=0} ConnectRecord{topic='db_.leafy_factory.raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_raw_material"}],"optional":false,"name":"db_.leafy_factory.raw_materials.Key"},"payload":{"id_raw_material":1}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_raw_material=1,item_code=hinges_ss,raw_material_name=Stainless steel hinge,raw_material_description=Stainless steel hinge,unit_measurement=pieces,raw_material_stock=9580,raw_material_status=high,raw_material_currency=USD,cost_per_part=1.50},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=first_in_data_collection,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=raw_materials,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480096,ts_us=1735922480096874,ts_ns=1735922480096874000}, valueSchema=Schema{db_.leafy_factory.raw_materials.Envelope:STRUCT}, timestamp=1735922481557, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=1, timestampType=CreateTime, originalTopic=db_.leafy_factory.raw_materials, originalKafkaPartition=0, originalKafkaOffset=1} ConnectRecord{topic='db_.leafy_factory.raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_raw_material"}],"optional":false,"name":"db_.leafy_factory.raw_materials.Key"},"payload":{"id_raw_material":2}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_raw_material=2,item_code=screw_ss,raw_material_name=Stainless steel screw,raw_material_description=Stainless steel screw,unit_measurement=pieces,raw_material_stock=93280,raw_material_status=high,raw_material_currency=USD,cost_per_part=0.05},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=raw_materials,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480096,ts_us=1735922480096965,ts_ns=1735922480096965000}, valueSchema=Schema{db_.leafy_factory.raw_materials.Envelope:STRUCT}, timestamp=1735922481557, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=2, timestampType=CreateTime, originalTopic=db_.leafy_factory.raw_materials, originalKafkaPartition=0, originalKafkaOffset=2} ConnectRecord{topic='db_.leafy_factory.raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_raw_material"}],"optional":false,"name":"db_.leafy_factory.raw_materials.Key"},"payload":{"id_raw_material":3}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_raw_material=3,item_code=aluminum_6061,raw_material_name=Aluminum,raw_material_description=lightweight aluminum,unit_measurement=kg,raw_material_stock=9601,raw_material_status=high,raw_material_currency=USD,cost_per_part=3.00},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=raw_materials,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480097,ts_us=1735922480097029,ts_ns=1735922480097029000}, valueSchema=Schema{db_.leafy_factory.raw_materials.Envelope:STRUCT}, timestamp=1735922481558, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=3, timestampType=CreateTime, originalTopic=db_.leafy_factory.raw_materials, originalKafkaPartition=0, originalKafkaOffset=3} ConnectRecord{topic='db_.leafy_factory.raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_raw_material"}],"optional":false,"name":"db_.leafy_factory.raw_materials.Key"},"payload":{"id_raw_material":4}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_raw_material=4,item_code=brackets_gs,raw_material_name=Galvanized bracket,raw_material_description=Galvanized brackets anti-corrosion,unit_measurement=pieces,raw_material_stock=8320,raw_material_status=high,raw_material_currency=USD,cost_per_part=2.50},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=raw_materials,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480097,ts_us=1735922480097104,ts_ns=1735922480097104000}, valueSchema=Schema{db_.leafy_factory.raw_materials.Envelope:STRUCT}, timestamp=1735922481558, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=4, timestampType=CreateTime, originalTopic=db_.leafy_factory.raw_materials, originalKafkaPartition=0, originalKafkaOffset=4} ConnectRecord{topic='db_.leafy_factory.raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_raw_material"}],"optional":false,"name":"db_.leafy_factory.raw_materials.Key"},"payload":{"id_raw_material":5}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_raw_material=5,item_code=titanium_lw,raw_material_name=Lightweight titanium,raw_material_description=Lightweight titanium known for its strength and lightweight properties,unit_measurement=kg,raw_material_stock=10000,raw_material_status=high,raw_material_currency=USD,cost_per_part=30.00},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=raw_materials,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480097,ts_us=1735922480097164,ts_ns=1735922480097164000}, valueSchema=Schema{db_.leafy_factory.raw_materials.Envelope:STRUCT}, timestamp=1735922481559, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=5, timestampType=CreateTime, originalTopic=db_.leafy_factory.raw_materials, originalKafkaPartition=0, originalKafkaOffset=5} ConnectRecord{topic='db_.leafy_factory.raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_raw_material"}],"optional":false,"name":"db_.leafy_factory.raw_materials.Key"},"payload":{"id_raw_material":6}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_raw_material=6,item_code=wood_hc,raw_material_name=Hickory wood,raw_material_description=Durable, lightweight, and shock-resistant, ideal for tool handles.,unit_measurement=kg,raw_material_stock=10000,raw_material_status=high,raw_material_currency=USD,cost_per_part=4.00},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=raw_materials,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480097,ts_us=1735922480097232,ts_ns=1735922480097232000}, valueSchema=Schema{db_.leafy_factory.raw_materials.Envelope:STRUCT}, timestamp=1735922481559, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=6, timestampType=CreateTime, originalTopic=db_.leafy_factory.raw_materials, originalKafkaPartition=0, originalKafkaOffset=6} ConnectRecord{topic='db_.leafy_factory.raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_raw_material"}],"optional":false,"name":"db_.leafy_factory.raw_materials.Key"},"payload":{"id_raw_material":7}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_raw_material=7,item_code=magnet_nm,raw_material_name=Neodymium magnet,raw_material_description=Strong magnet embedded in the hammer head to hold nails.,unit_measurement=kg,raw_material_stock=10000,raw_material_status=high,raw_material_currency=USD,cost_per_part=50.00},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=raw_materials,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480097,ts_us=1735922480097292,ts_ns=1735922480097292000}, valueSchema=Schema{db_.leafy_factory.raw_materials.Envelope:STRUCT}, timestamp=1735922481559, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=7, timestampType=CreateTime, originalTopic=db_.leafy_factory.raw_materials, originalKafkaPartition=0, originalKafkaOffset=7} ConnectRecord{topic='db_.leafy_factory.raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_raw_material"}],"optional":false,"name":"db_.leafy_factory.raw_materials.Key"},"payload":{"id_raw_material":8}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_raw_material=8,item_code=fasteners_ham,raw_material_name=Steel fasteners,raw_material_description=Steel fasteners for titanium hammers.,unit_measurement=kg,raw_material_stock=10000,raw_material_status=high,raw_material_currency=USD,cost_per_part=1.00},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=last_in_data_collection,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=raw_materials,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480097,ts_us=1735922480097390,ts_ns=1735922480097390000}, valueSchema=Schema{db_.leafy_factory.raw_materials.Envelope:STRUCT}, timestamp=1735922481559, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=8, timestampType=CreateTime, originalTopic=db_.leafy_factory.raw_materials, originalKafkaPartition=0, originalKafkaOffset=8} ConnectRecord{topic='db_.leafy_factory.raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_raw_material"}],"optional":false,"name":"db_.leafy_factory.raw_materials.Key"},"payload":{"id_raw_material":3}}, keySchema=Schema{STRING}, value=Struct{before=Struct{id_raw_material=3,item_code=aluminum_6061,raw_material_name=Aluminum,raw_material_description=lightweight aluminum,unit_measurement=kg,raw_material_stock=9601,raw_material_status=high,raw_material_currency=USD,cost_per_part=3.00},after=Struct{id_raw_material=3,item_code=aluminum_6061,raw_material_name=Aluminum,raw_material_description=lightweight aluminum,unit_measurement=kg,raw_material_stock=9582,raw_material_status=high,raw_material_currency=USD,cost_per_part=3.00},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922620000,snapshot=false,db=leafy_factory,ts_us=1735922620000000,ts_ns=1735922620000000000,table=raw_materials,server_id=1,file=mariadb-bin.000001,pos=-190,row=0},op=u,ts_ms=1735922620087,ts_us=1735922620087807,ts_ns=1735922620087807000}, valueSchema=Schema{db_.leafy_factory.raw_materials.Envelope:STRUCT}, timestamp=1735922620292, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=9, timestampType=CreateTime, originalTopic=db_.leafy_factory.raw_materials, originalKafkaPartition=0, originalKafkaOffset=9} ConnectRecord{topic='db_.leafy_factory.raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_raw_material"}],"optional":false,"name":"db_.leafy_factory.raw_materials.Key"},"payload":{"id_raw_material":1}}, keySchema=Schema{STRING}, value=Struct{before=Struct{id_raw_material=1,item_code=hinges_ss,raw_material_name=Stainless steel hinge,raw_material_description=Stainless steel hinge,unit_measurement=pieces,raw_material_stock=9580,raw_material_status=high,raw_material_currency=USD,cost_per_part=1.50},after=Struct{id_raw_material=1,item_code=hinges_ss,raw_material_name=Stainless steel hinge,raw_material_description=Stainless steel hinge,unit_measurement=pieces,raw_material_stock=9560,raw_material_status=high,raw_material_currency=USD,cost_per_part=1.50},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922620000,snapshot=false,db=leafy_factory,ts_us=1735922620000000,ts_ns=1735922620000000000,table=raw_materials,server_id=1,file=mariadb-bin.000001,pos=-218,row=0},op=u,ts_ms=1735922620089,ts_us=1735922620089767,ts_ns=1735922620089767000}, valueSchema=Schema{db_.leafy_factory.raw_materials.Envelope:STRUCT}, timestamp=1735922620292, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=10, timestampType=CreateTime, originalTopic=db_.leafy_factory.raw_materials, originalKafkaPartition=0, originalKafkaOffset=10} ConnectRecord{topic='db_.leafy_factory.raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_raw_material"}],"optional":false,"name":"db_.leafy_factory.raw_materials.Key"},"payload":{"id_raw_material":4}}, keySchema=Schema{STRING}, value=Struct{before=Struct{id_raw_material=4,item_code=brackets_gs,raw_material_name=Galvanized bracket,raw_material_description=Galvanized brackets anti-corrosion,unit_measurement=pieces,raw_material_stock=8320,raw_material_status=high,raw_material_currency=USD,cost_per_part=2.50},after=Struct{id_raw_material=4,item_code=brackets_gs,raw_material_name=Galvanized bracket,raw_material_description=Galvanized brackets anti-corrosion,unit_measurement=pieces,raw_material_stock=8240,raw_material_status=high,raw_material_currency=USD,cost_per_part=2.50},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922620000,snapshot=false,db=leafy_factory,ts_us=1735922620000000,ts_ns=1735922620000000000,table=raw_materials,server_id=1,file=mariadb-bin.000001,pos=-242,row=0},op=u,ts_ms=1735922620090,ts_us=1735922620090383,ts_ns=1735922620090383000}, valueSchema=Schema{db_.leafy_factory.raw_materials.Envelope:STRUCT}, timestamp=1735922620293, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=11, timestampType=CreateTime, originalTopic=db_.leafy_factory.raw_materials, originalKafkaPartition=0, originalKafkaOffset=11} ConnectRecord{topic='db_.leafy_factory.raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_raw_material"}],"optional":false,"name":"db_.leafy_factory.raw_materials.Key"},"payload":{"id_raw_material":2}}, keySchema=Schema{STRING}, value=Struct{before=Struct{id_raw_material=2,item_code=screw_ss,raw_material_name=Stainless steel screw,raw_material_description=Stainless steel screw,unit_measurement=pieces,raw_material_stock=93280,raw_material_status=high,raw_material_currency=USD,cost_per_part=0.05},after=Struct{id_raw_material=2,item_code=screw_ss,raw_material_name=Stainless steel screw,raw_material_description=Stainless steel screw,unit_measurement=pieces,raw_material_stock=92960,raw_material_status=high,raw_material_currency=USD,cost_per_part=0.05},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922620000,snapshot=false,db=leafy_factory,ts_us=1735922620000000,ts_ns=1735922620000000000,table=raw_materials,server_id=1,file=mariadb-bin.000001,pos=-216,row=0},op=u,ts_ms=1735922620090,ts_us=1735922620090837,ts_ns=1735922620090837000}, valueSchema=Schema{db_.leafy_factory.raw_materials.Envelope:STRUCT}, timestamp=1735922620293, headers=ConnectHeaders(headers=)}] (com.mongodb.kafka.connect.sink.MongoSinkTask:247)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 6}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 6}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,801] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.raw_materials', partition=0, offset=0, timestamp=1735922481557, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 6}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 6}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,802] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.raw_materials', partition=0, offset=1, timestamp=1735922481557, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 6}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 6}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,802] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.raw_materials', partition=0, offset=2, timestamp=1735922481558, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 6}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 6}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,802] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.raw_materials', partition=0, offset=3, timestamp=1735922481558, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 6}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 6}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,803] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.raw_materials', partition=0, offset=4, timestamp=1735922481559, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 6}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 6}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,803] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.raw_materials', partition=0, offset=5, timestamp=1735922481559, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 6}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 6}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,804] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.raw_materials', partition=0, offset=6, timestamp=1735922481559, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 6}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 6}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,804] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.raw_materials', partition=0, offset=7, timestamp=1735922481559, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 6}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 6}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,804] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.raw_materials', partition=0, offset=8, timestamp=1735922620292, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 6}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 6}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,805] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.raw_materials', partition=0, offset=9, timestamp=1735922620292, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 6}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 6}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,805] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.raw_materials', partition=0, offset=10, timestamp=1735922620293, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 6}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 6}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:47,806] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.raw_materials', partition=0, offset=11, timestamp=1735922620293, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 6}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 6}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:48,031] ERROR [mongodb-sink|task-0] Failed to put into the sink the following records: [SinkRecord{kafkaOffset=0, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=0} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":1}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=1,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=1},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=first_in_data_collection,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480089,ts_us=1735922480089209,ts_ns=1735922480089209000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735922480998, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=1, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=1} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":2}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=2,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480089,ts_us=1735922480089287,ts_ns=1735922480089287000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735922481000, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=2, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=2} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":3}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=3,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=3},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480089,ts_us=1735922480089355,ts_ns=1735922480089355000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735922481000, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=3, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=3} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":4}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=4,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=4},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480089,ts_us=1735922480089425,ts_ns=1735922480089425000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735922481001, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=4, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=4} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":5}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=5,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=5},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480089,ts_us=1735922480089496,ts_ns=1735922480089496000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735922481001, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=5, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=5} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":6}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=6,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=6},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480089,ts_us=1735922480089562,ts_ns=1735922480089562000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735922481002, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=6, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=6} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":7}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=7,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=7},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480089,ts_us=1735922480089638,ts_ns=1735922480089638000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735922481002, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=7, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=7} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":8}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=8,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=8},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480089,ts_us=1735922480089704,ts_ns=1735922480089704000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735922481002, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=8, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=8} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":9}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=9,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=9},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480089,ts_us=1735922480089771,ts_ns=1735922480089771000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735922481002, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=9, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=9} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":10}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=10,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=10},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480089,ts_us=1735922480089844,ts_ns=1735922480089844000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735922481003, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=10, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=10} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":11}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=11,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=11},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480089,ts_us=1735922480089921,ts_ns=1735922480089921000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735922481003, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=11, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=11} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":12}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=12,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=12},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480089,ts_us=1735922480089986,ts_ns=1735922480089986000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735922481003, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=12, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=12} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":13}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=13,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=13},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480090,ts_us=1735922480090051,ts_ns=1735922480090051000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735922481004, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=13, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=13} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":14}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=14,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=14},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480090,ts_us=1735922480090115,ts_ns=1735922480090115000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735922481004, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=14, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=14} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":15}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=15,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=15},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480090,ts_us=1735922480090180,ts_ns=1735922480090180000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735922481005, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=15, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=15} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":16}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=16,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=16},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480090,ts_us=1735922480090263,ts_ns=1735922480090263000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735922481006, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=16, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=16} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":17}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=17,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=17},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480090,ts_us=1735922480090334,ts_ns=1735922480090334000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735922481007, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=17, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=17} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":18}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=18,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=18},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480090,ts_us=1735922480090397,ts_ns=1735922480090397000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735922481007, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=18, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=18} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":19}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=19,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=19},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480090,ts_us=1735922480090465,ts_ns=1735922480090465000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735922481007, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=19, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=19} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":20}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=20,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=20},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=true,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480090,ts_us=1735922480090535,ts_ns=1735922480090535000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735922481007, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=20, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=20} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":21}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=21,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=21},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=last_in_data_collection,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480090,ts_us=1735922480090663,ts_ns=1735922480090663000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735922481008, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=21, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=21} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":22}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=22,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=22},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922620000,snapshot=false,db=leafy_factory,ts_us=1735922620000000,ts_ns=1735922620000000000,table=product_cost,server_id=1,file=mariadb-bin.000001,pos=-57,row=0},op=c,ts_ms=1735922620093,ts_us=1735922620093649,ts_ns=1735922620093649000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735922620294, headers=ConnectHeaders(headers=)}] (com.mongodb.kafka.connect.sink.MongoSinkTask:247)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 7}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 7}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:48,034] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=0, timestamp=1735922480998, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 7}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 7}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:48,034] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=1, timestamp=1735922481000, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 7}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 7}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:48,035] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=2, timestamp=1735922481000, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 7}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 7}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:48,035] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=3, timestamp=1735922481001, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 7}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 7}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:48,035] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=4, timestamp=1735922481001, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 7}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 7}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:48,036] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=5, timestamp=1735922481002, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 7}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 7}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:48,036] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=6, timestamp=1735922481002, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 7}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 7}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:48,036] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=7, timestamp=1735922481002, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 7}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 7}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:48,036] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=8, timestamp=1735922481002, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 7}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 7}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:48,037] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=9, timestamp=1735922481003, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 7}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 7}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:48,037] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=10, timestamp=1735922481003, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 7}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 7}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:48,037] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=11, timestamp=1735922481003, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 7}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 7}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:48,038] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=12, timestamp=1735922481004, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 7}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 7}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:48,038] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=13, timestamp=1735922481004, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 7}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 7}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:48,039] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=14, timestamp=1735922481005, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 7}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 7}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:48,039] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=15, timestamp=1735922481006, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 7}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 7}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:48,039] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=16, timestamp=1735922481007, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 7}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 7}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:48,040] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=17, timestamp=1735922481007, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 7}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 7}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:48,040] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=18, timestamp=1735922481007, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 7}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 7}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:48,040] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=19, timestamp=1735922481007, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 7}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 7}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:48,041] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=20, timestamp=1735922481008, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 7}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 7}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:48,041] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=21, timestamp=1735922620294, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922747, "i": 7}}, "signature": {"hash": {"$binary": {"base64": "TBhYsQulk2EOzrZCTirXxOXV5rw=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922747, "i": 7}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:48,186] ERROR [mongodb-sink|task-0] Failed to put into the sink the following records: [SinkRecord{kafkaOffset=0, timestampType=CreateTime, originalTopic=db_.leafy_factory.products, originalKafkaPartition=0, originalKafkaOffset=0} ConnectRecord{topic='db_.leafy_factory.products', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_product"}],"optional":false,"name":"db_.leafy_factory.products.Key"},"payload":{"id_product":1}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_product=1,product_name=2 Step ladder,product_description=Two Step Ladder - Folding Small 2 Step Stool 330lbs with Non-Slip Feets, Aluminum Lightweight Metal Step Stool by CHEAGO, Portable Solid Handy Work Ladder for Home, Kitchen, RV, Garage},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=first_in_data_collection,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=products,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480093,ts_us=1735922480093455,ts_ns=1735922480093455000}, valueSchema=Schema{db_.leafy_factory.products.Envelope:STRUCT}, timestamp=1735922481279, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=1, timestampType=CreateTime, originalTopic=db_.leafy_factory.products, originalKafkaPartition=0, originalKafkaOffset=1} ConnectRecord{topic='db_.leafy_factory.products', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_product"}],"optional":false,"name":"db_.leafy_factory.products.Key"},"payload":{"id_product":2}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_product=2,product_name=Titanium Hammer,product_description=Titanium Hammer With Curved Hickory Handle},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=last_in_data_collection,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=products,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480093,ts_us=1735922480093517,ts_ns=1735922480093517000}, valueSchema=Schema{db_.leafy_factory.products.Envelope:STRUCT}, timestamp=1735922481280, headers=ConnectHeaders(headers=)}] (com.mongodb.kafka.connect.sink.MongoSinkTask:247)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922748, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "OS5AB+A42kcX37+dydty+myYkb0=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922748, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:48,188] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.products', partition=0, offset=0, timestamp=1735922481279, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922748, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "OS5AB+A42kcX37+dydty+myYkb0=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922748, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:48,188] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.products', partition=0, offset=1, timestamp=1735922481280, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922748, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "OS5AB+A42kcX37+dydty+myYkb0=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922748, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:48,341] ERROR [mongodb-sink|task-0] Failed to put into the sink the following records: [SinkRecord{kafkaOffset=0, timestampType=CreateTime, originalTopic=db_.leafy_factory.jobs, originalKafkaPartition=0, originalKafkaOffset=0} ConnectRecord{topic='db_.leafy_factory.jobs', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_job"}],"optional":false,"name":"db_.leafy_factory.jobs.Key"},"payload":{"id_job":1}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_job=1,target_output=10,nOk_products=2,quality_rate=98,job_status=Completed,creation_date=1735568659000,work_id=1},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735922480000,snapshot=last_in_data_collection,db=leafy_factory,ts_us=1735922480000000,ts_ns=1735922480000000000,table=jobs,server_id=0,file=mariadb-bin.000001,pos=52374,row=0},op=r,ts_ms=1735922480084,ts_us=1735922480084689,ts_ns=1735922480084689000}, valueSchema=Schema{db_.leafy_factory.jobs.Envelope:STRUCT}, timestamp=1735922480597, headers=ConnectHeaders(headers=)}] (com.mongodb.kafka.connect.sink.MongoSinkTask:247)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922748, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "OS5AB+A42kcX37+dydty+myYkb0=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922748, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:45:48,342] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.jobs', partition=0, offset=0, timestamp=1735922480597, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735922748, "i": 1}}, "signature": {"hash": {"$binary": {"base64": "OS5AB+A42kcX37+dydty+myYkb0=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735922748, "i": 1}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:52:22,706] INFO Successfully processed removal of connector 'mongodb-sink' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2025-01-03 10:52:22,707] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Connector mongodb-sink config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2432)
[2025-01-03 10:52:22,707] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2025-01-03 10:52:22,707] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 10:52:22,709] INFO [0:0:0:0:0:0:0:1] - - [03/Jan/2025:16:52:22 +0000] "DELETE /connectors/mongodb-sink HTTP/1.1" 204 0 "-" "curl/8.7.1" 25 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-01-03 10:52:22,711] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=6, memberId='connect-192.168.1.5:8083-50ebba96-5ef2-4278-939c-c372fff74be9', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2025-01-03 10:52:22,716] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=6, memberId='connect-192.168.1.5:8083-50ebba96-5ef2-4278-939c-c372fff74be9', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2025-01-03 10:52:22,717] INFO [mongodb-sink|worker] Stopping connector mongodb-sink (org.apache.kafka.connect.runtime.Worker:452)
[2025-01-03 10:52:22,717] INFO [mongodb-sink|task-0] Stopping task mongodb-sink-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2025-01-03 10:52:22,717] INFO [mongodb-sink|worker] Scheduled shutdown for WorkerConnector{id=mongodb-sink} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2025-01-03 10:52:22,718] INFO [mongodb-sink|task-0] Stopping MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:124)
[2025-01-03 10:52:22,719] INFO [mongodb-sink|worker] Completed shutdown for WorkerConnector{id=mongodb-sink} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2025-01-03 10:52:22,829] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Revoke previously assigned partitions db_.leafy_factory.factories-0, db_.leafy_factory.jobs-0, db_.leafy_factory.jobs_machines-0, db_.leafy_factory.machines-0, db_.leafy_factory.product_cost-0, db_.leafy_factory.production_lines-0, db_.leafy_factory.products-0, db_.leafy_factory.products_raw_materials-0, db_.leafy_factory.raw_materials-0, db_.leafy_factory.work_orders-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:80)
[2025-01-03 10:52:22,829] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Member connector-consumer-mongodb-sink-0-db949108-291d-4c08-87d7-ecdf87a8dc34 sending LeaveGroup request to coordinator 192.168.1.5:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1174)
[2025-01-03 10:52:22,830] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 10:52:22,830] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 10:52:22,989] INFO [mongodb-sink|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:52:22,989] INFO [mongodb-sink|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:52:22,990] INFO [mongodb-sink|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:52:22,990] INFO [mongodb-sink|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:52:22,994] INFO [mongodb-sink|task-0] App info kafka.consumer for connector-consumer-mongodb-sink-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:52:22,999] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2025-01-03 10:52:23,007] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2025-01-03 10:52:23,008] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Joined group at generation 6 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.1.5:8083-50ebba96-5ef2-4278-939c-c372fff74be9', leaderUrl='http://192.168.1.5:8083/', offset=9, connectorIds=[mariadb-connector], taskIds=[mariadb-connector-0], revokedConnectorIds=[mongodb-sink], revokedTaskIds=[mongodb-sink-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2025-01-03 10:52:23,009] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 9 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2025-01-03 10:52:23,009] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2025-01-03 10:52:23,009] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2025-01-03 10:52:23,009] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 10:52:23,010] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=7, memberId='connect-192.168.1.5:8083-50ebba96-5ef2-4278-939c-c372fff74be9', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2025-01-03 10:52:23,014] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=7, memberId='connect-192.168.1.5:8083-50ebba96-5ef2-4278-939c-c372fff74be9', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2025-01-03 10:52:23,014] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Joined group at generation 7 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.1.5:8083-50ebba96-5ef2-4278-939c-c372fff74be9', leaderUrl='http://192.168.1.5:8083/', offset=9, connectorIds=[mariadb-connector], taskIds=[mariadb-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2025-01-03 10:52:23,014] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 9 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2025-01-03 10:52:23,014] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2025-01-03 10:52:40,010] INFO [0:0:0:0:0:0:0:1] - - [03/Jan/2025:16:52:40 +0000] "GET /connectors/mongodb-sink/status HTTP/1.1" 404 73 "-" "curl/8.7.1" 2 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-01-03 10:53:01,958] INFO [0:0:0:0:0:0:0:1] - - [03/Jan/2025:16:53:01 +0000] "GET /connectors/mongodb-sink/status HTTP/1.1" 404 73 "-" "curl/8.7.1" 6 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-01-03 10:53:06,127] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:87)
[2025-01-03 10:53:06,127] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:358)
[2025-01-03 10:53:06,135] INFO Stopped http_8083@521a506c{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:383)
[2025-01-03 10:53:06,135] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:149)
[2025-01-03 10:53:06,141] INFO Stopped o.e.j.s.ServletContextHandler@19924f15{/,null,STOPPED} (org.eclipse.jetty.server.handler.ContextHandler:1159)
[2025-01-03 10:53:06,141] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:387)
[2025-01-03 10:53:06,141] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder stopping (org.apache.kafka.connect.runtime.distributed.DistributedHerder:851)
[2025-01-03 10:53:06,141] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Stopping connectors and tasks that are still assigned to this worker. (org.apache.kafka.connect.runtime.distributed.DistributedHerder:808)
[2025-01-03 10:53:06,142] INFO [mariadb-connector|worker] Stopping connector mariadb-connector (org.apache.kafka.connect.runtime.Worker:452)
[2025-01-03 10:53:06,142] INFO [mariadb-connector|worker] Scheduled shutdown for WorkerConnector{id=mariadb-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2025-01-03 10:53:06,142] INFO [mariadb-connector|worker] Completed shutdown for WorkerConnector{id=mariadb-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2025-01-03 10:53:06,143] INFO [mariadb-connector|task-0] Stopping task mariadb-connector-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2025-01-03 10:53:06,515] INFO [mariadb-connector|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:434)
[2025-01-03 10:53:06,592] INFO [mariadb-connector|task-0] Finished streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:325)
[2025-01-03 10:53:06,592] INFO [mariadb-connector|task-0] Stopped reading binlog after 37 events, last recorded offset: {ts_sec=1735922620, file=mariadb-bin.000001, pos=55627, server_id=1, event=1} (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:1218)
[2025-01-03 10:53:06,593] INFO [mariadb-connector|task-0] SignalProcessor stopped (io.debezium.pipeline.signal.SignalProcessor:127)
[2025-01-03 10:53:06,594] INFO [mariadb-connector|task-0] Debezium ServiceRegistry stopped. (io.debezium.service.DefaultServiceRegistry:105)
[2025-01-03 10:53:06,610] INFO [mariadb-connector|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:983)
[2025-01-03 10:53:06,612] INFO [mariadb-connector|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:983)
[2025-01-03 10:53:06,613] INFO [mariadb-connector|task-0] [Producer clientId=db_-schemahistory] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 10:53:06,617] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:53:06,617] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:53:06,617] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:53:06,617] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:53:06,619] INFO [mariadb-connector|task-0] App info kafka.producer for db_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:53:06,619] INFO [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 10:53:06,622] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:53:06,622] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:53:06,623] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:53:06,623] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:53:06,623] INFO [mariadb-connector|task-0] App info kafka.producer for connector-producer-mariadb-connector-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:53:06,625] INFO Stopping KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:317)
[2025-01-03 10:53:06,626] INFO [Producer clientId=connect-cluster-statuses] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 10:53:06,628] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:53:06,628] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:53:06,629] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:53:06,629] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:53:06,629] INFO App info kafka.producer for connect-cluster-statuses unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:53:06,629] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 10:53:06,629] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 10:53:06,631] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Node 0 sent an invalid full fetch response with extraIds=(QKqIhd-mSoyT-nZUsw7-LA), response=() (org.apache.kafka.clients.FetchSessionHandler:556)
[2025-01-03 10:53:06,631] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:53:06,631] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:53:06,631] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:53:06,631] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:53:06,633] INFO App info kafka.consumer for connect-cluster-statuses unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:53:06,633] INFO Stopped KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:341)
[2025-01-03 10:53:06,633] INFO Closing KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:407)
[2025-01-03 10:53:06,633] INFO Stopping KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:317)
[2025-01-03 10:53:06,633] INFO [Producer clientId=connect-cluster-configs] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 10:53:06,635] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:53:06,635] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:53:06,635] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:53:06,635] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:53:06,635] INFO App info kafka.producer for connect-cluster-configs unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:53:06,635] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 10:53:06,635] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 10:53:06,946] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:53:06,946] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:53:06,946] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:53:06,946] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:53:06,950] INFO App info kafka.consumer for connect-cluster-configs unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:53:06,950] INFO Stopped KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:341)
[2025-01-03 10:53:06,950] INFO Closed KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:412)
[2025-01-03 10:53:06,950] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:250)
[2025-01-03 10:53:06,952] INFO Stopping KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:261)
[2025-01-03 10:53:06,952] INFO Stopping KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:317)
[2025-01-03 10:53:06,952] INFO [Producer clientId=connect-cluster-offsets] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 10:53:06,955] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:53:06,955] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:53:06,955] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:53:06,955] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:53:06,955] INFO App info kafka.producer for connect-cluster-offsets unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:53:06,955] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 10:53:06,956] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 10:53:07,279] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:53:07,280] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:53:07,280] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:53:07,280] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:53:07,283] INFO App info kafka.consumer for connect-cluster-offsets unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:53:07,284] INFO Stopped KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:341)
[2025-01-03 10:53:07,284] INFO Stopped KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:263)
[2025-01-03 10:53:07,284] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:53:07,284] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:53:07,284] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:53:07,284] INFO App info kafka.connect for 192.168.1.5:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:53:07,284] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:271)
[2025-01-03 10:53:07,286] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Member connect-192.168.1.5:8083-50ebba96-5ef2-4278-939c-c372fff74be9 sending LeaveGroup request to coordinator 192.168.1.5:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1174)
[2025-01-03 10:53:07,286] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1056)
[2025-01-03 10:53:07,286] WARN [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Close timed out with 1 pending requests to coordinator, terminating client connections (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1141)
[2025-01-03 10:53:07,286] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:53:07,287] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:53:07,287] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:53:07,289] INFO App info kafka.connect for connect-192.168.1.5:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:53:07,290] INFO App info kafka.admin.client for connect-cluster-shared-admin unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:53:07,292] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:53:07,292] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:53:07,292] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:53:07,292] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder stopped (org.apache.kafka.connect.runtime.distributed.DistributedHerder:394)
[2025-01-03 10:53:07,293] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder stopped (org.apache.kafka.connect.runtime.distributed.DistributedHerder:858)
[2025-01-03 10:53:07,293] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:92)
[2025-01-03 10:55:49,918] INFO Kafka Connect worker initializing ... (org.apache.kafka.connect.cli.AbstractConnectCli:114)
[2025-01-03 10:55:49,920] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote=true, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 23, 23+37-2369
	jvm.classpath = /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/activation-1.1.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/aopalliance-repackaged-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/argparse4j-0.7.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/audience-annotations-0.12.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/caffeine-2.9.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-beanutils-1.9.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-cli-1.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-collections-3.2.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-digester-2.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-io-2.14.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-lang3-3.12.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-logging-1.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-validator-1.7.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-basic-auth-extension-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-json-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-mirror-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-mirror-client-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-runtime-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-transforms-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/error_prone_annotations-2.10.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-api-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-locator-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-utils-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-annotations-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-core-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-databind-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-dataformat-csv-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-datatype-jdk8-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-jaxrs-base-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-jaxrs-json-provider-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-afterburner-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-jaxb-annotations-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-scala_2.13-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.activation-api-1.2.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.annotation-api-1.3.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.inject-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.validation-api-2.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.xml.bind-api-2.3.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javassist-3.29.2-GA.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.activation-api-1.2.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.annotation-api-1.3.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.servlet-api-3.1.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.ws.rs-api-2.1.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jaxb-api-2.3.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-client-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-common-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-container-servlet-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-container-servlet-core-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-hk2-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-server-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-client-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-continuation-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-http-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-io-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-security-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-server-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-servlet-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-servlets-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-util-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-util-ajax-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jline-3.25.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jopt-simple-5.0.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jose4j-0.9.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jsr305-3.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-clients-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-group-coordinator-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-group-coordinator-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-metadata-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-raft-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-server-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-server-common-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-shell-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-storage-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-storage-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-examples-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-scala_2.13-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-test-utils-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-tools-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-tools-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-transaction-coordinator-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka_2.13-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/lz4-java-1.8.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/maven-artifact-3.9.6.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/metrics-core-2.2.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/metrics-core-4.1.12.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-buffer-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-codec-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-common-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-handler-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-resolver-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-classes-epoll-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-native-epoll-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-native-unix-common-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/opentelemetry-proto-1.0.0-alpha.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/osgi-resource-locator-1.0.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/paranamer-2.8.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/pcollections-4.0.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/plexus-utils-3.5.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/protobuf-java-3.25.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/reflections-0.10.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/reload4j-1.2.25.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/rocksdbjni-7.9.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-collection-compat_2.13-2.10.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-java8-compat_2.13-1.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-library-2.13.14.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-logging_2.13-3.9.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-reflect-2.13.14.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/slf4j-api-1.7.36.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/slf4j-reload4j-1.7.36.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/snappy-java-1.1.10.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/swagger-annotations-2.2.8.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/trogdor-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zookeeper-3.8.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zookeeper-jute-3.8.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zstd-jni-1.5.6-4.jar
	os.spec = Mac OS X, aarch64, 15.0
	os.vcpus = 11
 (org.apache.kafka.connect.runtime.WorkerInfo:72)
[2025-01-03 10:55:49,920] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.AbstractConnectCli:120)
[2025-01-03 10:55:49,931] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:55:49,960] INFO Using up-to-date JsonConverter implementation (io.debezium.converters.CloudEventsConverter:120)
[2025-01-03 10:55:50,036] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:55:50,036] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1 (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:55:50,040] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:55:50,040] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:55:50,052] INFO Using up-to-date JsonConverter implementation (io.debezium.converters.CloudEventsConverter:120)
[2025-01-03 10:55:50,065] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:55:50,068] INFO Loading plugin from: classpath (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:55:50,070] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@5a07e868 (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:55:50,070] INFO Scanning plugins with ServiceLoaderScanner took 140 ms (org.apache.kafka.connect.runtime.isolation.PluginScanner:71)
[2025-01-03 10:55:50,071] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:55:50,208] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:55:50,208] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1 (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:55:50,268] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:55:50,268] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:55:50,426] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:55:50,426] INFO Loading plugin from: classpath (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 10:55:50,834] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@5a07e868 (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 10:55:50,834] INFO Scanning plugins with ReflectionScanner took 763 ms (org.apache.kafka.connect.runtime.isolation.PluginScanner:71)
[2025-01-03 10:55:50,835] WARN One or more plugins are missing ServiceLoader manifests may not be usable with plugin.discovery=service_load: [
file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1/	com.mongodb.kafka.connect.MongoSinkConnector	sink	1.14.1
file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1/	com.mongodb.kafka.connect.MongoSourceConnector	source	1.14.1
file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/	io.debezium.connector.mongodb.MongoDbSinkConnector	sink	3.0.5.Final
]
Read the documentation at https://kafka.apache.org/documentation.html#connect_plugindiscovery for instructions on migrating your plugins to take advantage of the performance improvements of service_load mode. To silence this warning, set plugin.discovery=only_scan in the worker config. (org.apache.kafka.connect.runtime.isolation.Plugins:123)
[2025-01-03 10:55:50,836] INFO Added plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,836] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,836] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,836] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,836] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,836] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,836] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,836] INFO Added plugin 'io.debezium.transforms.HeaderToValue' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,836] INFO Added plugin 'io.debezium.transforms.SchemaChangeEventFilter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,836] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,836] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,836] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,836] INFO Added plugin 'io.debezium.transforms.ExtractSchemaToNewRecord' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,836] INFO Added plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,836] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,836] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,836] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,836] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,836] INFO Added plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,836] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,837] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,837] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,837] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,837] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,837] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,837] INFO Added plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,837] INFO Added plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,837] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,837] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,837] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,837] INFO Added plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,837] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,837] INFO Added plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,837] INFO Added plugin 'io.debezium.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,837] INFO Added plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,837] INFO Added plugin 'io.debezium.transforms.partitions.PartitionRouting' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,837] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,837] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,837] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,837] INFO Added plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,837] INFO Added plugin 'io.debezium.transforms.TimezoneConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,837] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,837] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,837] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,837] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,837] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,838] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,838] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,838] INFO Added plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,838] INFO Added plugin 'io.debezium.transforms.ExtractChangedRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,838] INFO Added plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,838] INFO Added plugin 'io.debezium.connector.mongodb.transforms.outbox.MongoEventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,838] INFO Added plugin 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,838] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,838] INFO Added plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,838] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,838] INFO Added plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,838] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,838] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,838] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,838] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,838] INFO Added plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,838] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,838] INFO Added plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,838] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,838] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,838] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,838] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,838] INFO Added plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,838] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,838] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,838] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 10:55:50,839] INFO Added alias 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,839] INFO Added alias 'ExtractNewDocumentState' to plugin 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,839] INFO Added alias 'JsonConverter' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,839] INFO Added alias 'DebeziumMySql' to plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,839] INFO Added alias 'MongoSourceConnector' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,839] INFO Added alias 'CloudEventsConverter' to plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,839] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,839] INFO Added alias 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,839] INFO Added alias 'DebeziumMySqlConnectRestExtension' to plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,839] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,839] INFO Added alias 'MySql' to plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,840] INFO Added alias 'MongoDbSinkConnector' to plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,840] INFO Added alias 'EnvVar' to plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,840] INFO Added alias 'EnvVarConfigProvider' to plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,840] INFO Added alias 'MongoDb' to plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,840] INFO Added alias 'MirrorCheckpointConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,840] INFO Added alias 'Boolean' to plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,840] INFO Added alias 'MySqlConnector' to plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,840] INFO Added alias 'NoneConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,840] INFO Added alias 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,840] INFO Added alias 'HeaderToValue' to plugin 'io.debezium.transforms.HeaderToValue' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,840] INFO Added alias 'PartitionRouting' to plugin 'io.debezium.transforms.partitions.PartitionRouting' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,840] INFO Added alias 'MongoDbSink' to plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,840] INFO Added alias 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,840] INFO Added alias 'StringConverter' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,840] INFO Added alias 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,840] INFO Added alias 'ByLogicalTableRouter' to plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,840] INFO Added alias 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,840] INFO Added alias 'MongoDbConnector' to plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,840] INFO Added alias 'FileConfigProvider' to plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,840] INFO Added alias 'IntegerConverter' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,840] INFO Added alias 'EventRouter' to plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,840] INFO Added alias 'SchemaChangeEventFilter' to plugin 'io.debezium.transforms.SchemaChangeEventFilter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,840] INFO Added alias 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,840] INFO Added alias 'LongConverter' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,840] INFO Added alias 'DropHeaders' to plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,840] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,840] INFO Added alias 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,840] INFO Added alias 'ExtractSchemaToNewRecord' to plugin 'io.debezium.transforms.ExtractSchemaToNewRecord' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,840] INFO Added alias 'BinaryData' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,840] INFO Added alias 'File' to plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,840] INFO Added alias 'FloatConverter' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'DirectoryConfigProvider' to plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'ReadToInsertEvent' to plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'ShortConverter' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'CloudEvents' to plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'ActivateTracingSpan' to plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'TimezoneConverter' to plugin 'io.debezium.transforms.TimezoneConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'DoubleConverter' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'Simple' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'BinaryDataConverter' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'AllConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'ExtractNewRecordState' to plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'DebeziumMongoDb' to plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'MirrorHeartbeatConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'InsertHeader' to plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'Directory' to plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'MirrorSourceConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'MongoSinkConnector' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'PrincipalConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'DebeziumMongoDbConnectRestExtension' to plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'BooleanConverter' to plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'MongoEventRouter' to plugin 'io.debezium.connector.mongodb.transforms.outbox.MongoEventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'ExtractChangedRecordState' to plugin 'io.debezium.transforms.ExtractChangedRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,841] INFO Added alias 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 10:55:50,856] INFO DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	config.providers = []
	config.storage.replication.factor = 1
	config.storage.topic = connect-configs
	connect.protocol = sessioned
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = All
	exactly.once.source.support = disabled
	group.id = connect-cluster
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	heartbeat.interval.ms = 3000
	inter.worker.key.generation.algorithm = HmacSHA256
	inter.worker.key.size = null
	inter.worker.key.ttl.ms = 3600000
	inter.worker.signature.algorithm = HmacSHA256
	inter.worker.verification.algorithms = [HmacSHA256]
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = [http://:8083]
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 1
	offset.storage.topic = connect-offsets
	plugin.discovery = hybrid_warn
	plugin.path = [/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins]
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	status.storage.partitions = 5
	status.storage.replication.factor = 1
	status.storage.topic = connect-status
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000
 (org.apache.kafka.connect.runtime.distributed.DistributedConfig:371)
[2025-01-03 10:55:50,857] INFO Creating Kafka admin client (org.apache.kafka.connect.runtime.WorkerConfig:281)
[2025-01-03 10:55:50,859] INFO AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:371)
[2025-01-03 10:55:50,879] INFO These configurations '[config.storage.topic, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2025-01-03 10:55:50,879] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:55:50,879] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:55:50,879] INFO Kafka startTimeMs: 1735923350879 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:55:50,987] INFO Kafka cluster ID: tztLDPmmThCv7RTvhR2k6w (org.apache.kafka.connect.runtime.WorkerConfig:298)
[2025-01-03 10:55:50,988] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:55:50,990] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:55:50,990] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:55:50,990] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:55:50,992] INFO PublicConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	listeners = [http://:8083]
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
 (org.apache.kafka.connect.runtime.rest.RestServerConfig$PublicConfig:371)
[2025-01-03 10:55:50,995] INFO Logging initialized @1312ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:170)
[2025-01-03 10:55:51,007] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:125)
[2025-01-03 10:55:51,008] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:196)
[2025-01-03 10:55:51,016] INFO jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 23+37-2369 (org.eclipse.jetty.server.Server:375)
[2025-01-03 10:55:51,023] INFO Started http_8083@64fba3e6{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:333)
[2025-01-03 10:55:51,023] INFO Started @1340ms (org.eclipse.jetty.server.Server:415)
[2025-01-03 10:55:51,030] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 10:55:51,031] INFO REST server listening at http://192.168.1.5:8083/, advertising URL http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:216)
[2025-01-03 10:55:51,031] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 10:55:51,031] INFO REST admin endpoints at http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2025-01-03 10:55:51,031] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 10:55:51,031] INFO Setting up All Policy for ConnectorClientConfigOverride. This will allow all client configurations to be overridden (org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy:45)
[2025-01-03 10:55:51,032] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:55:51,040] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:55:51,040] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:55:51,040] INFO Kafka startTimeMs: 1735923351040 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:55:51,042] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:55:51,042] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:55:51,049] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 10:55:51,060] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:55:51,060] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:55:51,060] INFO Kafka startTimeMs: 1735923351060 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:55:51,061] INFO Kafka Connect worker initialization took 1143ms (org.apache.kafka.connect.cli.AbstractConnectCli:141)
[2025-01-03 10:55:51,061] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:67)
[2025-01-03 10:55:51,062] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2025-01-03 10:55:51,062] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder starting (org.apache.kafka.connect.runtime.distributed.DistributedHerder:375)
[2025-01-03 10:55:51,062] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:233)
[2025-01-03 10:55:51,062] INFO Starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:232)
[2025-01-03 10:55:51,062] INFO Starting KafkaBasedLog with topic connect-offsets reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog:254)
[2025-01-03 10:55:51,063] INFO AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-shared-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:371)
[2025-01-03 10:55:51,064] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2025-01-03 10:55:51,064] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:55:51,064] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:55:51,064] INFO Kafka startTimeMs: 1735923351064 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:55:51,073] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:238)
[2025-01-03 10:55:51,085] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:334)
[2025-01-03 10:55:51,087] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:339)
[2025-01-03 10:55:51,087] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:132)
[2025-01-03 10:55:51,228] INFO Started o.e.j.s.ServletContextHandler@62871522{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:921)
[2025-01-03 10:55:51,228] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:303)
[2025-01-03 10:55:51,228] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:77)
[2025-01-03 10:55:51,331] INFO Created topic (name=connect-offsets, numPartitions=25, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:9092 (org.apache.kafka.connect.util.TopicAdmin:416)
[2025-01-03 10:55:51,334] INFO ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-offsets
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 10:55:51,341] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:55:51,347] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 10:55:51,347] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:55:51,347] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:55:51,347] INFO Kafka startTimeMs: 1735923351347 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:55:51,350] INFO [Producer clientId=connect-cluster-offsets] Cluster ID: tztLDPmmThCv7RTvhR2k6w (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:55:51,350] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-offsets
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 10:55:51,353] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:55:51,362] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2025-01-03 10:55:51,362] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:55:51,362] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:55:51,362] INFO Kafka startTimeMs: 1735923351362 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:55:51,365] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Cluster ID: tztLDPmmThCv7RTvhR2k6w (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:55:51,367] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Assigned to partition(s): connect-offsets-0, connect-offsets-5, connect-offsets-10, connect-offsets-20, connect-offsets-15, connect-offsets-9, connect-offsets-11, connect-offsets-4, connect-offsets-16, connect-offsets-17, connect-offsets-3, connect-offsets-24, connect-offsets-23, connect-offsets-13, connect-offsets-18, connect-offsets-22, connect-offsets-8, connect-offsets-2, connect-offsets-12, connect-offsets-19, connect-offsets-14, connect-offsets-1, connect-offsets-6, connect-offsets-7, connect-offsets-21 (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:579)
[2025-01-03 10:55:51,367] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:55:51,368] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-5 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:55:51,368] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-10 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:55:51,368] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-20 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:55:51,368] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-15 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:55:51,368] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-9 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:55:51,368] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-11 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:55:51,368] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:55:51,368] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-16 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:55:51,368] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-17 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:55:51,368] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:55:51,368] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-24 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:55:51,368] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-23 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:55:51,368] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-13 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:55:51,368] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-18 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:55:51,368] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-22 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:55:51,368] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-8 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:55:51,368] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:55:51,368] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-12 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:55:51,368] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-19 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:55:51,368] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-14 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:55:51,368] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:55:51,368] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-6 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:55:51,368] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-7 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:55:51,369] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-21 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:55:51,384] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:55:51,384] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:55:51,384] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:55:51,384] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:55:51,384] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:55:51,384] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:55:51,384] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:55:51,384] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:55:51,384] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:55:51,384] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:55:51,384] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:55:51,384] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:55:51,384] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:55:51,384] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:55:51,384] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:55:51,385] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:55:51,385] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:55:51,385] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:55:51,385] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:55:51,385] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:55:51,385] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:55:51,385] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:55:51,385] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:55:51,385] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:55:51,385] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:55:51,385] INFO Finished reading KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:311)
[2025-01-03 10:55:51,385] INFO Started KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:313)
[2025-01-03 10:55:51,385] INFO Finished reading offsets topic and starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:249)
[2025-01-03 10:55:51,386] INFO Worker started (org.apache.kafka.connect.runtime.Worker:243)
[2025-01-03 10:55:51,386] INFO Starting KafkaBasedLog with topic connect-status reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog:254)
[2025-01-03 10:55:51,455] INFO Created topic (name=connect-status, numPartitions=5, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:9092 (org.apache.kafka.connect.util.TopicAdmin:416)
[2025-01-03 10:55:51,456] INFO ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-statuses
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 10:55:51,456] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:55:51,457] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 10:55:51,457] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:55:51,457] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:55:51,457] INFO Kafka startTimeMs: 1735923351457 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:55:51,458] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-statuses
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 10:55:51,458] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:55:51,459] INFO [Producer clientId=connect-cluster-statuses] Cluster ID: tztLDPmmThCv7RTvhR2k6w (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:55:51,459] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2025-01-03 10:55:51,459] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:55:51,459] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:55:51,459] INFO Kafka startTimeMs: 1735923351459 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:55:51,462] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Cluster ID: tztLDPmmThCv7RTvhR2k6w (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:55:51,463] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Assigned to partition(s): connect-status-0, connect-status-4, connect-status-1, connect-status-2, connect-status-3 (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:579)
[2025-01-03 10:55:51,463] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:55:51,463] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:55:51,463] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:55:51,463] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:55:51,463] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:55:51,467] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:55:51,467] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:55:51,468] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:55:51,468] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:55:51,468] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:55:51,468] INFO Finished reading KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:311)
[2025-01-03 10:55:51,468] INFO Started KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:313)
[2025-01-03 10:55:51,469] INFO Starting KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:378)
[2025-01-03 10:55:51,469] INFO Starting KafkaBasedLog with topic connect-configs reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog:254)
[2025-01-03 10:55:51,497] INFO Created topic (name=connect-configs, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:9092 (org.apache.kafka.connect.util.TopicAdmin:416)
[2025-01-03 10:55:51,498] INFO ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-configs
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 10:55:51,498] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:55:51,499] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 10:55:51,499] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:55:51,499] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:55:51,499] INFO Kafka startTimeMs: 1735923351499 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:55:51,499] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-configs
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 10:55:51,500] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:55:51,501] INFO [Producer clientId=connect-cluster-configs] Cluster ID: tztLDPmmThCv7RTvhR2k6w (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:55:51,501] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2025-01-03 10:55:51,501] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:55:51,501] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:55:51,501] INFO Kafka startTimeMs: 1735923351501 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:55:51,502] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Cluster ID: tztLDPmmThCv7RTvhR2k6w (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:55:51,502] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Assigned to partition(s): connect-configs-0 (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:579)
[2025-01-03 10:55:51,502] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Seeking to earliest offset of partition connect-configs-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 10:55:51,507] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Resetting offset for partition connect-configs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 10:55:51,507] INFO Finished reading KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:311)
[2025-01-03 10:55:51,507] INFO Started KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:313)
[2025-01-03 10:55:51,507] INFO Started KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:402)
[2025-01-03 10:55:51,510] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Cluster ID: tztLDPmmThCv7RTvhR2k6w (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:55:51,510] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Discovered group coordinator 192.168.1.5:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:937)
[2025-01-03 10:55:51,511] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2025-01-03 10:55:51,511] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 10:55:51,514] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 10:55:51,515] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=1, memberId='connect-192.168.1.5:8083-39bc8cd0-3317-4e6c-9c00-57df0a53f941', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2025-01-03 10:55:51,520] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=1, memberId='connect-192.168.1.5:8083-39bc8cd0-3317-4e6c-9c00-57df0a53f941', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2025-01-03 10:55:51,520] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Joined group at generation 1 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.1.5:8083-39bc8cd0-3317-4e6c-9c00-57df0a53f941', leaderUrl='http://192.168.1.5:8083/', offset=-1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2025-01-03 10:55:51,520] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder started (org.apache.kafka.connect.runtime.distributed.DistributedHerder:387)
[2025-01-03 10:55:51,520] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connectors and tasks using config offset -1 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2025-01-03 10:55:51,520] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2025-01-03 10:55:51,551] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Session key updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2510)
[2025-01-03 10:56:14,536] INFO Loading the custom source info struct maker plugin: io.debezium.connector.mysql.MySqlSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2025-01-03 10:56:14,958] INFO Using 'SHOW MASTER STATUS' to get binary log status (io.debezium.connector.mysql.jdbc.MySqlConnection:41)
[2025-01-03 10:56:14,961] INFO Successfully tested connection for jdbc:mysql://localhost:3306/?useInformationSchema=true&nullCatalogMeansCurrent=false&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8&zeroDateTimeBehavior=CONVERT_TO_NULL&connectTimeout=30000 with user 'root' (io.debezium.connector.binlog.BinlogConnector:66)
[2025-01-03 10:56:14,962] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection:983)
[2025-01-03 10:56:14,963] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:371)
[2025-01-03 10:56:14,968] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Connector mariadb-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2025-01-03 10:56:14,969] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2025-01-03 10:56:14,969] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 10:56:14,971] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=2, memberId='connect-192.168.1.5:8083-39bc8cd0-3317-4e6c-9c00-57df0a53f941', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2025-01-03 10:56:14,973] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=2, memberId='connect-192.168.1.5:8083-39bc8cd0-3317-4e6c-9c00-57df0a53f941', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2025-01-03 10:56:14,973] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Joined group at generation 2 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.1.5:8083-39bc8cd0-3317-4e6c-9c00-57df0a53f941', leaderUrl='http://192.168.1.5:8083/', offset=2, connectorIds=[mariadb-connector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2025-01-03 10:56:14,973] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 2 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2025-01-03 10:56:14,973] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connector mariadb-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2025-01-03 10:56:14,974] INFO [mariadb-connector|worker] Creating connector mariadb-connector of type io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:313)
[2025-01-03 10:56:14,975] INFO [mariadb-connector|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2025-01-03 10:56:14,975] INFO [mariadb-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 10:56:14,976] INFO [mariadb-connector|worker] Instantiated connector mariadb-connector with version 3.0.5.Final of type class io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:335)
[2025-01-03 10:56:14,977] INFO [mariadb-connector|worker] Finished creating connector mariadb-connector (org.apache.kafka.connect.runtime.Worker:356)
[2025-01-03 10:56:14,977] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2025-01-03 10:56:14,979] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2025-01-03 10:56:14,979] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 10:56:14,982] INFO [0:0:0:0:0:0:0:1] - - [03/Jan/2025:16:56:14 +0000] "POST /connectors HTTP/1.1" 201 681 "-" "curl/8.7.1" 516 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-01-03 10:56:14,987] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Tasks [mariadb-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2025-01-03 10:56:14,987] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2025-01-03 10:56:14,987] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 10:56:14,988] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=3, memberId='connect-192.168.1.5:8083-39bc8cd0-3317-4e6c-9c00-57df0a53f941', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2025-01-03 10:56:14,990] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=3, memberId='connect-192.168.1.5:8083-39bc8cd0-3317-4e6c-9c00-57df0a53f941', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2025-01-03 10:56:14,990] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Joined group at generation 3 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.1.5:8083-39bc8cd0-3317-4e6c-9c00-57df0a53f941', leaderUrl='http://192.168.1.5:8083/', offset=4, connectorIds=[mariadb-connector], taskIds=[mariadb-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2025-01-03 10:56:14,990] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 4 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2025-01-03 10:56:14,991] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting task mariadb-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2025-01-03 10:56:14,992] INFO [mariadb-connector|task-0] Creating task mariadb-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2025-01-03 10:56:14,992] INFO [mariadb-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mariadb-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2025-01-03 10:56:14,992] INFO [mariadb-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mariadb-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 10:56:14,993] INFO [mariadb-connector|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.mysql.MySqlConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2025-01-03 10:56:14,994] INFO [mariadb-connector|task-0] Instantiated task mariadb-connector-0 with version 3.0.5.Final of type io.debezium.connector.mysql.MySqlConnectorTask (org.apache.kafka.connect.runtime.Worker:665)
[2025-01-03 10:56:14,994] INFO [mariadb-connector|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:56:14,994] INFO [mariadb-connector|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mariadb-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:678)
[2025-01-03 10:56:14,995] INFO [mariadb-connector|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 10:56:14,995] INFO [mariadb-connector|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mariadb-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:684)
[2025-01-03 10:56:14,995] INFO [mariadb-connector|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mariadb-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2025-01-03 10:56:14,996] INFO [mariadb-connector|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:1795)
[2025-01-03 10:56:14,996] INFO [mariadb-connector|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2025-01-03 10:56:14,996] INFO [mariadb-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 10:56:14,997] INFO [mariadb-connector|task-0] ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-mariadb-connector-0
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 10:56:14,997] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:56:14,998] INFO [mariadb-connector|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 10:56:14,998] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:56:14,998] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:56:14,998] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735923374998 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:56:15,000] INFO [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] Cluster ID: tztLDPmmThCv7RTvhR2k6w (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:56:15,001] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2025-01-03 10:56:15,002] INFO [mariadb-connector|task-0] Starting MySqlConnectorTask with configuration:
   connector.class = io.debezium.connector.mysql.MySqlConnector
   database.user = root
   database.server.id = 184054
   database.history.kafka.bootstrap.servers = localhost:9092
   database.history.kafka.topic = db.history.leafy_factory
   database.server.name = leafy_factory
   schema.history.internal.kafka.bootstrap.servers = localhost:9092
   database.port = 3306
   include.schema.changes = false
   topic.prefix = db_
   schema.history.internal.kafka.topic = db.history.internal
   task.class = io.debezium.connector.mysql.MySqlConnectorTask
   database.hostname = localhost
   database.password = ********
   name = mariadb-connector
   database.include.list = leafy_factory
 (io.debezium.connector.common.BaseSourceTask:250)
[2025-01-03 10:56:15,002] INFO [mariadb-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mysql.MySqlSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2025-01-03 10:56:15,002] INFO [mariadb-connector|task-0] Loading the custom topic naming strategy plugin: io.debezium.schema.DefaultTopicNamingStrategy (io.debezium.config.CommonConnectorConfig:1401)
[2025-01-03 10:56:15,057] INFO [mariadb-connector|task-0] Using 'SHOW MASTER STATUS' to get binary log status (io.debezium.connector.mysql.jdbc.MySqlConnection:41)
[2025-01-03 10:56:15,061] INFO [mariadb-connector|task-0] No previous offsets found (io.debezium.connector.common.BaseSourceTask:536)
[2025-01-03 10:56:15,078] INFO [mariadb-connector|task-0] KafkaSchemaHistory Consumer config: {key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, enable.auto.commit=false, group.id=db_-schemahistory, bootstrap.servers=localhost:9092, fetch.min.bytes=1, session.timeout.ms=10000, auto.offset.reset=earliest, client.id=db_-schemahistory} (io.debezium.storage.kafka.history.KafkaSchemaHistory:245)
[2025-01-03 10:56:15,078] INFO [mariadb-connector|task-0] KafkaSchemaHistory Producer config: {retries=1, value.serializer=org.apache.kafka.common.serialization.StringSerializer, acks=1, batch.size=32768, max.block.ms=10000, bootstrap.servers=localhost:9092, buffer.memory=1048576, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=db_-schemahistory, linger.ms=0} (io.debezium.storage.kafka.history.KafkaSchemaHistory:246)
[2025-01-03 10:56:15,078] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = db-history-config-check (io.debezium.util.Threads:270)
[2025-01-03 10:56:15,079] INFO [mariadb-connector|task-0] Idempotence will be disabled because acks is set to 1, not set to 'all'. (org.apache.kafka.clients.producer.ProducerConfig:587)
[2025-01-03 10:56:15,079] INFO [mariadb-connector|task-0] ProducerConfig values: 
	acks = 1
	auto.include.jmx.reporter = true
	batch.size = 32768
	bootstrap.servers = [localhost:9092]
	buffer.memory = 1048576
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 10:56:15,079] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:56:15,081] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:56:15,081] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:56:15,081] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735923375081 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:56:15,083] INFO [mariadb-connector|task-0] [Producer clientId=db_-schemahistory] Cluster ID: tztLDPmmThCv7RTvhR2k6w (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:56:15,138] INFO [mariadb-connector|task-0] Using 'SHOW MASTER STATUS' to get binary log status (io.debezium.connector.mysql.jdbc.MySqlConnection:41)
[2025-01-03 10:56:15,144] INFO [mariadb-connector|task-0] Closing connection before starting schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:123)
[2025-01-03 10:56:15,144] INFO [mariadb-connector|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:983)
[2025-01-03 10:56:15,145] INFO [mariadb-connector|task-0] Connector started for the first time. (io.debezium.connector.common.BaseSourceTask:89)
[2025-01-03 10:56:15,145] INFO [mariadb-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = db_-schemahistory
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 10:56:15,145] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 10:56:15,146] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:56:15,146] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:56:15,146] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735923375146 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:56:15,148] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Cluster ID: tztLDPmmThCv7RTvhR2k6w (org.apache.kafka.clients.Metadata:365)
[2025-01-03 10:56:15,149] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 10:56:15,149] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 10:56:15,150] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:56:15,150] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:56:15,150] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:56:15,150] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:56:15,151] INFO [mariadb-connector|task-0] App info kafka.consumer for db_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:56:15,151] INFO [mariadb-connector|task-0] AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:371)
[2025-01-03 10:56:15,152] INFO [mariadb-connector|task-0] These configurations '[value.serializer, acks, batch.size, max.block.ms, buffer.memory, key.serializer, linger.ms]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2025-01-03 10:56:15,152] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 10:56:15,152] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 10:56:15,152] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735923375152 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 10:56:15,189] INFO [mariadb-connector|task-0] Database schema history topic '(name=db.history.internal, numPartitions=1, replicationFactor=default, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807, retention.bytes=-1})' created (io.debezium.storage.kafka.history.KafkaSchemaHistory:555)
[2025-01-03 10:56:15,190] INFO [mariadb-connector|task-0] App info kafka.admin.client for db_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 10:56:15,190] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 10:56:15,190] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 10:56:15,190] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 10:56:15,190] INFO [mariadb-connector|task-0] Reconnecting after finishing schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:136)
[2025-01-03 10:56:15,212] INFO [mariadb-connector|task-0] No previous offset found (io.debezium.connector.mysql.MySqlConnectorTask:147)
[2025-01-03 10:56:15,219] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = SignalProcessor (io.debezium.util.Threads:270)
[2025-01-03 10:56:15,224] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = change-event-source-coordinator (io.debezium.util.Threads:270)
[2025-01-03 10:56:15,224] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = blocking-snapshot (io.debezium.util.Threads:270)
[2025-01-03 10:56:15,225] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-change-event-source-coordinator (io.debezium.util.Threads:287)
[2025-01-03 10:56:15,225] INFO [mariadb-connector|task-0] WorkerSourceTask{id=mariadb-connector-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.AbstractWorkerSourceTask:280)
[2025-01-03 10:56:15,226] INFO [mariadb-connector|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:137)
[2025-01-03 10:56:15,226] INFO [mariadb-connector|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:140)
[2025-01-03 10:56:15,229] INFO [mariadb-connector|task-0] According to the connector configuration both schema and data will be snapshot. (io.debezium.relational.RelationalSnapshotChangeEventSource:282)
[2025-01-03 10:56:15,230] INFO [mariadb-connector|task-0] Snapshot step 1 - Preparing (io.debezium.relational.RelationalSnapshotChangeEventSource:135)
[2025-01-03 10:56:15,230] INFO [mariadb-connector|task-0] Snapshot step 2 - Determining captured tables (io.debezium.relational.RelationalSnapshotChangeEventSource:144)
[2025-01-03 10:56:15,230] INFO [mariadb-connector|task-0] Read list of available databases (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:116)
[2025-01-03 10:56:15,233] INFO [mariadb-connector|task-0] 	 list of available databases is: [information_schema, leafy_factory, mysql, performance_schema, sys, test] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:118)
[2025-01-03 10:56:15,233] INFO [mariadb-connector|task-0] Read list of available tables in each database (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:126)
[2025-01-03 10:56:15,274] INFO [mariadb-connector|task-0] 	snapshot continuing with database(s): [leafy_factory] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:147)
[2025-01-03 10:56:15,274] INFO [mariadb-connector|task-0] Adding table leafy_factory.jobs_machines to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:56:15,274] INFO [mariadb-connector|task-0] Adding table leafy_factory.product_cost to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:56:15,274] INFO [mariadb-connector|task-0] Adding table leafy_factory.production_lines to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:56:15,274] INFO [mariadb-connector|task-0] Adding table leafy_factory.products_raw_materials to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:56:15,274] INFO [mariadb-connector|task-0] Adding table leafy_factory.machines to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:56:15,274] INFO [mariadb-connector|task-0] Adding table leafy_factory.jobs to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:56:15,274] INFO [mariadb-connector|task-0] Adding table leafy_factory.raw_materials to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:56:15,274] INFO [mariadb-connector|task-0] Adding table leafy_factory.work_orders to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:56:15,274] INFO [mariadb-connector|task-0] Adding table leafy_factory.products to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:56:15,274] INFO [mariadb-connector|task-0] Adding table leafy_factory.factories to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 10:56:15,275] INFO [mariadb-connector|task-0] Created connection pool with 1 threads (io.debezium.relational.RelationalSnapshotChangeEventSource:236)
[2025-01-03 10:56:15,275] INFO [mariadb-connector|task-0] Snapshot step 3 - Locking captured tables [leafy_factory.factories, leafy_factory.jobs, leafy_factory.jobs_machines, leafy_factory.machines, leafy_factory.product_cost, leafy_factory.production_lines, leafy_factory.products, leafy_factory.products_raw_materials, leafy_factory.raw_materials, leafy_factory.work_orders] (io.debezium.relational.RelationalSnapshotChangeEventSource:153)
[2025-01-03 10:56:15,277] INFO [mariadb-connector|task-0] Flush and obtain global read lock to prevent writes to database (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:488)
[2025-01-03 10:56:15,278] INFO [mariadb-connector|task-0] Snapshot step 4 - Determining snapshot offset (io.debezium.relational.RelationalSnapshotChangeEventSource:159)
[2025-01-03 10:56:15,279] INFO [mariadb-connector|task-0] Read binlog position of MySQL primary server (io.debezium.connector.mysql.MySqlSnapshotChangeEventSource:58)
[2025-01-03 10:56:15,279] INFO [mariadb-connector|task-0] Snapshot step 5 - Reading structure of captured tables (io.debezium.relational.RelationalSnapshotChangeEventSource:162)
[2025-01-03 10:56:15,279] INFO [mariadb-connector|task-0] All eligible tables schema should be captured, capturing: [leafy_factory.factories, leafy_factory.jobs, leafy_factory.jobs_machines, leafy_factory.machines, leafy_factory.product_cost, leafy_factory.production_lines, leafy_factory.products, leafy_factory.products_raw_materials, leafy_factory.raw_materials, leafy_factory.work_orders] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:314)
[2025-01-03 10:56:15,809] INFO [mariadb-connector|task-0] Reading structure of database 'leafy_factory' (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:348)
[2025-01-03 10:56:15,872] INFO [mariadb-connector|task-0] Snapshot step 6 - Persisting schema history (io.debezium.relational.RelationalSnapshotChangeEventSource:166)
[2025-01-03 10:56:15,910] INFO [mariadb-connector|task-0] Releasing global read lock to enable MySQL writes (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:497)
[2025-01-03 10:56:15,911] INFO [mariadb-connector|task-0] Writes to MySQL tables prevented for a total of 00:00:00.634 (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:501)
[2025-01-03 10:56:15,911] INFO [mariadb-connector|task-0] Snapshot step 7 - Snapshotting data (io.debezium.relational.RelationalSnapshotChangeEventSource:178)
[2025-01-03 10:56:15,911] INFO [mariadb-connector|task-0] Creating snapshot worker pool with 1 worker thread(s) (io.debezium.relational.RelationalSnapshotChangeEventSource:480)
[2025-01-03 10:56:15,912] INFO [mariadb-connector|task-0] For table 'leafy_factory.factories' using select statement: 'SELECT `id_factory`, `factory_name`, `factory_location`, `factory_timestamp` FROM `leafy_factory`.`factories`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:56:15,913] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.factories is OptionalLong[0] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:56:15,913] INFO [mariadb-connector|task-0] For table 'leafy_factory.jobs' using select statement: 'SELECT `id_job`, `target_output`, `nOk_products`, `quality_rate`, `job_status`, `creation_date`, `work_id` FROM `leafy_factory`.`jobs`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:56:15,914] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.jobs is OptionalLong[0] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:56:15,914] INFO [mariadb-connector|task-0] For table 'leafy_factory.jobs_machines' using select statement: 'SELECT `id_jobs_machines`, `job_id`, `machine_id` FROM `leafy_factory`.`jobs_machines`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:56:15,916] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.jobs_machines is OptionalLong[2] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:56:15,916] INFO [mariadb-connector|task-0] For table 'leafy_factory.machines' using select statement: 'SELECT `id_machine`, `machine_status`, `last_maintenance`, `operator`, `avg_output`, `reject_count`, `production_line_id` FROM `leafy_factory`.`machines`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:56:15,918] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.machines is OptionalLong[4] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:56:15,918] INFO [mariadb-connector|task-0] For table 'leafy_factory.product_cost' using select statement: 'SELECT `id_cost`, `raw_material_cost_per_product`, `overhead_per_product`, `total_cost_per_product`, `cost_ok_with_overhead`, `cost_nok_with_overhead`, `actual_total_cost`, `work_id` FROM `leafy_factory`.`product_cost`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:56:15,919] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.product_cost is OptionalLong[12] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:56:15,919] INFO [mariadb-connector|task-0] For table 'leafy_factory.production_lines' using select statement: 'SELECT `id_production_line`, `factory_id` FROM `leafy_factory`.`production_lines`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:56:15,920] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.production_lines is OptionalLong[2] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:56:15,920] INFO [mariadb-connector|task-0] For table 'leafy_factory.products' using select statement: 'SELECT `id_product`, `product_name`, `product_description` FROM `leafy_factory`.`products`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:56:15,921] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.products is OptionalLong[2] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:56:15,921] INFO [mariadb-connector|task-0] For table 'leafy_factory.products_raw_materials' using select statement: 'SELECT `id_products_raw_materials`, `product_id`, `raw_material_id` FROM `leafy_factory`.`products_raw_materials`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:56:15,922] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.products_raw_materials is OptionalLong[9] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:56:15,922] INFO [mariadb-connector|task-0] For table 'leafy_factory.raw_materials' using select statement: 'SELECT `id_raw_material`, `item_code`, `raw_material_name`, `raw_material_description`, `unit_measurement`, `raw_material_stock`, `raw_material_status`, `raw_material_currency`, `cost_per_part` FROM `leafy_factory`.`raw_materials`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:56:15,923] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.raw_materials is OptionalLong[8] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:56:15,923] INFO [mariadb-connector|task-0] For table 'leafy_factory.work_orders' using select statement: 'SELECT `id_work`, `planned_start_date`, `planned_end_date`, `actual_start_date`, `actual_end_date`, `quantity`, `wo_status`, `creation_date`, `product_id`, `nOk_products` FROM `leafy_factory`.`work_orders`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 10:56:15,924] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.work_orders is OptionalLong[12] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 10:56:15,925] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.factories' (1 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:56:15,936] INFO [mariadb-connector|task-0] 	 Finished exporting 1 records for table 'leafy_factory.factories' (1 of 10 tables); total duration '00:00:00.011' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:56:15,937] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.jobs' (2 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:56:15,939] INFO [mariadb-connector|task-0] 	 Finished exporting 1 records for table 'leafy_factory.jobs' (2 of 10 tables); total duration '00:00:00.002' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:56:15,939] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.jobs_machines' (3 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:56:15,940] INFO [mariadb-connector|task-0] 	 Finished exporting 2 records for table 'leafy_factory.jobs_machines' (3 of 10 tables); total duration '00:00:00.001' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:56:15,940] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.machines' (4 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:56:15,942] INFO [mariadb-connector|task-0] 	 Finished exporting 4 records for table 'leafy_factory.machines' (4 of 10 tables); total duration '00:00:00.002' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:56:15,942] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.product_cost' (5 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:56:15,945] INFO [mariadb-connector|task-0] 	 Finished exporting 22 records for table 'leafy_factory.product_cost' (5 of 10 tables); total duration '00:00:00.003' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:56:15,945] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.production_lines' (6 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:56:15,947] INFO [mariadb-connector|task-0] 	 Finished exporting 2 records for table 'leafy_factory.production_lines' (6 of 10 tables); total duration '00:00:00.002' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:56:15,947] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.products' (7 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:56:15,949] INFO [mariadb-connector|task-0] 	 Finished exporting 2 records for table 'leafy_factory.products' (7 of 10 tables); total duration '00:00:00.002' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:56:15,949] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.products_raw_materials' (8 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:56:15,950] INFO [mariadb-connector|task-0] 	 Finished exporting 8 records for table 'leafy_factory.products_raw_materials' (8 of 10 tables); total duration '00:00:00.001' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:56:15,951] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.raw_materials' (9 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:56:15,952] INFO [mariadb-connector|task-0] 	 Finished exporting 8 records for table 'leafy_factory.raw_materials' (9 of 10 tables); total duration '00:00:00.001' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:56:15,953] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.work_orders' (10 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 10:56:15,958] INFO [mariadb-connector|task-0] 	 Finished exporting 22 records for table 'leafy_factory.work_orders' (10 of 10 tables); total duration '00:00:00.005' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 10:56:15,960] INFO [mariadb-connector|task-0] Snapshot - Final stage (io.debezium.pipeline.source.AbstractSnapshotChangeEventSource:108)
[2025-01-03 10:56:15,960] INFO [mariadb-connector|task-0] Snapshot completed (io.debezium.pipeline.source.AbstractSnapshotChangeEventSource:112)
[2025-01-03 10:56:15,966] INFO [mariadb-connector|task-0] Snapshot ended with SnapshotResult [status=COMPLETED, offset=BinlogOffsetContext{sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=BinlogSourceInfo{currentGtid='null', currentBinlogFilename='mariadb-bin.000001', currentBinlogPosition=55627, currentRowNumber=0, serverId=0, sourceTime=2025-01-03T16:56:15Z, threadId=-1, currentQuery='null', tableIds=[leafy_factory.work_orders], databaseName='leafy_factory'}, snapshotCompleted=true, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet='null', currentGtidSet='null', restartBinlogFilename='mariadb-bin.000001', restartBinlogPosition=55627, restartRowsToSkip=0, restartEventsToSkip=0, currentEventLengthInBytes=0, inTransaction=false, transactionId='null', incrementalSnapshotContext=IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]}] (io.debezium.pipeline.ChangeEventSourceCoordinator:298)
[2025-01-03 10:56:15,968] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = binlog-client (io.debezium.util.Threads:270)
[2025-01-03 10:56:15,969] INFO [mariadb-connector|task-0] Enable ssl PREFERRED mode for connector db_ (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:1289)
[2025-01-03 10:56:15,973] INFO [mariadb-connector|task-0] SignalProcessor started. Scheduling it every 5000ms (io.debezium.pipeline.signal.SignalProcessor:105)
[2025-01-03 10:56:15,973] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-SignalProcessor (io.debezium.util.Threads:287)
[2025-01-03 10:56:15,973] INFO [mariadb-connector|task-0] Starting streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:323)
[2025-01-03 10:56:15,974] INFO [mariadb-connector|task-0] Skip 0 events on streaming start (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:278)
[2025-01-03 10:56:15,974] INFO [mariadb-connector|task-0] Skip 0 rows on streaming start (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:282)
[2025-01-03 10:56:15,974] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-binlog-client (io.debezium.util.Threads:287)
[2025-01-03 10:56:15,975] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-binlog-client (io.debezium.util.Threads:287)
[2025-01-03 10:56:15,999] INFO [mariadb-connector|task-0] Connected to binlog at localhost:3306, starting at BinlogOffsetContext{sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=BinlogSourceInfo{currentGtid='null', currentBinlogFilename='mariadb-bin.000001', currentBinlogPosition=55627, currentRowNumber=0, serverId=0, sourceTime=2025-01-03T16:56:15Z, threadId=-1, currentQuery='null', tableIds=[leafy_factory.work_orders], databaseName='leafy_factory'}, snapshotCompleted=true, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet='null', currentGtidSet='null', restartBinlogFilename='mariadb-bin.000001', restartBinlogPosition=55627, restartRowsToSkip=0, restartEventsToSkip=0, currentEventLengthInBytes=0, inTransaction=false, transactionId='null', incrementalSnapshotContext=IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]} (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:1232)
[2025-01-03 10:56:15,999] INFO [mariadb-connector|task-0] Waiting for keepalive thread to start (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:299)
[2025-01-03 10:56:16,000] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-binlog-client (io.debezium.util.Threads:287)
[2025-01-03 10:56:16,105] INFO [mariadb-connector|task-0] Keepalive thread is running (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:306)
[2025-01-03 10:56:16,255] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 4 : {db_.leafy_factory.factories=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:56:16,393] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 7 : {db_.leafy_factory.jobs=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:56:16,536] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 10 : {db_.leafy_factory.jobs_machines=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:56:16,673] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 13 : {db_.leafy_factory.machines=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:56:16,809] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 17 : {db_.leafy_factory.product_cost=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:56:16,957] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 23 : {db_.leafy_factory.production_lines=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:56:17,080] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 31 : {db_.leafy_factory.products=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:56:17,218] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 35 : {db_.leafy_factory.products_raw_materials=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:56:17,353] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 39 : {db_.leafy_factory.raw_materials=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 10:56:25,006] INFO [mariadb-connector|task-0|offsets] WorkerSourceTask{id=mariadb-connector-0} Committing offsets for 72 acknowledged messages (org.apache.kafka.connect.runtime.WorkerSourceTask:236)
[2025-01-03 10:56:46,204] INFO [mariadb-connector|task-0] 78 records sent during previous 00:00:31.209, last recorded offset of {server=db_} partition is {ts_sec=1735923405, file=mariadb-bin.000001, pos=0, row=1, server_id=1} (io.debezium.connector.common.BaseSourceTask:351)
[2025-01-03 10:56:55,031] INFO [mariadb-connector|task-0|offsets] WorkerSourceTask{id=mariadb-connector-0} Committing offsets for 6 acknowledged messages (org.apache.kafka.connect.runtime.WorkerSourceTask:236)
[2025-01-03 10:57:22,426] ERROR Uncaught exception in REST call to /connectors (org.apache.kafka.connect.runtime.rest.errors.ConnectExceptionMapper:65)
com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException: Unrecognized field "connector.class" (class org.apache.kafka.connect.runtime.rest.entities.CreateConnectorRequest), not marked as ignorable (3 known properties: "config", "name", "initial_state"])
 at [Source: REDACTED (`StreamReadFeature.INCLUDE_SOURCE_IN_LOCATION` disabled); line: 1, column: 742] (through reference chain: org.apache.kafka.connect.runtime.rest.entities.CreateConnectorRequest["connector.class"])
	at com.fasterxml.jackson.databind.exc.UnrecognizedPropertyException.from(UnrecognizedPropertyException.java:61)
	at com.fasterxml.jackson.databind.DeserializationContext.handleUnknownProperty(DeserializationContext.java:1153)
	at com.fasterxml.jackson.databind.deser.std.StdDeserializer.handleUnknownProperty(StdDeserializer.java:2224)
	at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.handleUnknownProperty(BeanDeserializerBase.java:1793)
	at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.handleUnknownProperties(BeanDeserializerBase.java:1743)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeUsingPropertyBased(BeanDeserializer.java:546)
	at com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromObjectUsingNonDefault(BeanDeserializerBase.java:1493)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:348)
	at com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:185)
	at com.fasterxml.jackson.databind.deser.DefaultDeserializationContext.readRootValue(DefaultDeserializationContext.java:342)
	at com.fasterxml.jackson.databind.ObjectReader._bind(ObjectReader.java:2099)
	at com.fasterxml.jackson.databind.ObjectReader.readValue(ObjectReader.java:1249)
	at com.fasterxml.jackson.jaxrs.base.ProviderBase.readFrom(ProviderBase.java:801)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$TerminalReaderInterceptor.invokeReadFrom(ReaderInterceptorExecutor.java:233)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor$TerminalReaderInterceptor.aroundReadFrom(ReaderInterceptorExecutor.java:212)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor.proceed(ReaderInterceptorExecutor.java:132)
	at org.glassfish.jersey.server.internal.MappableExceptionWrapperInterceptor.aroundReadFrom(MappableExceptionWrapperInterceptor.java:49)
	at org.glassfish.jersey.message.internal.ReaderInterceptorExecutor.proceed(ReaderInterceptorExecutor.java:132)
	at org.glassfish.jersey.message.internal.MessageBodyFactory.readFrom(MessageBodyFactory.java:1072)
	at org.glassfish.jersey.message.internal.InboundMessageContext.readEntity(InboundMessageContext.java:919)
	at org.glassfish.jersey.server.ContainerRequest.readEntity(ContainerRequest.java:290)
	at org.glassfish.jersey.server.internal.inject.EntityParamValueParamProvider$EntityValueSupplier.apply(EntityParamValueParamProvider.java:73)
	at org.glassfish.jersey.server.internal.inject.EntityParamValueParamProvider$EntityValueSupplier.apply(EntityParamValueParamProvider.java:56)
	at org.glassfish.jersey.server.spi.internal.ParamValueFactoryWithSource.apply(ParamValueFactoryWithSource.java:50)
	at org.glassfish.jersey.server.spi.internal.ParameterValueHelper.getParameterValues(ParameterValueHelper.java:68)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$AbstractMethodParamInvoker.getParamValues(JavaResourceMethodDispatcherProvider.java:109)
	at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:176)
	at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:81)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:478)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:400)
	at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:81)
	at org.glassfish.jersey.server.ServerRuntime$1.run(ServerRuntime.java:256)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
	at org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:292)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:274)
	at org.glassfish.jersey.internal.Errors.process(Errors.java:244)
	at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:265)
	at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:235)
	at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:684)
	at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:394)
	at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:346)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:358)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:311)
	at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:205)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1624)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:233)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1440)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:188)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1594)
	at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:186)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1355)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:191)
	at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:181)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)
	at org.eclipse.jetty.server.Server.handle(Server.java:516)
	at org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)
	at org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)
	at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)
	at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)
	at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)
	at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)
	at org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 10:57:22,436] INFO [0:0:0:0:0:0:0:1] - - [03/Jan/2025:16:57:22 +0000] "POST /connectors HTTP/1.1" 500 457 "-" "curl/8.7.1" 42 (org.apache.kafka.connect.runtime.rest.RestServer:62)
