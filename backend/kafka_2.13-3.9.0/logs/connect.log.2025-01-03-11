[2025-01-03 11:01:31,134] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:87)
[2025-01-03 11:01:31,138] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:358)
[2025-01-03 11:01:31,144] INFO Stopped http_8083@64fba3e6{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:383)
[2025-01-03 11:01:31,145] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:149)
[2025-01-03 11:01:31,149] INFO Stopped o.e.j.s.ServletContextHandler@62871522{/,null,STOPPED} (org.eclipse.jetty.server.handler.ContextHandler:1159)
[2025-01-03 11:01:31,150] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:387)
[2025-01-03 11:01:31,150] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder stopping (org.apache.kafka.connect.runtime.distributed.DistributedHerder:851)
[2025-01-03 11:01:31,150] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Stopping connectors and tasks that are still assigned to this worker. (org.apache.kafka.connect.runtime.distributed.DistributedHerder:808)
[2025-01-03 11:01:31,150] INFO [mariadb-connector|worker] Stopping connector mariadb-connector (org.apache.kafka.connect.runtime.Worker:452)
[2025-01-03 11:01:31,150] INFO [mariadb-connector|worker] Scheduled shutdown for WorkerConnector{id=mariadb-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2025-01-03 11:01:31,151] INFO [mariadb-connector|worker] Completed shutdown for WorkerConnector{id=mariadb-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2025-01-03 11:01:31,153] INFO [mariadb-connector|task-0] Stopping task mariadb-connector-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2025-01-03 11:01:31,312] INFO [mariadb-connector|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:434)
[2025-01-03 11:01:31,386] INFO [mariadb-connector|task-0] Finished streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:325)
[2025-01-03 11:01:31,387] INFO [mariadb-connector|task-0] Stopped reading binlog after 29 events, last recorded offset: {ts_sec=1735923405, file=mariadb-bin.000001, pos=58880, server_id=1, event=1} (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:1218)
[2025-01-03 11:01:31,388] INFO [mariadb-connector|task-0] SignalProcessor stopped (io.debezium.pipeline.signal.SignalProcessor:127)
[2025-01-03 11:01:31,389] INFO [mariadb-connector|task-0] Debezium ServiceRegistry stopped. (io.debezium.service.DefaultServiceRegistry:105)
[2025-01-03 11:01:31,392] INFO [mariadb-connector|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:983)
[2025-01-03 11:01:31,394] INFO [mariadb-connector|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:983)
[2025-01-03 11:01:31,394] INFO [mariadb-connector|task-0] [Producer clientId=db_-schemahistory] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 11:01:31,399] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:01:31,400] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:31,400] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:31,400] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:01:31,401] INFO [mariadb-connector|task-0] App info kafka.producer for db_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:01:31,402] INFO [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 11:01:31,405] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:01:31,406] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:31,406] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:31,406] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:01:31,406] INFO [mariadb-connector|task-0] App info kafka.producer for connector-producer-mariadb-connector-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:01:31,407] INFO Stopping KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:317)
[2025-01-03 11:01:31,408] INFO [Producer clientId=connect-cluster-statuses] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 11:01:31,412] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:01:31,412] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:31,412] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:31,412] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:01:31,412] INFO App info kafka.producer for connect-cluster-statuses unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:01:31,412] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 11:01:31,412] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 11:01:31,413] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Node 0 sent an invalid full fetch response with extraIds=(bOSiVzVBRAqjWd9QVo1Ovg), response=() (org.apache.kafka.clients.FetchSessionHandler:556)
[2025-01-03 11:01:31,414] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:01:31,414] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:31,414] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:31,414] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:01:31,415] INFO App info kafka.consumer for connect-cluster-statuses unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:01:31,415] INFO Stopped KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:341)
[2025-01-03 11:01:31,415] INFO Closing KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:407)
[2025-01-03 11:01:31,416] INFO Stopping KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:317)
[2025-01-03 11:01:31,416] INFO [Producer clientId=connect-cluster-configs] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 11:01:31,417] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:01:31,417] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:31,417] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:31,417] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:01:31,418] INFO App info kafka.producer for connect-cluster-configs unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:01:31,418] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 11:01:31,418] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 11:01:31,816] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:01:31,817] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:31,817] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:31,817] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:01:31,819] INFO App info kafka.consumer for connect-cluster-configs unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:01:31,819] INFO Stopped KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:341)
[2025-01-03 11:01:31,820] INFO Closed KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:412)
[2025-01-03 11:01:31,820] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:250)
[2025-01-03 11:01:31,822] INFO Stopping KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:261)
[2025-01-03 11:01:31,822] INFO Stopping KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:317)
[2025-01-03 11:01:31,823] INFO [Producer clientId=connect-cluster-offsets] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 11:01:31,826] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:01:31,826] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:31,826] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:31,826] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:01:31,827] INFO App info kafka.producer for connect-cluster-offsets unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:01:31,827] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 11:01:31,827] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 11:01:32,255] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:01:32,255] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:32,256] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:32,256] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:01:32,258] INFO App info kafka.consumer for connect-cluster-offsets unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:01:32,258] INFO Stopped KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:341)
[2025-01-03 11:01:32,259] INFO Stopped KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:263)
[2025-01-03 11:01:32,259] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:01:32,259] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:32,259] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:01:32,259] INFO App info kafka.connect for 192.168.1.5:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:01:32,259] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:271)
[2025-01-03 11:01:32,262] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Member connect-192.168.1.5:8083-39bc8cd0-3317-4e6c-9c00-57df0a53f941 sending LeaveGroup request to coordinator 192.168.1.5:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1174)
[2025-01-03 11:01:32,263] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1056)
[2025-01-03 11:01:32,263] WARN [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Close timed out with 1 pending requests to coordinator, terminating client connections (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1141)
[2025-01-03 11:01:32,264] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:01:32,264] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:32,264] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:01:32,266] INFO App info kafka.connect for connect-192.168.1.5:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:01:32,270] INFO App info kafka.admin.client for connect-cluster-shared-admin unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:01:32,271] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:01:32,271] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:32,271] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:01:32,272] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder stopped (org.apache.kafka.connect.runtime.distributed.DistributedHerder:394)
[2025-01-03 11:01:32,272] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder stopped (org.apache.kafka.connect.runtime.distributed.DistributedHerder:858)
[2025-01-03 11:01:32,272] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:92)
[2025-01-03 11:01:54,313] INFO Kafka Connect worker initializing ... (org.apache.kafka.connect.cli.AbstractConnectCli:114)
[2025-01-03 11:01:54,315] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote=true, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 23, 23+37-2369
	jvm.classpath = /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/activation-1.1.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/aopalliance-repackaged-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/argparse4j-0.7.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/audience-annotations-0.12.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/caffeine-2.9.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-beanutils-1.9.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-cli-1.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-collections-3.2.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-digester-2.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-io-2.14.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-lang3-3.12.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-logging-1.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-validator-1.7.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-basic-auth-extension-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-json-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-mirror-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-mirror-client-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-runtime-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-transforms-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/error_prone_annotations-2.10.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-api-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-locator-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-utils-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-annotations-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-core-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-databind-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-dataformat-csv-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-datatype-jdk8-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-jaxrs-base-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-jaxrs-json-provider-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-afterburner-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-jaxb-annotations-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-scala_2.13-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.activation-api-1.2.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.annotation-api-1.3.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.inject-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.validation-api-2.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.xml.bind-api-2.3.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javassist-3.29.2-GA.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.activation-api-1.2.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.annotation-api-1.3.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.servlet-api-3.1.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.ws.rs-api-2.1.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jaxb-api-2.3.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-client-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-common-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-container-servlet-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-container-servlet-core-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-hk2-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-server-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-client-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-continuation-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-http-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-io-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-security-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-server-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-servlet-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-servlets-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-util-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-util-ajax-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jline-3.25.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jopt-simple-5.0.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jose4j-0.9.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jsr305-3.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-clients-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-group-coordinator-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-group-coordinator-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-metadata-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-raft-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-server-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-server-common-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-shell-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-storage-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-storage-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-examples-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-scala_2.13-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-test-utils-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-tools-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-tools-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-transaction-coordinator-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka_2.13-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/lz4-java-1.8.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/maven-artifact-3.9.6.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/metrics-core-2.2.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/metrics-core-4.1.12.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-buffer-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-codec-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-common-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-handler-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-resolver-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-classes-epoll-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-native-epoll-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-native-unix-common-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/opentelemetry-proto-1.0.0-alpha.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/osgi-resource-locator-1.0.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/paranamer-2.8.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/pcollections-4.0.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/plexus-utils-3.5.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/protobuf-java-3.25.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/reflections-0.10.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/reload4j-1.2.25.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/rocksdbjni-7.9.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-collection-compat_2.13-2.10.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-java8-compat_2.13-1.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-library-2.13.14.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-logging_2.13-3.9.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-reflect-2.13.14.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/slf4j-api-1.7.36.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/slf4j-reload4j-1.7.36.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/snappy-java-1.1.10.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/swagger-annotations-2.2.8.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/trogdor-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zookeeper-3.8.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zookeeper-jute-3.8.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zstd-jni-1.5.6-4.jar
	os.spec = Mac OS X, aarch64, 15.0
	os.vcpus = 11
 (org.apache.kafka.connect.runtime.WorkerInfo:72)
[2025-01-03 11:01:54,316] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.AbstractConnectCli:120)
[2025-01-03 11:01:54,326] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 11:01:54,355] INFO Using up-to-date JsonConverter implementation (io.debezium.converters.CloudEventsConverter:120)
[2025-01-03 11:01:54,432] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 11:01:54,432] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1 (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 11:01:54,436] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 11:01:54,436] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 11:01:54,447] INFO Using up-to-date JsonConverter implementation (io.debezium.converters.CloudEventsConverter:120)
[2025-01-03 11:01:54,461] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 11:01:54,463] INFO Loading plugin from: classpath (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 11:01:54,465] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@5a07e868 (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 11:01:54,465] INFO Scanning plugins with ServiceLoaderScanner took 139 ms (org.apache.kafka.connect.runtime.isolation.PluginScanner:71)
[2025-01-03 11:01:54,466] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 11:01:54,588] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 11:01:54,588] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1 (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 11:01:54,650] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 11:01:54,650] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 11:01:54,776] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 11:01:54,776] INFO Loading plugin from: classpath (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 11:01:55,204] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@5a07e868 (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 11:01:55,204] INFO Scanning plugins with ReflectionScanner took 738 ms (org.apache.kafka.connect.runtime.isolation.PluginScanner:71)
[2025-01-03 11:01:55,205] WARN One or more plugins are missing ServiceLoader manifests may not be usable with plugin.discovery=service_load: [
file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1/	com.mongodb.kafka.connect.MongoSinkConnector	sink	1.14.1
file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1/	com.mongodb.kafka.connect.MongoSourceConnector	source	1.14.1
file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/	io.debezium.connector.mongodb.MongoDbSinkConnector	sink	3.0.5.Final
]
Read the documentation at https://kafka.apache.org/documentation.html#connect_plugindiscovery for instructions on migrating your plugins to take advantage of the performance improvements of service_load mode. To silence this warning, set plugin.discovery=only_scan in the worker config. (org.apache.kafka.connect.runtime.isolation.Plugins:123)
[2025-01-03 11:01:55,205] INFO Added plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,205] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,205] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,205] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,206] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,206] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,206] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,206] INFO Added plugin 'io.debezium.transforms.HeaderToValue' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,206] INFO Added plugin 'io.debezium.transforms.SchemaChangeEventFilter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,206] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,206] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,206] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,206] INFO Added plugin 'io.debezium.transforms.ExtractSchemaToNewRecord' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,206] INFO Added plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,206] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,206] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,206] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,206] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,206] INFO Added plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,206] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,206] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,206] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,206] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,206] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,206] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,206] INFO Added plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,206] INFO Added plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,206] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,206] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,207] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,207] INFO Added plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,207] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,207] INFO Added plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,207] INFO Added plugin 'io.debezium.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,207] INFO Added plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,207] INFO Added plugin 'io.debezium.transforms.partitions.PartitionRouting' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,207] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,207] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,207] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,207] INFO Added plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,207] INFO Added plugin 'io.debezium.transforms.TimezoneConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,207] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,207] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,207] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,207] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,207] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,207] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,207] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,207] INFO Added plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,207] INFO Added plugin 'io.debezium.transforms.ExtractChangedRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,207] INFO Added plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,207] INFO Added plugin 'io.debezium.connector.mongodb.transforms.outbox.MongoEventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,207] INFO Added plugin 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,208] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,208] INFO Added plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,208] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,208] INFO Added plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,208] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,208] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,208] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,208] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,208] INFO Added plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,208] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,208] INFO Added plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,208] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,208] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,208] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,208] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,208] INFO Added plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,208] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,208] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,208] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:01:55,209] INFO Added alias 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,209] INFO Added alias 'ExtractNewDocumentState' to plugin 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,209] INFO Added alias 'JsonConverter' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,210] INFO Added alias 'DebeziumMySql' to plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,210] INFO Added alias 'MongoSourceConnector' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,210] INFO Added alias 'CloudEventsConverter' to plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,210] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,210] INFO Added alias 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,210] INFO Added alias 'DebeziumMySqlConnectRestExtension' to plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,210] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,210] INFO Added alias 'MySql' to plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,210] INFO Added alias 'MongoDbSinkConnector' to plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,210] INFO Added alias 'EnvVar' to plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,210] INFO Added alias 'EnvVarConfigProvider' to plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,210] INFO Added alias 'MongoDb' to plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,210] INFO Added alias 'MirrorCheckpointConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,210] INFO Added alias 'Boolean' to plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,210] INFO Added alias 'MySqlConnector' to plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,210] INFO Added alias 'NoneConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,210] INFO Added alias 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,210] INFO Added alias 'HeaderToValue' to plugin 'io.debezium.transforms.HeaderToValue' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,210] INFO Added alias 'PartitionRouting' to plugin 'io.debezium.transforms.partitions.PartitionRouting' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,210] INFO Added alias 'MongoDbSink' to plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,210] INFO Added alias 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,210] INFO Added alias 'StringConverter' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,210] INFO Added alias 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,210] INFO Added alias 'ByLogicalTableRouter' to plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,210] INFO Added alias 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,210] INFO Added alias 'MongoDbConnector' to plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,210] INFO Added alias 'FileConfigProvider' to plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,210] INFO Added alias 'IntegerConverter' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,211] INFO Added alias 'EventRouter' to plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,211] INFO Added alias 'SchemaChangeEventFilter' to plugin 'io.debezium.transforms.SchemaChangeEventFilter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,211] INFO Added alias 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,211] INFO Added alias 'LongConverter' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,211] INFO Added alias 'DropHeaders' to plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,211] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,211] INFO Added alias 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,211] INFO Added alias 'ExtractSchemaToNewRecord' to plugin 'io.debezium.transforms.ExtractSchemaToNewRecord' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,211] INFO Added alias 'BinaryData' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,211] INFO Added alias 'File' to plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,211] INFO Added alias 'FloatConverter' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,211] INFO Added alias 'DirectoryConfigProvider' to plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,211] INFO Added alias 'ReadToInsertEvent' to plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,211] INFO Added alias 'ShortConverter' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,211] INFO Added alias 'CloudEvents' to plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,211] INFO Added alias 'ActivateTracingSpan' to plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,211] INFO Added alias 'TimezoneConverter' to plugin 'io.debezium.transforms.TimezoneConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,211] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,211] INFO Added alias 'DoubleConverter' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,211] INFO Added alias 'Simple' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,211] INFO Added alias 'BinaryDataConverter' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,211] INFO Added alias 'AllConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,211] INFO Added alias 'ExtractNewRecordState' to plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,211] INFO Added alias 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,211] INFO Added alias 'DebeziumMongoDb' to plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,211] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,211] INFO Added alias 'MirrorHeartbeatConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,211] INFO Added alias 'InsertHeader' to plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,211] INFO Added alias 'Directory' to plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,211] INFO Added alias 'MirrorSourceConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,212] INFO Added alias 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,212] INFO Added alias 'MongoSinkConnector' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,212] INFO Added alias 'PrincipalConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,212] INFO Added alias 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,212] INFO Added alias 'DebeziumMongoDbConnectRestExtension' to plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,212] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,212] INFO Added alias 'BooleanConverter' to plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,212] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,212] INFO Added alias 'MongoEventRouter' to plugin 'io.debezium.connector.mongodb.transforms.outbox.MongoEventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,212] INFO Added alias 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,212] INFO Added alias 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,212] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,212] INFO Added alias 'ExtractChangedRecordState' to plugin 'io.debezium.transforms.ExtractChangedRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,212] INFO Added alias 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,212] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,212] INFO Added alias 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:01:55,226] INFO DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	config.providers = []
	config.storage.replication.factor = 1
	config.storage.topic = connect-configs
	connect.protocol = sessioned
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = All
	exactly.once.source.support = disabled
	group.id = connect-cluster
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	heartbeat.interval.ms = 3000
	inter.worker.key.generation.algorithm = HmacSHA256
	inter.worker.key.size = null
	inter.worker.key.ttl.ms = 3600000
	inter.worker.signature.algorithm = HmacSHA256
	inter.worker.verification.algorithms = [HmacSHA256]
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = [http://:8083]
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 1
	offset.storage.topic = connect-offsets
	plugin.discovery = hybrid_warn
	plugin.path = [/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins]
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	status.storage.partitions = 5
	status.storage.replication.factor = 1
	status.storage.topic = connect-status
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000
 (org.apache.kafka.connect.runtime.distributed.DistributedConfig:371)
[2025-01-03 11:01:55,227] INFO Creating Kafka admin client (org.apache.kafka.connect.runtime.WorkerConfig:281)
[2025-01-03 11:01:55,229] INFO AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:371)
[2025-01-03 11:01:55,249] INFO These configurations '[config.storage.topic, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2025-01-03 11:01:55,249] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:01:55,249] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:01:55,249] INFO Kafka startTimeMs: 1735923715249 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:01:55,356] INFO Kafka cluster ID: tztLDPmmThCv7RTvhR2k6w (org.apache.kafka.connect.runtime.WorkerConfig:298)
[2025-01-03 11:01:55,357] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:01:55,359] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:01:55,359] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:55,359] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:01:55,361] INFO PublicConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	listeners = [http://:8083]
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
 (org.apache.kafka.connect.runtime.rest.RestServerConfig$PublicConfig:371)
[2025-01-03 11:01:55,365] INFO Logging initialized @1277ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:170)
[2025-01-03 11:01:55,377] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:125)
[2025-01-03 11:01:55,377] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:196)
[2025-01-03 11:01:55,385] INFO jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 23+37-2369 (org.eclipse.jetty.server.Server:375)
[2025-01-03 11:01:55,393] INFO Started http_8083@f4f843f{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:333)
[2025-01-03 11:01:55,393] INFO Started @1306ms (org.eclipse.jetty.server.Server:415)
[2025-01-03 11:01:55,399] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 11:01:55,400] INFO REST server listening at http://192.168.1.5:8083/, advertising URL http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:216)
[2025-01-03 11:01:55,400] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 11:01:55,400] INFO REST admin endpoints at http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2025-01-03 11:01:55,400] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 11:01:55,400] INFO Setting up All Policy for ConnectorClientConfigOverride. This will allow all client configurations to be overridden (org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy:45)
[2025-01-03 11:01:55,401] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 11:01:55,409] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:01:55,409] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:01:55,409] INFO Kafka startTimeMs: 1735923715409 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:01:55,412] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 11:01:55,412] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 11:01:55,420] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 11:01:55,433] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:01:55,434] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:01:55,434] INFO Kafka startTimeMs: 1735923715433 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:01:55,435] INFO Kafka Connect worker initialization took 1121ms (org.apache.kafka.connect.cli.AbstractConnectCli:141)
[2025-01-03 11:01:55,435] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:67)
[2025-01-03 11:01:55,436] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2025-01-03 11:01:55,436] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder starting (org.apache.kafka.connect.runtime.distributed.DistributedHerder:375)
[2025-01-03 11:01:55,436] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:233)
[2025-01-03 11:01:55,436] INFO Starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:232)
[2025-01-03 11:01:55,436] INFO Starting KafkaBasedLog with topic connect-offsets reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog:254)
[2025-01-03 11:01:55,437] INFO AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-shared-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:371)
[2025-01-03 11:01:55,438] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2025-01-03 11:01:55,438] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:01:55,438] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:01:55,438] INFO Kafka startTimeMs: 1735923715438 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:01:55,447] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:238)
[2025-01-03 11:01:55,455] INFO ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-offsets
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 11:01:55,461] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:334)
[2025-01-03 11:01:55,461] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:339)
[2025-01-03 11:01:55,461] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:132)
[2025-01-03 11:01:55,463] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:01:55,469] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 11:01:55,470] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:01:55,470] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:01:55,470] INFO Kafka startTimeMs: 1735923715469 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:01:55,472] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-offsets
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 11:01:55,473] INFO [Producer clientId=connect-cluster-offsets] Cluster ID: tztLDPmmThCv7RTvhR2k6w (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:01:55,476] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:01:55,487] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2025-01-03 11:01:55,487] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:01:55,487] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:01:55,487] INFO Kafka startTimeMs: 1735923715487 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:01:55,490] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Cluster ID: tztLDPmmThCv7RTvhR2k6w (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:01:55,492] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Assigned to partition(s): connect-offsets-0, connect-offsets-5, connect-offsets-10, connect-offsets-20, connect-offsets-15, connect-offsets-9, connect-offsets-11, connect-offsets-4, connect-offsets-16, connect-offsets-17, connect-offsets-3, connect-offsets-24, connect-offsets-23, connect-offsets-13, connect-offsets-18, connect-offsets-22, connect-offsets-8, connect-offsets-2, connect-offsets-12, connect-offsets-19, connect-offsets-14, connect-offsets-1, connect-offsets-6, connect-offsets-7, connect-offsets-21 (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:579)
[2025-01-03 11:01:55,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:01:55,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-5 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:01:55,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-10 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:01:55,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-20 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:01:55,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-15 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:01:55,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-9 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:01:55,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-11 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:01:55,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:01:55,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-16 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:01:55,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-17 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:01:55,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:01:55,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-24 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:01:55,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-23 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:01:55,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-13 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:01:55,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-18 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:01:55,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-22 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:01:55,493] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-8 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:01:55,494] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:01:55,494] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-12 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:01:55,494] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-19 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:01:55,494] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-14 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:01:55,494] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:01:55,494] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-6 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:01:55,494] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-7 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:01:55,494] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-21 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:01:55,508] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:01:55,508] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:01:55,508] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:01:55,508] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:01:55,509] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:01:55,509] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:01:55,509] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:01:55,509] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:01:55,509] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:01:55,509] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:01:55,509] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:01:55,509] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:01:55,509] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:01:55,509] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:01:55,509] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:01:55,509] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:01:55,509] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:01:55,509] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:01:55,509] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:01:55,509] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:01:55,509] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:01:55,509] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:01:55,509] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:01:55,509] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:01:55,509] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:01:55,533] INFO Finished reading KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:311)
[2025-01-03 11:01:55,533] INFO Started KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:313)
[2025-01-03 11:01:55,533] INFO Finished reading offsets topic and starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:249)
[2025-01-03 11:01:55,534] INFO Worker started (org.apache.kafka.connect.runtime.Worker:243)
[2025-01-03 11:01:55,534] INFO Starting KafkaBasedLog with topic connect-status reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog:254)
[2025-01-03 11:01:55,538] INFO ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-statuses
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 11:01:55,538] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:01:55,540] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 11:01:55,540] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:01:55,540] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:01:55,540] INFO Kafka startTimeMs: 1735923715540 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:01:55,540] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-statuses
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 11:01:55,541] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:01:55,543] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2025-01-03 11:01:55,543] INFO [Producer clientId=connect-cluster-statuses] Cluster ID: tztLDPmmThCv7RTvhR2k6w (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:01:55,543] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:01:55,543] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:01:55,543] INFO Kafka startTimeMs: 1735923715543 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:01:55,546] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Cluster ID: tztLDPmmThCv7RTvhR2k6w (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:01:55,547] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Assigned to partition(s): connect-status-0, connect-status-4, connect-status-1, connect-status-2, connect-status-3 (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:579)
[2025-01-03 11:01:55,547] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:01:55,547] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:01:55,547] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:01:55,547] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:01:55,547] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:01:55,554] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:01:55,554] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:01:55,554] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:01:55,554] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:01:55,554] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:01:55,559] INFO Finished reading KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:311)
[2025-01-03 11:01:55,560] INFO Started KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:313)
[2025-01-03 11:01:55,561] INFO Starting KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:378)
[2025-01-03 11:01:55,561] INFO Starting KafkaBasedLog with topic connect-configs reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog:254)
[2025-01-03 11:01:55,571] INFO ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-configs
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 11:01:55,571] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:01:55,573] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 11:01:55,573] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:01:55,573] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:01:55,573] INFO Kafka startTimeMs: 1735923715573 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:01:55,573] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-configs
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 11:01:55,573] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:01:55,575] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2025-01-03 11:01:55,575] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:01:55,575] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:01:55,575] INFO Kafka startTimeMs: 1735923715575 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:01:55,575] INFO [Producer clientId=connect-cluster-configs] Cluster ID: tztLDPmmThCv7RTvhR2k6w (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:01:55,578] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Cluster ID: tztLDPmmThCv7RTvhR2k6w (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:01:55,578] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Assigned to partition(s): connect-configs-0 (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:579)
[2025-01-03 11:01:55,578] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Seeking to earliest offset of partition connect-configs-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:01:55,585] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Resetting offset for partition connect-configs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:01:55,588] INFO Finished reading KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:311)
[2025-01-03 11:01:55,588] INFO Started KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:313)
[2025-01-03 11:01:55,588] INFO Started KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:402)
[2025-01-03 11:01:55,591] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Cluster ID: tztLDPmmThCv7RTvhR2k6w (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:01:55,592] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Discovered group coordinator 192.168.1.5:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:937)
[2025-01-03 11:01:55,592] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2025-01-03 11:01:55,592] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 11:01:55,595] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 11:01:55,596] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=5, memberId='connect-192.168.1.5:8083-3cffb021-6cce-4a3a-95e1-f39f15ee0d8f', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2025-01-03 11:01:55,603] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=5, memberId='connect-192.168.1.5:8083-3cffb021-6cce-4a3a-95e1-f39f15ee0d8f', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2025-01-03 11:01:55,603] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Joined group at generation 5 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.1.5:8083-3cffb021-6cce-4a3a-95e1-f39f15ee0d8f', leaderUrl='http://192.168.1.5:8083/', offset=4, connectorIds=[mariadb-connector], taskIds=[mariadb-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2025-01-03 11:01:55,603] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder started (org.apache.kafka.connect.runtime.distributed.DistributedHerder:387)
[2025-01-03 11:01:55,603] WARN [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Catching up to assignment's config offset. (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1811)
[2025-01-03 11:01:55,604] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Current config state offset -1 is behind group assignment 4, reading to end of config log (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1884)
[2025-01-03 11:01:55,605] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished reading to end of log and updated config snapshot, new config log offset: 4 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1911)
[2025-01-03 11:01:55,605] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 4 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2025-01-03 11:01:55,605] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connector mariadb-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2025-01-03 11:01:55,605] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting task mariadb-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2025-01-03 11:01:55,607] INFO [mariadb-connector|worker] Creating connector mariadb-connector of type io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:313)
[2025-01-03 11:01:55,607] INFO [mariadb-connector|task-0] Creating task mariadb-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2025-01-03 11:01:55,607] INFO [mariadb-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mariadb-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2025-01-03 11:01:55,608] INFO [mariadb-connector|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2025-01-03 11:01:55,608] INFO [mariadb-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mariadb-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 11:01:55,608] INFO [mariadb-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 11:01:55,609] INFO [mariadb-connector|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.mysql.MySqlConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2025-01-03 11:01:55,610] INFO [mariadb-connector|worker] Instantiated connector mariadb-connector with version 3.0.5.Final of type class io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:335)
[2025-01-03 11:01:55,610] INFO [mariadb-connector|worker] Finished creating connector mariadb-connector (org.apache.kafka.connect.runtime.Worker:356)
[2025-01-03 11:01:55,610] INFO [mariadb-connector|task-0] Instantiated task mariadb-connector-0 with version 3.0.5.Final of type io.debezium.connector.mysql.MySqlConnectorTask (org.apache.kafka.connect.runtime.Worker:665)
[2025-01-03 11:01:55,611] INFO [mariadb-connector|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 11:01:55,611] INFO [mariadb-connector|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mariadb-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:678)
[2025-01-03 11:01:55,611] INFO [mariadb-connector|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 11:01:55,611] INFO [mariadb-connector|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mariadb-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:684)
[2025-01-03 11:01:55,611] INFO [mariadb-connector|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mariadb-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2025-01-03 11:01:55,612] INFO [mariadb-connector|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:1795)
[2025-01-03 11:01:55,612] INFO Started o.e.j.s.ServletContextHandler@48af5f38{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:921)
[2025-01-03 11:01:55,612] INFO [mariadb-connector|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2025-01-03 11:01:55,612] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:303)
[2025-01-03 11:01:55,612] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:77)
[2025-01-03 11:01:55,612] INFO [mariadb-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 11:01:55,612] INFO [mariadb-connector|task-0] ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-mariadb-connector-0
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 11:01:55,613] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:01:55,614] INFO [mariadb-connector|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 11:01:55,614] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:01:55,614] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:01:55,614] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735923715614 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:01:55,616] INFO [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] Cluster ID: tztLDPmmThCv7RTvhR2k6w (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:01:55,617] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2025-01-03 11:01:55,618] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2025-01-03 11:01:55,618] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 11:01:55,627] INFO [mariadb-connector|task-0] Starting MySqlConnectorTask with configuration:
   connector.class = io.debezium.connector.mysql.MySqlConnector
   database.user = root
   database.server.id = 184054
   database.history.kafka.bootstrap.servers = localhost:9092
   database.history.kafka.topic = db.history.leafy_factory
   database.server.name = leafy_factory
   schema.history.internal.kafka.bootstrap.servers = localhost:9092
   database.port = 3306
   include.schema.changes = false
   topic.prefix = db_
   schema.history.internal.kafka.topic = db.history.internal
   task.class = io.debezium.connector.mysql.MySqlConnectorTask
   database.hostname = localhost
   database.password = ********
   name = mariadb-connector
   database.include.list = leafy_factory
 (io.debezium.connector.common.BaseSourceTask:250)
[2025-01-03 11:01:55,629] INFO [mariadb-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mysql.MySqlSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2025-01-03 11:01:55,634] INFO [mariadb-connector|task-0] Loading the custom topic naming strategy plugin: io.debezium.schema.DefaultTopicNamingStrategy (io.debezium.config.CommonConnectorConfig:1401)
[2025-01-03 11:01:56,031] INFO [mariadb-connector|task-0] Using 'SHOW MASTER STATUS' to get binary log status (io.debezium.connector.mysql.jdbc.MySqlConnection:41)
[2025-01-03 11:01:56,036] INFO [mariadb-connector|task-0] Found previous partition offset BinlogPartition{serverName='db_'} io.debezium.connector.mysql.MySqlPartition@183c0: {file=mariadb-bin.000001, pos=0, row=1} (io.debezium.connector.common.BaseSourceTask:529)
[2025-01-03 11:01:56,070] INFO [mariadb-connector|task-0] KafkaSchemaHistory Consumer config: {key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, enable.auto.commit=false, group.id=db_-schemahistory, bootstrap.servers=localhost:9092, fetch.min.bytes=1, session.timeout.ms=10000, auto.offset.reset=earliest, client.id=db_-schemahistory} (io.debezium.storage.kafka.history.KafkaSchemaHistory:245)
[2025-01-03 11:01:56,071] INFO [mariadb-connector|task-0] KafkaSchemaHistory Producer config: {retries=1, value.serializer=org.apache.kafka.common.serialization.StringSerializer, acks=1, batch.size=32768, max.block.ms=10000, bootstrap.servers=localhost:9092, buffer.memory=1048576, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=db_-schemahistory, linger.ms=0} (io.debezium.storage.kafka.history.KafkaSchemaHistory:246)
[2025-01-03 11:01:56,071] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = db-history-config-check (io.debezium.util.Threads:270)
[2025-01-03 11:01:56,072] INFO [mariadb-connector|task-0] Idempotence will be disabled because acks is set to 1, not set to 'all'. (org.apache.kafka.clients.producer.ProducerConfig:587)
[2025-01-03 11:01:56,072] INFO [mariadb-connector|task-0] ProducerConfig values: 
	acks = 1
	auto.include.jmx.reporter = true
	batch.size = 32768
	bootstrap.servers = [localhost:9092]
	buffer.memory = 1048576
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 11:01:56,073] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:01:56,074] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:01:56,074] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:01:56,074] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735923716074 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:01:56,075] INFO [mariadb-connector|task-0] [Producer clientId=db_-schemahistory] Cluster ID: tztLDPmmThCv7RTvhR2k6w (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:01:56,127] INFO [mariadb-connector|task-0] Using 'SHOW MASTER STATUS' to get binary log status (io.debezium.connector.mysql.jdbc.MySqlConnection:41)
[2025-01-03 11:01:56,134] INFO [mariadb-connector|task-0] Closing connection before starting schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:123)
[2025-01-03 11:01:56,135] INFO [mariadb-connector|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:983)
[2025-01-03 11:01:56,135] INFO [mariadb-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = db_-schemahistory
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 11:01:56,136] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:01:56,137] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:01:56,137] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:01:56,137] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735923716137 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:01:56,139] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Cluster ID: tztLDPmmThCv7RTvhR2k6w (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:01:56,140] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 11:01:56,140] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 11:01:56,141] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:01:56,141] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:56,141] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:56,141] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:01:56,142] INFO [mariadb-connector|task-0] App info kafka.consumer for db_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:01:56,142] INFO [mariadb-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = db_-schemahistory
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 11:01:56,142] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:01:56,143] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:01:56,143] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:01:56,143] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735923716143 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:01:56,144] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-db-history-config-check (io.debezium.util.Threads:287)
[2025-01-03 11:01:56,144] INFO [mariadb-connector|task-0] AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory-topic-check
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:371)
[2025-01-03 11:01:56,145] INFO [mariadb-connector|task-0] These configurations '[value.serializer, acks, batch.size, max.block.ms, buffer.memory, key.serializer, linger.ms]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2025-01-03 11:01:56,145] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:01:56,145] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:01:56,145] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735923716145 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:01:56,146] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Cluster ID: tztLDPmmThCv7RTvhR2k6w (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:01:56,152] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 11:01:56,152] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 11:01:56,153] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:01:56,153] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:56,153] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:56,153] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:01:56,153] INFO [mariadb-connector|task-0] Database schema history topic 'db.history.internal' has correct settings (io.debezium.storage.kafka.history.KafkaSchemaHistory:473)
[2025-01-03 11:01:56,153] INFO [mariadb-connector|task-0] App info kafka.admin.client for db_-schemahistory-topic-check unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:01:56,154] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:01:56,154] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:56,154] INFO [mariadb-connector|task-0] App info kafka.consumer for db_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:01:56,154] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:01:56,154] INFO [mariadb-connector|task-0] Get all known binlogs (io.debezium.connector.binlog.jdbc.BinlogConnectorConnection:157)
[2025-01-03 11:01:56,178] INFO [mariadb-connector|task-0] Server has the binlog file 'mariadb-bin.000001' required by the connector (io.debezium.connector.binlog.jdbc.BinlogConnectorConnection:399)
[2025-01-03 11:01:56,179] INFO [mariadb-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = db_-schemahistory
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 11:01:56,179] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:01:56,180] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:01:56,180] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:01:56,180] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735923716180 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:01:56,182] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Cluster ID: tztLDPmmThCv7RTvhR2k6w (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:01:56,182] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 11:01:56,182] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 11:01:56,183] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:01:56,183] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:56,183] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:56,183] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:01:56,183] INFO [mariadb-connector|task-0] App info kafka.consumer for db_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:01:56,183] INFO [mariadb-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = db_-schemahistory
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 11:01:56,184] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:01:56,185] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:01:56,185] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:01:56,185] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735923716185 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:01:56,186] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Cluster ID: tztLDPmmThCv7RTvhR2k6w (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:01:56,189] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 11:01:56,189] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 11:01:56,189] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:01:56,189] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:56,189] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:56,189] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:01:56,190] INFO [mariadb-connector|task-0] App info kafka.consumer for db_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:01:56,190] INFO [mariadb-connector|task-0] Started database schema history recovery (io.debezium.relational.history.SchemaHistoryMetrics:115)
[2025-01-03 11:01:56,192] INFO [mariadb-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = db_-schemahistory
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 11:01:56,192] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:01:56,193] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:01:56,193] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:01:56,193] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735923716193 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:01:56,194] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Subscribed to topic(s): db.history.internal (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:481)
[2025-01-03 11:01:56,195] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Cluster ID: tztLDPmmThCv7RTvhR2k6w (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:01:56,198] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Discovered group coordinator 192.168.1.5:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:937)
[2025-01-03 11:01:56,198] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:605)
[2025-01-03 11:01:56,201] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Request joining group due to: need to re-join with the given member-id: db_-schemahistory-eb0d820e-d0f7-4e67-b0d1-d2b15eec5973 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 11:01:56,201] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:605)
[2025-01-03 11:01:56,202] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Successfully joined group with generation Generation{generationId=1, memberId='db_-schemahistory-eb0d820e-d0f7-4e67-b0d1-d2b15eec5973', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:666)
[2025-01-03 11:01:56,204] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Finished assignment for group at generation 1: {db_-schemahistory-eb0d820e-d0f7-4e67-b0d1-d2b15eec5973=Assignment(partitions=[db.history.internal-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:664)
[2025-01-03 11:01:56,206] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Successfully synced group in generation Generation{generationId=1, memberId='db_-schemahistory-eb0d820e-d0f7-4e67-b0d1-d2b15eec5973', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:843)
[2025-01-03 11:01:56,206] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Notifying assignor about the new Assignment(partitions=[db.history.internal-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:324)
[2025-01-03 11:01:56,206] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Adding newly assigned partitions: db.history.internal-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:58)
[2025-01-03 11:01:56,207] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Found no committed offset for partition db.history.internal-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1508)
[2025-01-03 11:01:56,207] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Resetting offset for partition db.history.internal-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:01:56,215] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Revoke previously assigned partitions db.history.internal-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:80)
[2025-01-03 11:01:56,215] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Member db_-schemahistory-eb0d820e-d0f7-4e67-b0d1-d2b15eec5973 sending LeaveGroup request to coordinator 192.168.1.5:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1174)
[2025-01-03 11:01:56,216] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 11:01:56,216] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 11:01:56,713] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:01:56,714] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:56,714] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:56,714] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:01:56,718] INFO [mariadb-connector|task-0] App info kafka.consumer for db_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:01:56,718] INFO [mariadb-connector|task-0] Finished database schema history recovery of 24 change(s) in 528 ms (io.debezium.relational.history.SchemaHistoryMetrics:121)
[2025-01-03 11:01:56,720] INFO [mariadb-connector|task-0] Reconnecting after finishing schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:136)
[2025-01-03 11:01:56,740] INFO [mariadb-connector|task-0] Found previous offset BinlogOffsetContext{sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=BinlogSourceInfo{currentGtid='null', currentBinlogFilename='mariadb-bin.000001', currentBinlogPosition=0, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery='null', tableIds=[], databaseName='null'}, snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet='null', currentGtidSet='null', restartBinlogFilename='mariadb-bin.000001', restartBinlogPosition=0, restartRowsToSkip=1, restartEventsToSkip=0, currentEventLengthInBytes=0, inTransaction=false, transactionId='null', incrementalSnapshotContext=IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]} (io.debezium.connector.mysql.MySqlConnectorTask:150)
[2025-01-03 11:01:56,755] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = SignalProcessor (io.debezium.util.Threads:270)
[2025-01-03 11:01:56,766] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = change-event-source-coordinator (io.debezium.util.Threads:270)
[2025-01-03 11:01:56,766] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = blocking-snapshot (io.debezium.util.Threads:270)
[2025-01-03 11:01:56,768] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-change-event-source-coordinator (io.debezium.util.Threads:287)
[2025-01-03 11:01:56,768] INFO [mariadb-connector|task-0] WorkerSourceTask{id=mariadb-connector-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.AbstractWorkerSourceTask:280)
[2025-01-03 11:01:56,770] INFO [mariadb-connector|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:137)
[2025-01-03 11:01:56,770] INFO [mariadb-connector|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:140)
[2025-01-03 11:01:56,774] INFO [mariadb-connector|task-0] A previous offset indicating a completed snapshot has been found. (io.debezium.relational.RelationalSnapshotChangeEventSource:275)
[2025-01-03 11:01:56,776] INFO [mariadb-connector|task-0] Snapshot ended with SnapshotResult [status=SKIPPED, offset=BinlogOffsetContext{sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=BinlogSourceInfo{currentGtid='null', currentBinlogFilename='mariadb-bin.000001', currentBinlogPosition=0, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery='null', tableIds=[], databaseName='null'}, snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet='null', currentGtidSet='null', restartBinlogFilename='mariadb-bin.000001', restartBinlogPosition=0, restartRowsToSkip=1, restartEventsToSkip=0, currentEventLengthInBytes=0, inTransaction=false, transactionId='null', incrementalSnapshotContext=IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]}] (io.debezium.pipeline.ChangeEventSourceCoordinator:298)
[2025-01-03 11:01:56,779] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = binlog-client (io.debezium.util.Threads:270)
[2025-01-03 11:01:56,782] INFO [mariadb-connector|task-0] Enable ssl PREFERRED mode for connector db_ (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:1289)
[2025-01-03 11:01:56,788] INFO [mariadb-connector|task-0] SignalProcessor started. Scheduling it every 5000ms (io.debezium.pipeline.signal.SignalProcessor:105)
[2025-01-03 11:01:56,788] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-SignalProcessor (io.debezium.util.Threads:287)
[2025-01-03 11:01:56,788] INFO [mariadb-connector|task-0] Starting streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:323)
[2025-01-03 11:01:56,788] WARN [mariadb-connector|task-0] After applying the include/exclude list filters, no changes will be captured. Please check your configuration! (io.debezium.relational.RelationalDatabaseSchema:72)
[2025-01-03 11:01:56,790] INFO [mariadb-connector|task-0] Skip 0 events on streaming start (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:278)
[2025-01-03 11:01:56,790] INFO [mariadb-connector|task-0] Skip 1 rows on streaming start (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:282)
[2025-01-03 11:01:56,790] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-binlog-client (io.debezium.util.Threads:287)
[2025-01-03 11:01:56,791] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-binlog-client (io.debezium.util.Threads:287)
[2025-01-03 11:01:56,822] INFO [mariadb-connector|task-0] Connected to binlog at localhost:3306, starting at BinlogOffsetContext{sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=BinlogSourceInfo{currentGtid='null', currentBinlogFilename='mariadb-bin.000001', currentBinlogPosition=0, currentRowNumber=0, serverId=0, sourceTime=null, threadId=-1, currentQuery='null', tableIds=[], databaseName='null'}, snapshotCompleted=false, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet='null', currentGtidSet='null', restartBinlogFilename='mariadb-bin.000001', restartBinlogPosition=0, restartRowsToSkip=1, restartEventsToSkip=0, currentEventLengthInBytes=0, inTransaction=false, transactionId='null', incrementalSnapshotContext=IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]} (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:1232)
[2025-01-03 11:01:56,823] INFO [mariadb-connector|task-0] Waiting for keepalive thread to start (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:299)
[2025-01-03 11:01:56,823] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-binlog-client (io.debezium.util.Threads:287)
[2025-01-03 11:01:56,928] INFO [mariadb-connector|task-0] Keepalive thread is running (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:306)
[2025-01-03 11:01:57,006] ERROR [mariadb-connector|task-0] Encountered change event 'Event{header=EventHeaderV4{timestamp=1735846965000, eventType=TABLE_MAP, serverId=1, headerLength=19, dataLength=69, nextPosition=0, flags=0}, data=TableMapEventData{tableId=26, database='leafy_factory', table='raw_materials', columnTypes=3, 15, 15, 15, 15, 3, 15, 15, -10, columnMetadata=0, 400, 400, 400, 400, 0, 400, 400, 522, columnNullability={}, eventMetadata=null}}' at offset {file=mariadb-bin.000001, pos=0, server_id=1} for table leafy_factory.raw_materials whose schema isn't known to this connector. One possible cause is an incomplete database schema history topic. Take a new snapshot in this case.
Use the mysqlbinlog tool to view the problematic event: mysqlbinlog --start-position=-88 --stop-position=0 --verbose mariadb-bin.000001 (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:991)
[2025-01-03 11:01:57,007] ERROR [mariadb-connector|task-0] Error during binlog processing. Last offset stored = {file=mariadb-bin.000001, pos=372, server_id=1, event=1}, binlog reader near position = mariadb-bin.000001/372 (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:1157)
[2025-01-03 11:01:57,007] ERROR [mariadb-connector|task-0] Producer failure (io.debezium.pipeline.ErrorHandler:52)
io.debezium.DebeziumException: Error processing binlog event
	at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.handleEvent(BinlogStreamingChangeEventSource.java:591)
	at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.lambda$execute$17(BinlogStreamingChangeEventSource.java:209)
	at com.github.shyiko.mysql.binlog.BinaryLogClient.notifyEventListeners(BinaryLogClient.java:1281)
	at com.github.shyiko.mysql.binlog.BinaryLogClient.listenForEventPackets(BinaryLogClient.java:1103)
	at com.github.shyiko.mysql.binlog.BinaryLogClient.connect(BinaryLogClient.java:657)
	at com.github.shyiko.mysql.binlog.BinaryLogClient$7.run(BinaryLogClient.java:959)
	at java.base/java.lang.Thread.run(Thread.java:1575)
Caused by: io.debezium.DebeziumException: Encountered change event for table leafy_factory.raw_materials whose schema isn't known to this connector
	at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.informAboutUnknownTableIfRequired(BinlogStreamingChangeEventSource.java:996)
	at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.informAboutUnknownTableIfRequired(BinlogStreamingChangeEventSource.java:1048)
	at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.handleUpdateTableMetadata(BinlogStreamingChangeEventSource.java:797)
	at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.lambda$execute$4(BinlogStreamingChangeEventSource.java:178)
	at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.handleEvent(BinlogStreamingChangeEventSource.java:571)
	... 6 more
[2025-01-03 11:01:57,009] INFO [mariadb-connector|task-0] Error processing binlog event, and propagating to Kafka Connect so it stops this connector. Future binlog events read before connector is shutdown will be ignored. (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:596)
[2025-01-03 11:01:57,278] ERROR [mariadb-connector|task-0] WorkerSourceTask{id=mariadb-connector-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:234)
org.apache.kafka.connect.errors.ConnectException: An exception occurred in the change event producer. This connector will be stopped.
	at io.debezium.pipeline.ErrorHandler.setProducerThrowable(ErrorHandler.java:67)
	at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.handleEvent(BinlogStreamingChangeEventSource.java:591)
	at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.lambda$execute$17(BinlogStreamingChangeEventSource.java:209)
	at com.github.shyiko.mysql.binlog.BinaryLogClient.notifyEventListeners(BinaryLogClient.java:1281)
	at com.github.shyiko.mysql.binlog.BinaryLogClient.listenForEventPackets(BinaryLogClient.java:1103)
	at com.github.shyiko.mysql.binlog.BinaryLogClient.connect(BinaryLogClient.java:657)
	at com.github.shyiko.mysql.binlog.BinaryLogClient$7.run(BinaryLogClient.java:959)
	at java.base/java.lang.Thread.run(Thread.java:1575)
Caused by: io.debezium.DebeziumException: Error processing binlog event
	... 7 more
Caused by: io.debezium.DebeziumException: Encountered change event for table leafy_factory.raw_materials whose schema isn't known to this connector
	at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.informAboutUnknownTableIfRequired(BinlogStreamingChangeEventSource.java:996)
	at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.informAboutUnknownTableIfRequired(BinlogStreamingChangeEventSource.java:1048)
	at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.handleUpdateTableMetadata(BinlogStreamingChangeEventSource.java:797)
	at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.lambda$execute$4(BinlogStreamingChangeEventSource.java:178)
	at io.debezium.connector.binlog.BinlogStreamingChangeEventSource.handleEvent(BinlogStreamingChangeEventSource.java:571)
	... 6 more
[2025-01-03 11:01:57,280] INFO [mariadb-connector|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:434)
[2025-01-03 11:01:57,347] INFO [mariadb-connector|task-0] Finished streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:325)
[2025-01-03 11:01:57,347] INFO [mariadb-connector|task-0] Stopped reading binlog after 400 events, last recorded offset: {file=mariadb-bin.000001, pos=58880, server_id=1, event=1} (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:1218)
[2025-01-03 11:01:57,349] INFO [mariadb-connector|task-0] SignalProcessor stopped (io.debezium.pipeline.signal.SignalProcessor:127)
[2025-01-03 11:01:57,349] INFO [mariadb-connector|task-0] Debezium ServiceRegistry stopped. (io.debezium.service.DefaultServiceRegistry:105)
[2025-01-03 11:01:57,352] INFO [mariadb-connector|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:983)
[2025-01-03 11:01:57,353] INFO [mariadb-connector|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:983)
[2025-01-03 11:01:57,354] INFO [mariadb-connector|task-0] [Producer clientId=db_-schemahistory] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 11:01:57,357] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:01:57,357] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:57,358] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:57,358] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:01:57,360] INFO [mariadb-connector|task-0] App info kafka.producer for db_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:01:57,360] INFO [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 11:01:57,364] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:01:57,364] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:57,364] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:01:57,364] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:01:57,364] INFO [mariadb-connector|task-0] App info kafka.producer for connector-producer-mariadb-connector-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:02:29,307] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:87)
[2025-01-03 11:02:29,309] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:358)
[2025-01-03 11:02:29,315] INFO Stopped http_8083@f4f843f{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:383)
[2025-01-03 11:02:29,316] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:149)
[2025-01-03 11:02:29,320] INFO Stopped o.e.j.s.ServletContextHandler@48af5f38{/,null,STOPPED} (org.eclipse.jetty.server.handler.ContextHandler:1159)
[2025-01-03 11:02:29,320] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:387)
[2025-01-03 11:02:29,321] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder stopping (org.apache.kafka.connect.runtime.distributed.DistributedHerder:851)
[2025-01-03 11:02:29,321] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Stopping connectors and tasks that are still assigned to this worker. (org.apache.kafka.connect.runtime.distributed.DistributedHerder:808)
[2025-01-03 11:02:29,321] INFO [mariadb-connector|worker] Stopping connector mariadb-connector (org.apache.kafka.connect.runtime.Worker:452)
[2025-01-03 11:02:29,321] INFO [mariadb-connector|worker] Scheduled shutdown for WorkerConnector{id=mariadb-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2025-01-03 11:02:29,321] INFO [mariadb-connector|worker] Completed shutdown for WorkerConnector{id=mariadb-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2025-01-03 11:02:29,322] INFO [mariadb-connector|task-0] Stopping task mariadb-connector-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2025-01-03 11:02:29,323] INFO Stopping KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:317)
[2025-01-03 11:02:29,325] INFO [Producer clientId=connect-cluster-statuses] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 11:02:29,327] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:02:29,327] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:02:29,327] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:02:29,327] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:02:29,327] INFO App info kafka.producer for connect-cluster-statuses unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:02:29,328] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 11:02:29,328] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 11:02:29,330] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Node 0 sent an invalid full fetch response with extraIds=(bOSiVzVBRAqjWd9QVo1Ovg), response=() (org.apache.kafka.clients.FetchSessionHandler:556)
[2025-01-03 11:02:29,330] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:02:29,330] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:02:29,330] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:02:29,331] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:02:29,332] INFO App info kafka.consumer for connect-cluster-statuses unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:02:29,332] INFO Stopped KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:341)
[2025-01-03 11:02:29,332] INFO Closing KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:407)
[2025-01-03 11:02:29,333] INFO Stopping KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:317)
[2025-01-03 11:02:29,333] INFO [Producer clientId=connect-cluster-configs] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 11:02:29,334] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:02:29,335] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:02:29,335] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:02:29,335] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:02:29,335] INFO App info kafka.producer for connect-cluster-configs unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:02:29,335] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 11:02:29,335] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 11:02:29,386] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:02:29,386] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:02:29,386] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:02:29,386] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:02:29,387] INFO App info kafka.consumer for connect-cluster-configs unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:02:29,387] INFO Stopped KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:341)
[2025-01-03 11:02:29,388] INFO Closed KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:412)
[2025-01-03 11:02:29,388] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:250)
[2025-01-03 11:02:29,388] INFO Stopping KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:261)
[2025-01-03 11:02:29,388] INFO Stopping KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:317)
[2025-01-03 11:02:29,388] INFO [Producer clientId=connect-cluster-offsets] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 11:02:29,389] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:02:29,389] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:02:29,389] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:02:29,389] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:02:29,389] INFO App info kafka.producer for connect-cluster-offsets unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:02:29,390] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 11:02:29,390] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 11:02:29,867] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:02:29,867] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:02:29,868] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:02:29,868] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:02:29,871] INFO App info kafka.consumer for connect-cluster-offsets unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:02:29,872] INFO Stopped KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:341)
[2025-01-03 11:02:29,872] INFO Stopped KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:263)
[2025-01-03 11:02:29,872] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:02:29,872] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:02:29,872] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:02:29,872] INFO App info kafka.connect for 192.168.1.5:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:02:29,872] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:271)
[2025-01-03 11:02:29,874] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Member connect-192.168.1.5:8083-3cffb021-6cce-4a3a-95e1-f39f15ee0d8f sending LeaveGroup request to coordinator 192.168.1.5:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1174)
[2025-01-03 11:02:29,875] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1056)
[2025-01-03 11:02:29,875] WARN [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Close timed out with 1 pending requests to coordinator, terminating client connections (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1141)
[2025-01-03 11:02:29,875] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:02:29,876] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:02:29,876] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:02:29,878] INFO App info kafka.connect for connect-192.168.1.5:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:02:29,879] INFO App info kafka.admin.client for connect-cluster-shared-admin unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:02:29,881] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:02:29,882] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:02:29,882] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:02:29,882] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder stopped (org.apache.kafka.connect.runtime.distributed.DistributedHerder:394)
[2025-01-03 11:02:29,882] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder stopped (org.apache.kafka.connect.runtime.distributed.DistributedHerder:858)
[2025-01-03 11:02:29,882] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:92)
[2025-01-03 11:05:17,648] INFO Kafka Connect worker initializing ... (org.apache.kafka.connect.cli.AbstractConnectCli:114)
[2025-01-03 11:05:17,650] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote=true, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 23, 23+37-2369
	jvm.classpath = /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/activation-1.1.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/aopalliance-repackaged-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/argparse4j-0.7.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/audience-annotations-0.12.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/caffeine-2.9.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-beanutils-1.9.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-cli-1.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-collections-3.2.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-digester-2.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-io-2.14.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-lang3-3.12.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-logging-1.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-validator-1.7.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-basic-auth-extension-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-json-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-mirror-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-mirror-client-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-runtime-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-transforms-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/error_prone_annotations-2.10.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-api-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-locator-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-utils-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-annotations-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-core-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-databind-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-dataformat-csv-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-datatype-jdk8-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-jaxrs-base-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-jaxrs-json-provider-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-afterburner-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-jaxb-annotations-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-scala_2.13-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.activation-api-1.2.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.annotation-api-1.3.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.inject-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.validation-api-2.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.xml.bind-api-2.3.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javassist-3.29.2-GA.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.activation-api-1.2.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.annotation-api-1.3.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.servlet-api-3.1.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.ws.rs-api-2.1.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jaxb-api-2.3.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-client-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-common-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-container-servlet-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-container-servlet-core-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-hk2-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-server-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-client-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-continuation-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-http-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-io-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-security-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-server-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-servlet-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-servlets-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-util-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-util-ajax-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jline-3.25.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jopt-simple-5.0.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jose4j-0.9.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jsr305-3.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-clients-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-group-coordinator-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-group-coordinator-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-metadata-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-raft-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-server-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-server-common-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-shell-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-storage-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-storage-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-examples-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-scala_2.13-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-test-utils-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-tools-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-tools-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-transaction-coordinator-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka_2.13-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/lz4-java-1.8.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/maven-artifact-3.9.6.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/metrics-core-2.2.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/metrics-core-4.1.12.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-buffer-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-codec-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-common-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-handler-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-resolver-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-classes-epoll-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-native-epoll-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-native-unix-common-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/opentelemetry-proto-1.0.0-alpha.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/osgi-resource-locator-1.0.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/paranamer-2.8.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/pcollections-4.0.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/plexus-utils-3.5.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/protobuf-java-3.25.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/reflections-0.10.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/reload4j-1.2.25.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/rocksdbjni-7.9.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-collection-compat_2.13-2.10.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-java8-compat_2.13-1.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-library-2.13.14.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-logging_2.13-3.9.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-reflect-2.13.14.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/slf4j-api-1.7.36.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/slf4j-reload4j-1.7.36.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/snappy-java-1.1.10.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/swagger-annotations-2.2.8.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/trogdor-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zookeeper-3.8.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zookeeper-jute-3.8.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zstd-jni-1.5.6-4.jar
	os.spec = Mac OS X, aarch64, 15.0
	os.vcpus = 11
 (org.apache.kafka.connect.runtime.WorkerInfo:72)
[2025-01-03 11:05:17,651] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.AbstractConnectCli:120)
[2025-01-03 11:05:17,662] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 11:05:17,694] INFO Using up-to-date JsonConverter implementation (io.debezium.converters.CloudEventsConverter:120)
[2025-01-03 11:05:17,782] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 11:05:17,783] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1 (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 11:05:17,786] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 11:05:17,786] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 11:05:17,797] INFO Using up-to-date JsonConverter implementation (io.debezium.converters.CloudEventsConverter:120)
[2025-01-03 11:05:17,810] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 11:05:17,813] INFO Loading plugin from: classpath (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 11:05:17,816] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@5a07e868 (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 11:05:17,816] INFO Scanning plugins with ServiceLoaderScanner took 155 ms (org.apache.kafka.connect.runtime.isolation.PluginScanner:71)
[2025-01-03 11:05:17,817] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 11:05:17,955] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 11:05:17,955] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1 (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 11:05:18,015] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 11:05:18,015] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 11:05:18,176] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 11:05:18,177] INFO Loading plugin from: classpath (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 11:05:18,619] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@5a07e868 (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 11:05:18,620] INFO Scanning plugins with ReflectionScanner took 803 ms (org.apache.kafka.connect.runtime.isolation.PluginScanner:71)
[2025-01-03 11:05:18,621] WARN One or more plugins are missing ServiceLoader manifests may not be usable with plugin.discovery=service_load: [
file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1/	com.mongodb.kafka.connect.MongoSinkConnector	sink	1.14.1
file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1/	com.mongodb.kafka.connect.MongoSourceConnector	source	1.14.1
file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/	io.debezium.connector.mongodb.MongoDbSinkConnector	sink	3.0.5.Final
]
Read the documentation at https://kafka.apache.org/documentation.html#connect_plugindiscovery for instructions on migrating your plugins to take advantage of the performance improvements of service_load mode. To silence this warning, set plugin.discovery=only_scan in the worker config. (org.apache.kafka.connect.runtime.isolation.Plugins:123)
[2025-01-03 11:05:18,622] INFO Added plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,622] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,622] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,622] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,622] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,622] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,622] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,622] INFO Added plugin 'io.debezium.transforms.HeaderToValue' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,622] INFO Added plugin 'io.debezium.transforms.SchemaChangeEventFilter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,622] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,622] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,622] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,622] INFO Added plugin 'io.debezium.transforms.ExtractSchemaToNewRecord' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,622] INFO Added plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,622] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,622] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,622] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,622] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,622] INFO Added plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,622] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,622] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,622] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,623] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,623] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,623] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,623] INFO Added plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,623] INFO Added plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,623] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,623] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,623] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,623] INFO Added plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,623] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,623] INFO Added plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,623] INFO Added plugin 'io.debezium.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,623] INFO Added plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,623] INFO Added plugin 'io.debezium.transforms.partitions.PartitionRouting' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,623] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,623] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,623] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,623] INFO Added plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,623] INFO Added plugin 'io.debezium.transforms.TimezoneConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,623] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,623] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,623] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,623] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,623] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,623] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,623] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,623] INFO Added plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,624] INFO Added plugin 'io.debezium.transforms.ExtractChangedRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,624] INFO Added plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,624] INFO Added plugin 'io.debezium.connector.mongodb.transforms.outbox.MongoEventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,624] INFO Added plugin 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,624] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,624] INFO Added plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,624] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,624] INFO Added plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,624] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,624] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,624] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,624] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,624] INFO Added plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,624] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,624] INFO Added plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,624] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,624] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,624] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,624] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,624] INFO Added plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,624] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,624] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,624] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:05:18,625] INFO Added alias 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,625] INFO Added alias 'ExtractNewDocumentState' to plugin 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,625] INFO Added alias 'JsonConverter' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,625] INFO Added alias 'DebeziumMySql' to plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,625] INFO Added alias 'MongoSourceConnector' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,625] INFO Added alias 'CloudEventsConverter' to plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,625] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,625] INFO Added alias 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,625] INFO Added alias 'DebeziumMySqlConnectRestExtension' to plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,625] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,625] INFO Added alias 'MySql' to plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,625] INFO Added alias 'MongoDbSinkConnector' to plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,625] INFO Added alias 'EnvVar' to plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,625] INFO Added alias 'EnvVarConfigProvider' to plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,625] INFO Added alias 'MongoDb' to plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,625] INFO Added alias 'MirrorCheckpointConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,625] INFO Added alias 'Boolean' to plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,625] INFO Added alias 'MySqlConnector' to plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,625] INFO Added alias 'NoneConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,625] INFO Added alias 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,625] INFO Added alias 'HeaderToValue' to plugin 'io.debezium.transforms.HeaderToValue' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,625] INFO Added alias 'PartitionRouting' to plugin 'io.debezium.transforms.partitions.PartitionRouting' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'MongoDbSink' to plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'StringConverter' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'ByLogicalTableRouter' to plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'MongoDbConnector' to plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'FileConfigProvider' to plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'IntegerConverter' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'EventRouter' to plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'SchemaChangeEventFilter' to plugin 'io.debezium.transforms.SchemaChangeEventFilter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'LongConverter' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'DropHeaders' to plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'ExtractSchemaToNewRecord' to plugin 'io.debezium.transforms.ExtractSchemaToNewRecord' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'BinaryData' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'File' to plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'FloatConverter' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'DirectoryConfigProvider' to plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'ReadToInsertEvent' to plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'ShortConverter' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'CloudEvents' to plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'ActivateTracingSpan' to plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'TimezoneConverter' to plugin 'io.debezium.transforms.TimezoneConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'DoubleConverter' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'Simple' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'BinaryDataConverter' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'AllConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'ExtractNewRecordState' to plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'DebeziumMongoDb' to plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,626] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,627] INFO Added alias 'MirrorHeartbeatConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,627] INFO Added alias 'InsertHeader' to plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,627] INFO Added alias 'Directory' to plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,627] INFO Added alias 'MirrorSourceConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,627] INFO Added alias 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,627] INFO Added alias 'MongoSinkConnector' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,627] INFO Added alias 'PrincipalConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,627] INFO Added alias 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,627] INFO Added alias 'DebeziumMongoDbConnectRestExtension' to plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,627] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,627] INFO Added alias 'BooleanConverter' to plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,627] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,627] INFO Added alias 'MongoEventRouter' to plugin 'io.debezium.connector.mongodb.transforms.outbox.MongoEventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,627] INFO Added alias 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,627] INFO Added alias 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,627] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,627] INFO Added alias 'ExtractChangedRecordState' to plugin 'io.debezium.transforms.ExtractChangedRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,627] INFO Added alias 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,627] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,627] INFO Added alias 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:05:18,645] INFO DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	config.providers = []
	config.storage.replication.factor = 1
	config.storage.topic = connect-configs
	connect.protocol = sessioned
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = All
	exactly.once.source.support = disabled
	group.id = connect-cluster
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	heartbeat.interval.ms = 3000
	inter.worker.key.generation.algorithm = HmacSHA256
	inter.worker.key.size = null
	inter.worker.key.ttl.ms = 3600000
	inter.worker.signature.algorithm = HmacSHA256
	inter.worker.verification.algorithms = [HmacSHA256]
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = [http://:8083]
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 1
	offset.storage.topic = connect-offsets
	plugin.discovery = hybrid_warn
	plugin.path = [/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins]
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	status.storage.partitions = 5
	status.storage.replication.factor = 1
	status.storage.topic = connect-status
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000
 (org.apache.kafka.connect.runtime.distributed.DistributedConfig:371)
[2025-01-03 11:05:18,646] INFO Creating Kafka admin client (org.apache.kafka.connect.runtime.WorkerConfig:281)
[2025-01-03 11:05:18,647] INFO AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:371)
[2025-01-03 11:05:18,667] INFO These configurations '[config.storage.topic, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2025-01-03 11:05:18,667] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:05:18,667] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:05:18,667] INFO Kafka startTimeMs: 1735923918667 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:05:18,776] INFO Kafka cluster ID: l71C-upmSGSQL-StD83PcA (org.apache.kafka.connect.runtime.WorkerConfig:298)
[2025-01-03 11:05:18,776] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:05:18,778] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:05:18,778] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:05:18,778] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:05:18,780] INFO PublicConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	listeners = [http://:8083]
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
 (org.apache.kafka.connect.runtime.rest.RestServerConfig$PublicConfig:371)
[2025-01-03 11:05:18,784] INFO Logging initialized @1360ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:170)
[2025-01-03 11:05:18,796] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:125)
[2025-01-03 11:05:18,796] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:196)
[2025-01-03 11:05:18,805] INFO jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 23+37-2369 (org.eclipse.jetty.server.Server:375)
[2025-01-03 11:05:18,813] INFO Started http_8083@66b31d46{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:333)
[2025-01-03 11:05:18,813] INFO Started @1390ms (org.eclipse.jetty.server.Server:415)
[2025-01-03 11:05:18,820] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 11:05:18,820] INFO REST server listening at http://192.168.1.5:8083/, advertising URL http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:216)
[2025-01-03 11:05:18,820] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 11:05:18,820] INFO REST admin endpoints at http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2025-01-03 11:05:18,820] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 11:05:18,820] INFO Setting up All Policy for ConnectorClientConfigOverride. This will allow all client configurations to be overridden (org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy:45)
[2025-01-03 11:05:18,822] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 11:05:18,829] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:05:18,829] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:05:18,829] INFO Kafka startTimeMs: 1735923918829 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:05:18,831] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 11:05:18,831] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 11:05:18,838] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 11:05:18,851] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:05:18,851] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:05:18,851] INFO Kafka startTimeMs: 1735923918851 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:05:18,852] INFO Kafka Connect worker initialization took 1203ms (org.apache.kafka.connect.cli.AbstractConnectCli:141)
[2025-01-03 11:05:18,853] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:67)
[2025-01-03 11:05:18,853] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2025-01-03 11:05:18,853] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder starting (org.apache.kafka.connect.runtime.distributed.DistributedHerder:375)
[2025-01-03 11:05:18,854] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:233)
[2025-01-03 11:05:18,854] INFO Starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:232)
[2025-01-03 11:05:18,854] INFO Starting KafkaBasedLog with topic connect-offsets reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog:254)
[2025-01-03 11:05:18,854] INFO AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-shared-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:371)
[2025-01-03 11:05:18,855] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2025-01-03 11:05:18,855] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:05:18,855] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:05:18,855] INFO Kafka startTimeMs: 1735923918855 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:05:18,865] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:238)
[2025-01-03 11:05:18,883] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:334)
[2025-01-03 11:05:18,884] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:339)
[2025-01-03 11:05:18,884] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session:132)
[2025-01-03 11:05:19,039] INFO Started o.e.j.s.ServletContextHandler@2c15034f{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:921)
[2025-01-03 11:05:19,041] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:303)
[2025-01-03 11:05:19,041] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:77)
[2025-01-03 11:05:19,199] INFO Created topic (name=connect-offsets, numPartitions=25, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:9092 (org.apache.kafka.connect.util.TopicAdmin:416)
[2025-01-03 11:05:19,202] INFO ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-offsets
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 11:05:19,209] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:05:19,216] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 11:05:19,216] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:05:19,216] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:05:19,216] INFO Kafka startTimeMs: 1735923919216 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:05:19,219] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-offsets
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 11:05:19,220] INFO [Producer clientId=connect-cluster-offsets] Cluster ID: l71C-upmSGSQL-StD83PcA (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:05:19,223] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:05:19,232] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2025-01-03 11:05:19,232] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:05:19,232] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:05:19,232] INFO Kafka startTimeMs: 1735923919232 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:05:19,235] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Cluster ID: l71C-upmSGSQL-StD83PcA (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:05:19,238] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Assigned to partition(s): connect-offsets-0, connect-offsets-5, connect-offsets-10, connect-offsets-20, connect-offsets-15, connect-offsets-9, connect-offsets-11, connect-offsets-4, connect-offsets-16, connect-offsets-17, connect-offsets-3, connect-offsets-24, connect-offsets-23, connect-offsets-13, connect-offsets-18, connect-offsets-22, connect-offsets-8, connect-offsets-2, connect-offsets-12, connect-offsets-19, connect-offsets-14, connect-offsets-1, connect-offsets-6, connect-offsets-7, connect-offsets-21 (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:579)
[2025-01-03 11:05:19,239] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:05:19,239] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-5 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:05:19,239] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-10 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:05:19,239] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-20 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:05:19,239] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-15 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:05:19,239] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-9 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:05:19,239] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-11 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:05:19,239] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:05:19,239] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-16 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:05:19,239] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-17 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:05:19,239] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:05:19,239] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-24 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:05:19,240] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-23 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:05:19,240] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-13 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:05:19,240] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-18 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:05:19,240] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-22 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:05:19,240] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-8 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:05:19,240] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:05:19,240] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-12 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:05:19,240] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-19 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:05:19,240] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-14 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:05:19,240] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:05:19,240] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-6 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:05:19,240] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-7 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:05:19,240] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-21 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:05:19,257] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:05:19,257] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:05:19,257] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:05:19,257] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:05:19,257] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:05:19,257] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:05:19,257] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:05:19,257] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:05:19,257] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:05:19,257] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:05:19,257] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:05:19,257] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:05:19,257] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:05:19,257] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:05:19,257] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:05:19,257] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:05:19,257] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:05:19,257] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:05:19,257] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:05:19,258] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:05:19,258] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:05:19,258] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:05:19,258] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:05:19,258] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:05:19,258] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:05:19,258] INFO Finished reading KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:311)
[2025-01-03 11:05:19,258] INFO Started KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:313)
[2025-01-03 11:05:19,258] INFO Finished reading offsets topic and starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:249)
[2025-01-03 11:05:19,258] INFO Worker started (org.apache.kafka.connect.runtime.Worker:243)
[2025-01-03 11:05:19,258] INFO Starting KafkaBasedLog with topic connect-status reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog:254)
[2025-01-03 11:05:19,329] INFO Created topic (name=connect-status, numPartitions=5, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:9092 (org.apache.kafka.connect.util.TopicAdmin:416)
[2025-01-03 11:05:19,330] INFO ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-statuses
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 11:05:19,330] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:05:19,331] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 11:05:19,331] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:05:19,331] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:05:19,331] INFO Kafka startTimeMs: 1735923919331 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:05:19,332] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-statuses
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 11:05:19,332] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:05:19,333] INFO [Producer clientId=connect-cluster-statuses] Cluster ID: l71C-upmSGSQL-StD83PcA (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:05:19,334] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2025-01-03 11:05:19,334] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:05:19,334] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:05:19,334] INFO Kafka startTimeMs: 1735923919334 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:05:19,337] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Cluster ID: l71C-upmSGSQL-StD83PcA (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:05:19,337] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Assigned to partition(s): connect-status-0, connect-status-4, connect-status-1, connect-status-2, connect-status-3 (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:579)
[2025-01-03 11:05:19,337] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:05:19,337] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:05:19,337] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:05:19,337] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:05:19,338] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:05:19,343] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:05:19,343] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:05:19,343] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:05:19,343] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:05:19,343] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:05:19,343] INFO Finished reading KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:311)
[2025-01-03 11:05:19,343] INFO Started KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:313)
[2025-01-03 11:05:19,345] INFO Starting KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:378)
[2025-01-03 11:05:19,345] INFO Starting KafkaBasedLog with topic connect-configs reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog:254)
[2025-01-03 11:05:19,376] INFO Created topic (name=connect-configs, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:9092 (org.apache.kafka.connect.util.TopicAdmin:416)
[2025-01-03 11:05:19,376] INFO ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-configs
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 11:05:19,376] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:05:19,378] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 11:05:19,378] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:05:19,378] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:05:19,378] INFO Kafka startTimeMs: 1735923919378 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:05:19,378] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-configs
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 11:05:19,378] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:05:19,379] INFO [Producer clientId=connect-cluster-configs] Cluster ID: l71C-upmSGSQL-StD83PcA (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:05:19,380] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2025-01-03 11:05:19,380] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:05:19,380] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:05:19,380] INFO Kafka startTimeMs: 1735923919380 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:05:19,381] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Cluster ID: l71C-upmSGSQL-StD83PcA (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:05:19,382] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Assigned to partition(s): connect-configs-0 (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:579)
[2025-01-03 11:05:19,382] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Seeking to earliest offset of partition connect-configs-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:05:19,392] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Resetting offset for partition connect-configs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:05:19,392] INFO Finished reading KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:311)
[2025-01-03 11:05:19,392] INFO Started KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:313)
[2025-01-03 11:05:19,392] INFO Started KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:402)
[2025-01-03 11:05:19,395] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Cluster ID: l71C-upmSGSQL-StD83PcA (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:05:20,111] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Discovered group coordinator 192.168.1.5:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:937)
[2025-01-03 11:05:20,114] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2025-01-03 11:05:20,115] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 11:05:20,135] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 11:05:20,142] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=1, memberId='connect-192.168.1.5:8083-9840b094-d2a8-462f-a9d3-22e16815ccaa', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2025-01-03 11:05:20,170] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=1, memberId='connect-192.168.1.5:8083-9840b094-d2a8-462f-a9d3-22e16815ccaa', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2025-01-03 11:05:20,171] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Joined group at generation 1 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.1.5:8083-9840b094-d2a8-462f-a9d3-22e16815ccaa', leaderUrl='http://192.168.1.5:8083/', offset=-1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2025-01-03 11:05:20,171] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder started (org.apache.kafka.connect.runtime.distributed.DistributedHerder:387)
[2025-01-03 11:05:20,171] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connectors and tasks using config offset -1 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2025-01-03 11:05:20,171] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2025-01-03 11:05:20,209] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Session key updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2510)
[2025-01-03 11:05:45,755] INFO MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = ${topic}
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = test
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 11:05:45,791] INFO MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync", "version": "4.7.2"}, "os": {"type": "Darwin", "name": "Mac OS X", "architecture": "aarch64", "version": "15.0"}, "platform": "Java/Oracle Corporation/23+37-2369", "application": {"name": "IST-Shared"}} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=majority, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=MongoCredential{mechanism=null, userName='leafy_factory', source='admin', password=<hidden>, mechanismProperties=<hidden>}, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@19aff91f]}, clusterSettings={hosts=[127.0.0.1:27017], srvHost=ist-shared.n0kts.mongodb.net, srvServiceName=mongodb, mode=MULTIPLE, requiredClusterType=REPLICA_SET, requiredReplicaSetName='atlas-133bp7-shard-0', serverSelector='null', clusterListeners='[com.mongodb.kafka.connect.util.ConnectionValidator$1@50a2f187]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=true, invalidHostNameAllowed=false, context=null}, applicationName='IST-Shared', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null} (org.mongodb.driver.client:71)
[2025-01-03 11:05:45,956] INFO Adding discovered server ist-shared-shard-00-01.n0kts.mongodb.net:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2025-01-03 11:05:45,972] INFO Adding discovered server ist-shared-shard-00-02.n0kts.mongodb.net:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2025-01-03 11:05:45,973] INFO Adding discovered server ist-shared-shard-00-00.n0kts.mongodb.net:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2025-01-03 11:05:47,093] INFO Opened connection [connectionId{localValue:2, serverValue:1158500}] to ist-shared-shard-00-00.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:05:47,093] INFO Opened connection [connectionId{localValue:1, serverValue:1158499}] to ist-shared-shard-00-00.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:05:47,093] INFO Opened connection [connectionId{localValue:6, serverValue:699209}] to ist-shared-shard-00-02.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:05:47,100] INFO Monitor thread successfully connected to server with description ServerDescription{address=ist-shared-shard-00-00.n0kts.mongodb.net:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=501530875, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-00.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az1'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=null, setVersion=71, topologyVersion=TopologyVersion{processId=675b24d1d1431a587be694a8, counter=19}, lastWriteDate=Fri Jan 03 11:05:46 CST 2025, lastUpdateTimeNanos=46997591444541} (org.mongodb.driver.cluster:71)
[2025-01-03 11:05:47,107] INFO Opened connection [connectionId{localValue:3, serverValue:699208}] to ist-shared-shard-00-02.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:05:47,107] INFO Monitor thread successfully connected to server with description ServerDescription{address=ist-shared-shard-00-02.n0kts.mongodb.net:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=499610625, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-02.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az4'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=null, setVersion=71, topologyVersion=TopologyVersion{processId=67694a2575e55e11df78dcd1, counter=14}, lastWriteDate=Fri Jan 03 11:05:46 CST 2025, lastUpdateTimeNanos=46997606293708} (org.mongodb.driver.cluster:71)
[2025-01-03 11:05:47,117] INFO No server chosen by ReadPreferenceServerSelector{readPreference=primary} from cluster description ClusterDescription{type=REPLICA_SET, connectionMode=MULTIPLE, serverDescriptions=[ServerDescription{address=ist-shared-shard-00-02.n0kts.mongodb.net:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=499610625, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-02.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az4'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=null, setVersion=71, topologyVersion=TopologyVersion{processId=67694a2575e55e11df78dcd1, counter=14}, lastWriteDate=Fri Jan 03 11:05:46 CST 2025, lastUpdateTimeNanos=46997606293708}, ServerDescription{address=ist-shared-shard-00-01.n0kts.mongodb.net:27017, type=UNKNOWN, state=CONNECTING}, ServerDescription{address=ist-shared-shard-00-00.n0kts.mongodb.net:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=501530875, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-00.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az1'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=null, setVersion=71, topologyVersion=TopologyVersion{processId=675b24d1d1431a587be694a8, counter=19}, lastWriteDate=Fri Jan 03 11:05:46 CST 2025, lastUpdateTimeNanos=46997591444541}]}. Waiting for 30000 ms before timing out (org.mongodb.driver.cluster:71)
[2025-01-03 11:05:47,122] INFO Opened connection [connectionId{localValue:5, serverValue:1294660}] to ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:05:47,122] INFO Opened connection [connectionId{localValue:4, serverValue:1294661}] to ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:05:47,122] INFO Monitor thread successfully connected to server with description ServerDescription{address=ist-shared-shard-00-01.n0kts.mongodb.net:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=519811750, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-01.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az2'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=7fffffff000000000000010a, setVersion=71, topologyVersion=TopologyVersion{processId=67694b4e6118cd58a4791747, counter=41}, lastWriteDate=Fri Jan 03 11:05:46 CST 2025, lastUpdateTimeNanos=46997621497958} (org.mongodb.driver.cluster:71)
[2025-01-03 11:05:47,123] INFO Setting max election id to 7fffffff000000000000010a from replica set primary ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.cluster:71)
[2025-01-03 11:05:47,123] INFO Setting max set version to 71 from replica set primary ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.cluster:71)
[2025-01-03 11:05:47,123] INFO Discovered replica set primary ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.cluster:71)
[2025-01-03 11:05:47,786] INFO Opened connection [connectionId{localValue:7, serverValue:1294662}] to ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:05:47,900] INFO MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = ${topic}
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = __default
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 11:05:48,015] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:371)
[2025-01-03 11:05:48,030] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Connector mongodb-sink config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2025-01-03 11:05:48,031] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2025-01-03 11:05:48,031] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 11:05:48,035] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=2, memberId='connect-192.168.1.5:8083-9840b094-d2a8-462f-a9d3-22e16815ccaa', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2025-01-03 11:05:48,040] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=2, memberId='connect-192.168.1.5:8083-9840b094-d2a8-462f-a9d3-22e16815ccaa', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2025-01-03 11:05:48,040] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Joined group at generation 2 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.1.5:8083-9840b094-d2a8-462f-a9d3-22e16815ccaa', leaderUrl='http://192.168.1.5:8083/', offset=2, connectorIds=[mongodb-sink], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2025-01-03 11:05:48,040] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 2 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2025-01-03 11:05:48,041] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connector mongodb-sink (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2025-01-03 11:05:48,043] INFO [mongodb-sink|worker] Creating connector mongodb-sink of type com.mongodb.kafka.connect.MongoSinkConnector (org.apache.kafka.connect.runtime.Worker:313)
[2025-01-03 11:05:48,043] INFO [mongodb-sink|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = true
	errors.log.include.messages = true
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = all
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongodb-sink
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = []
	topics.regex = db_\.leafy_factory\..*
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2025-01-03 11:05:48,043] INFO [mongodb-sink|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = true
	errors.log.include.messages = true
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = all
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongodb-sink
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = []
	topics.regex = db_\.leafy_factory\..*
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 11:05:48,045] INFO [mongodb-sink|worker] Instantiated connector mongodb-sink with version 1.14.1 of type class com.mongodb.kafka.connect.MongoSinkConnector (org.apache.kafka.connect.runtime.Worker:335)
[2025-01-03 11:05:48,046] INFO [mongodb-sink|worker] Finished creating connector mongodb-sink (org.apache.kafka.connect.runtime.Worker:356)
[2025-01-03 11:05:48,046] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2025-01-03 11:05:48,050] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = true
	errors.log.include.messages = true
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = all
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongodb-sink
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = []
	topics.regex = db_\.leafy_factory\..*
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2025-01-03 11:05:48,050] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = true
	errors.log.include.messages = true
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = all
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongodb-sink
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = []
	topics.regex = db_\.leafy_factory\..*
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 11:05:48,054] INFO [0:0:0:0:0:0:0:1] - - [03/Jan/2025:17:05:45 +0000] "POST /connectors HTTP/1.1" 201 757 "-" "curl/8.7.1" 2609 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-01-03 11:05:48,061] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Tasks [mongodb-sink-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2025-01-03 11:05:48,062] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2025-01-03 11:05:48,062] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 11:05:48,064] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=3, memberId='connect-192.168.1.5:8083-9840b094-d2a8-462f-a9d3-22e16815ccaa', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2025-01-03 11:05:48,067] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=3, memberId='connect-192.168.1.5:8083-9840b094-d2a8-462f-a9d3-22e16815ccaa', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2025-01-03 11:05:48,068] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Joined group at generation 3 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.1.5:8083-9840b094-d2a8-462f-a9d3-22e16815ccaa', leaderUrl='http://192.168.1.5:8083/', offset=4, connectorIds=[mongodb-sink], taskIds=[mongodb-sink-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2025-01-03 11:05:48,068] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 4 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2025-01-03 11:05:48,069] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting task mongodb-sink-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2025-01-03 11:05:48,070] INFO [mongodb-sink|task-0] Creating task mongodb-sink-0 (org.apache.kafka.connect.runtime.Worker:646)
[2025-01-03 11:05:48,071] INFO [mongodb-sink|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = true
	errors.log.include.messages = true
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = all
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongodb-sink
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2025-01-03 11:05:48,071] INFO [mongodb-sink|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = true
	errors.log.include.messages = true
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = all
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongodb-sink
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 11:05:48,072] INFO [mongodb-sink|task-0] TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.sink.MongoSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2025-01-03 11:05:48,072] INFO [mongodb-sink|task-0] Instantiated task mongodb-sink-0 with version 1.14.1 of type com.mongodb.kafka.connect.sink.MongoSinkTask (org.apache.kafka.connect.runtime.Worker:665)
[2025-01-03 11:05:48,072] INFO [mongodb-sink|task-0] StringConverterConfig values: 
	converter.encoding = UTF-8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:371)
[2025-01-03 11:05:48,072] INFO [mongodb-sink|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 11:05:48,072] INFO [mongodb-sink|task-0] Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task mongodb-sink-0 using the connector config (org.apache.kafka.connect.runtime.Worker:680)
[2025-01-03 11:05:48,072] INFO [mongodb-sink|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongodb-sink-0 using the connector config (org.apache.kafka.connect.runtime.Worker:686)
[2025-01-03 11:05:48,073] INFO [mongodb-sink|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongodb-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2025-01-03 11:05:48,075] INFO [mongodb-sink|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:1795)
[2025-01-03 11:05:48,075] INFO [mongodb-sink|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = true
	errors.log.include.messages = true
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = all
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongodb-sink
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = []
	topics.regex = db_\.leafy_factory\..*
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2025-01-03 11:05:48,075] INFO [mongodb-sink|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = true
	errors.log.include.messages = true
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = all
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongodb-sink
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = []
	topics.regex = db_\.leafy_factory\..*
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 11:05:48,076] INFO [mongodb-sink|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mongodb-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mongodb-sink
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 11:05:48,077] INFO [mongodb-sink|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:05:48,082] INFO [mongodb-sink|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2025-01-03 11:05:48,082] INFO [mongodb-sink|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:05:48,082] INFO [mongodb-sink|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:05:48,082] INFO [mongodb-sink|task-0] Kafka startTimeMs: 1735923948082 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:05:48,086] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Subscribed to pattern: 'db_\.leafy_factory\..*' (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:534)
[2025-01-03 11:05:48,086] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2025-01-03 11:05:48,086] INFO [mongodb-sink|task-0] Starting MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:66)
[2025-01-03 11:05:48,107] INFO [mongodb-sink|task-0] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|mongo-kafka|sink", "version": "4.7.2|1.14.1"}, "os": {"type": "Darwin", "name": "Mac OS X", "architecture": "aarch64", "version": "15.0"}, "platform": "Java/Oracle Corporation/23+37-2369", "application": {"name": "IST-Shared"}} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=majority, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=MongoCredential{mechanism=null, userName='leafy_factory', source='admin', password=<hidden>, mechanismProperties=<hidden>}, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@19aff91f]}, clusterSettings={hosts=[127.0.0.1:27017], srvHost=ist-shared.n0kts.mongodb.net, srvServiceName=mongodb, mode=MULTIPLE, requiredClusterType=REPLICA_SET, requiredReplicaSetName='atlas-133bp7-shard-0', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=true, invalidHostNameAllowed=false, context=null}, applicationName='IST-Shared', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null} (org.mongodb.driver.client:71)
[2025-01-03 11:05:48,111] INFO [mongodb-sink|task-0] WorkerSinkTask{id=mongodb-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:325)
[2025-01-03 11:05:48,112] INFO [mongodb-sink|task-0] Adding discovered server ist-shared-shard-00-01.n0kts.mongodb.net:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2025-01-03 11:05:48,112] INFO [mongodb-sink|task-0] WorkerSinkTask{id=mongodb-sink-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:211)
[2025-01-03 11:05:48,113] INFO [mongodb-sink|task-0] Adding discovered server ist-shared-shard-00-02.n0kts.mongodb.net:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2025-01-03 11:05:48,114] INFO [mongodb-sink|task-0] Adding discovered server ist-shared-shard-00-00.n0kts.mongodb.net:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2025-01-03 11:05:48,116] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Cluster ID: l71C-upmSGSQL-StD83PcA (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:05:48,117] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Discovered group coordinator 192.168.1.5:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:937)
[2025-01-03 11:05:48,117] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:605)
[2025-01-03 11:05:48,122] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Request joining group due to: need to re-join with the given member-id: connector-consumer-mongodb-sink-0-521fa3b0-7973-4197-a1af-b6762725b2f8 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 11:05:48,122] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:605)
[2025-01-03 11:05:48,125] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Successfully joined group with generation Generation{generationId=1, memberId='connector-consumer-mongodb-sink-0-521fa3b0-7973-4197-a1af-b6762725b2f8', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:666)
[2025-01-03 11:05:48,126] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Finished assignment for group at generation 1: {connector-consumer-mongodb-sink-0-521fa3b0-7973-4197-a1af-b6762725b2f8=Assignment(partitions=[])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:664)
[2025-01-03 11:05:48,129] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Successfully synced group in generation Generation{generationId=1, memberId='connector-consumer-mongodb-sink-0-521fa3b0-7973-4197-a1af-b6762725b2f8', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:843)
[2025-01-03 11:05:48,129] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Notifying assignor about the new Assignment(partitions=[]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:324)
[2025-01-03 11:05:48,129] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Adding newly assigned partitions:  (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:58)
[2025-01-03 11:05:48,620] INFO [mongodb-sink|task-0] Opened connection [connectionId{localValue:11, serverValue:699211}] to ist-shared-shard-00-02.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:05:48,630] INFO [mongodb-sink|task-0] Opened connection [connectionId{localValue:9, serverValue:1294663}] to ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:05:48,630] INFO [mongodb-sink|task-0] Opened connection [connectionId{localValue:8, serverValue:1294664}] to ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:05:48,631] INFO [mongodb-sink|task-0] Monitor thread successfully connected to server with description ServerDescription{address=ist-shared-shard-00-01.n0kts.mongodb.net:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=317229834, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-01.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az2'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=7fffffff000000000000010a, setVersion=71, topologyVersion=TopologyVersion{processId=67694b4e6118cd58a4791747, counter=41}, lastWriteDate=Fri Jan 03 11:05:48 CST 2025, lastUpdateTimeNanos=46999129557833} (org.mongodb.driver.cluster:71)
[2025-01-03 11:05:48,631] INFO [mongodb-sink|task-0] Setting max election id to 7fffffff000000000000010a from replica set primary ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.cluster:71)
[2025-01-03 11:05:48,631] INFO [mongodb-sink|task-0] Setting max set version to 71 from replica set primary ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.cluster:71)
[2025-01-03 11:05:48,631] INFO [mongodb-sink|task-0] Discovered replica set primary ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.cluster:71)
[2025-01-03 11:05:48,709] INFO [mongodb-sink|task-0] Opened connection [connectionId{localValue:10, serverValue:699210}] to ist-shared-shard-00-02.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:05:48,710] INFO [mongodb-sink|task-0] Monitor thread successfully connected to server with description ServerDescription{address=ist-shared-shard-00-02.n0kts.mongodb.net:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=411046375, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-02.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az4'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=null, setVersion=71, topologyVersion=TopologyVersion{processId=67694a2575e55e11df78dcd1, counter=14}, lastWriteDate=Fri Jan 03 11:05:48 CST 2025, lastUpdateTimeNanos=46999208666916} (org.mongodb.driver.cluster:71)
[2025-01-03 11:05:48,717] INFO [mongodb-sink|task-0] Opened connection [connectionId{localValue:12, serverValue:1158501}] to ist-shared-shard-00-00.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:05:48,718] INFO [mongodb-sink|task-0] Monitor thread successfully connected to server with description ServerDescription{address=ist-shared-shard-00-00.n0kts.mongodb.net:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=418955042, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-00.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az1'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=null, setVersion=71, topologyVersion=TopologyVersion{processId=675b24d1d1431a587be694a8, counter=19}, lastWriteDate=Fri Jan 03 11:05:48 CST 2025, lastUpdateTimeNanos=46999216577208} (org.mongodb.driver.cluster:71)
[2025-01-03 11:05:48,733] INFO [mongodb-sink|task-0] Opened connection [connectionId{localValue:13, serverValue:1158502}] to ist-shared-shard-00-00.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:06:56,798] INFO Loading the custom source info struct maker plugin: io.debezium.connector.mysql.MySqlSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2025-01-03 11:06:57,193] INFO Using 'SHOW MASTER STATUS' to get binary log status (io.debezium.connector.mysql.jdbc.MySqlConnection:41)
[2025-01-03 11:06:57,196] INFO Successfully tested connection for jdbc:mysql://localhost:3306/?useInformationSchema=true&nullCatalogMeansCurrent=false&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8&zeroDateTimeBehavior=CONVERT_TO_NULL&connectTimeout=30000 with user 'root' (io.debezium.connector.binlog.BinlogConnector:66)
[2025-01-03 11:06:57,197] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection:983)
[2025-01-03 11:06:57,197] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:371)
[2025-01-03 11:06:57,200] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Connector mariadb-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2025-01-03 11:06:57,201] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2025-01-03 11:06:57,201] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 11:06:57,201] INFO [0:0:0:0:0:0:0:1] - - [03/Jan/2025:17:06:56 +0000] "POST /connectors HTTP/1.1" 201 681 "-" "curl/8.7.1" 428 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-01-03 11:06:57,202] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=4, memberId='connect-192.168.1.5:8083-9840b094-d2a8-462f-a9d3-22e16815ccaa', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2025-01-03 11:06:57,207] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=4, memberId='connect-192.168.1.5:8083-9840b094-d2a8-462f-a9d3-22e16815ccaa', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2025-01-03 11:06:57,207] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Joined group at generation 4 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.1.5:8083-9840b094-d2a8-462f-a9d3-22e16815ccaa', leaderUrl='http://192.168.1.5:8083/', offset=5, connectorIds=[mariadb-connector, mongodb-sink], taskIds=[mongodb-sink-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2025-01-03 11:06:57,207] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 5 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2025-01-03 11:06:57,207] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connector mariadb-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2025-01-03 11:06:57,207] INFO [mariadb-connector|worker] Creating connector mariadb-connector of type io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:313)
[2025-01-03 11:06:57,207] INFO [mariadb-connector|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2025-01-03 11:06:57,207] INFO [mariadb-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 11:06:57,209] INFO [mariadb-connector|worker] Instantiated connector mariadb-connector with version 3.0.5.Final of type class io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:335)
[2025-01-03 11:06:57,209] INFO [mariadb-connector|worker] Finished creating connector mariadb-connector (org.apache.kafka.connect.runtime.Worker:356)
[2025-01-03 11:06:57,209] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2025-01-03 11:06:57,209] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2025-01-03 11:06:57,209] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 11:06:57,216] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Tasks [mariadb-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2025-01-03 11:06:57,216] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2025-01-03 11:06:57,216] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 11:06:57,217] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=5, memberId='connect-192.168.1.5:8083-9840b094-d2a8-462f-a9d3-22e16815ccaa', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2025-01-03 11:06:57,218] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=5, memberId='connect-192.168.1.5:8083-9840b094-d2a8-462f-a9d3-22e16815ccaa', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2025-01-03 11:06:57,218] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Joined group at generation 5 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.1.5:8083-9840b094-d2a8-462f-a9d3-22e16815ccaa', leaderUrl='http://192.168.1.5:8083/', offset=7, connectorIds=[mariadb-connector, mongodb-sink], taskIds=[mariadb-connector-0, mongodb-sink-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2025-01-03 11:06:57,218] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 7 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2025-01-03 11:06:57,218] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting task mariadb-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2025-01-03 11:06:57,218] INFO [mariadb-connector|task-0] Creating task mariadb-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2025-01-03 11:06:57,219] INFO [mariadb-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mariadb-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2025-01-03 11:06:57,219] INFO [mariadb-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mariadb-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 11:06:57,219] INFO [mariadb-connector|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.mysql.MySqlConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2025-01-03 11:06:57,220] INFO [mariadb-connector|task-0] Instantiated task mariadb-connector-0 with version 3.0.5.Final of type io.debezium.connector.mysql.MySqlConnectorTask (org.apache.kafka.connect.runtime.Worker:665)
[2025-01-03 11:06:57,220] INFO [mariadb-connector|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 11:06:57,220] INFO [mariadb-connector|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mariadb-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:678)
[2025-01-03 11:06:57,220] INFO [mariadb-connector|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 11:06:57,220] INFO [mariadb-connector|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mariadb-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:684)
[2025-01-03 11:06:57,221] INFO [mariadb-connector|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mariadb-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2025-01-03 11:06:57,221] INFO [mariadb-connector|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:1795)
[2025-01-03 11:06:57,221] INFO [mariadb-connector|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2025-01-03 11:06:57,221] INFO [mariadb-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 11:06:57,221] INFO [mariadb-connector|task-0] ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-mariadb-connector-0
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 11:06:57,221] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:06:57,222] INFO [mariadb-connector|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 11:06:57,222] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:06:57,222] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:06:57,222] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735924017222 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:06:57,224] INFO [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] Cluster ID: l71C-upmSGSQL-StD83PcA (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:06:57,224] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2025-01-03 11:06:57,225] INFO [mariadb-connector|task-0] Starting MySqlConnectorTask with configuration:
   connector.class = io.debezium.connector.mysql.MySqlConnector
   database.user = root
   database.server.id = 184054
   database.history.kafka.bootstrap.servers = localhost:9092
   database.history.kafka.topic = db.history.leafy_factory
   database.server.name = leafy_factory
   schema.history.internal.kafka.bootstrap.servers = localhost:9092
   database.port = 3306
   include.schema.changes = false
   topic.prefix = db_
   schema.history.internal.kafka.topic = db.history.internal
   task.class = io.debezium.connector.mysql.MySqlConnectorTask
   database.hostname = localhost
   database.password = ********
   name = mariadb-connector
   database.include.list = leafy_factory
 (io.debezium.connector.common.BaseSourceTask:250)
[2025-01-03 11:06:57,225] INFO [mariadb-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mysql.MySqlSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2025-01-03 11:06:57,225] INFO [mariadb-connector|task-0] Loading the custom topic naming strategy plugin: io.debezium.schema.DefaultTopicNamingStrategy (io.debezium.config.CommonConnectorConfig:1401)
[2025-01-03 11:06:57,280] INFO [mariadb-connector|task-0] Using 'SHOW MASTER STATUS' to get binary log status (io.debezium.connector.mysql.jdbc.MySqlConnection:41)
[2025-01-03 11:06:57,284] INFO [mariadb-connector|task-0] No previous offsets found (io.debezium.connector.common.BaseSourceTask:536)
[2025-01-03 11:06:57,304] INFO [mariadb-connector|task-0] KafkaSchemaHistory Consumer config: {key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, enable.auto.commit=false, group.id=db_-schemahistory, bootstrap.servers=localhost:9092, fetch.min.bytes=1, session.timeout.ms=10000, auto.offset.reset=earliest, client.id=db_-schemahistory} (io.debezium.storage.kafka.history.KafkaSchemaHistory:245)
[2025-01-03 11:06:57,304] INFO [mariadb-connector|task-0] KafkaSchemaHistory Producer config: {retries=1, value.serializer=org.apache.kafka.common.serialization.StringSerializer, acks=1, batch.size=32768, max.block.ms=10000, bootstrap.servers=localhost:9092, buffer.memory=1048576, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=db_-schemahistory, linger.ms=0} (io.debezium.storage.kafka.history.KafkaSchemaHistory:246)
[2025-01-03 11:06:57,304] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = db-history-config-check (io.debezium.util.Threads:270)
[2025-01-03 11:06:57,305] INFO [mariadb-connector|task-0] Idempotence will be disabled because acks is set to 1, not set to 'all'. (org.apache.kafka.clients.producer.ProducerConfig:587)
[2025-01-03 11:06:57,305] INFO [mariadb-connector|task-0] ProducerConfig values: 
	acks = 1
	auto.include.jmx.reporter = true
	batch.size = 32768
	bootstrap.servers = [localhost:9092]
	buffer.memory = 1048576
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 11:06:57,306] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:06:57,307] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:06:57,307] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:06:57,307] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735924017307 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:06:57,308] INFO [mariadb-connector|task-0] [Producer clientId=db_-schemahistory] Cluster ID: l71C-upmSGSQL-StD83PcA (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:06:57,358] INFO [mariadb-connector|task-0] Using 'SHOW MASTER STATUS' to get binary log status (io.debezium.connector.mysql.jdbc.MySqlConnection:41)
[2025-01-03 11:06:57,365] INFO [mariadb-connector|task-0] Closing connection before starting schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:123)
[2025-01-03 11:06:57,365] INFO [mariadb-connector|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:983)
[2025-01-03 11:06:57,365] INFO [mariadb-connector|task-0] Connector started for the first time. (io.debezium.connector.common.BaseSourceTask:89)
[2025-01-03 11:06:57,366] INFO [mariadb-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = db_-schemahistory
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 11:06:57,366] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:06:57,371] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:06:57,371] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:06:57,371] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735924017370 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:06:57,372] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Cluster ID: l71C-upmSGSQL-StD83PcA (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:06:57,373] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 11:06:57,373] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 11:06:57,374] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:06:57,374] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:06:57,374] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:06:57,375] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:06:57,375] INFO [mariadb-connector|task-0] App info kafka.consumer for db_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:06:57,376] INFO [mariadb-connector|task-0] AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:371)
[2025-01-03 11:06:57,376] INFO [mariadb-connector|task-0] These configurations '[value.serializer, acks, batch.size, max.block.ms, buffer.memory, key.serializer, linger.ms]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2025-01-03 11:06:57,376] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:06:57,376] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:06:57,376] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735924017376 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:06:57,421] INFO [mariadb-connector|task-0] Database schema history topic '(name=db.history.internal, numPartitions=1, replicationFactor=default, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807, retention.bytes=-1})' created (io.debezium.storage.kafka.history.KafkaSchemaHistory:555)
[2025-01-03 11:06:57,422] INFO [mariadb-connector|task-0] App info kafka.admin.client for db_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:06:57,422] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:06:57,422] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:06:57,422] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:06:57,422] INFO [mariadb-connector|task-0] Reconnecting after finishing schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:136)
[2025-01-03 11:06:57,455] INFO [mariadb-connector|task-0] No previous offset found (io.debezium.connector.mysql.MySqlConnectorTask:147)
[2025-01-03 11:06:57,464] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = SignalProcessor (io.debezium.util.Threads:270)
[2025-01-03 11:06:57,470] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = change-event-source-coordinator (io.debezium.util.Threads:270)
[2025-01-03 11:06:57,471] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = blocking-snapshot (io.debezium.util.Threads:270)
[2025-01-03 11:06:57,471] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-change-event-source-coordinator (io.debezium.util.Threads:287)
[2025-01-03 11:06:57,471] INFO [mariadb-connector|task-0] WorkerSourceTask{id=mariadb-connector-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.AbstractWorkerSourceTask:280)
[2025-01-03 11:06:57,472] INFO [mariadb-connector|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:137)
[2025-01-03 11:06:57,472] INFO [mariadb-connector|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:140)
[2025-01-03 11:06:57,476] INFO [mariadb-connector|task-0] According to the connector configuration both schema and data will be snapshot. (io.debezium.relational.RelationalSnapshotChangeEventSource:282)
[2025-01-03 11:06:57,477] INFO [mariadb-connector|task-0] Snapshot step 1 - Preparing (io.debezium.relational.RelationalSnapshotChangeEventSource:135)
[2025-01-03 11:06:57,477] INFO [mariadb-connector|task-0] Snapshot step 2 - Determining captured tables (io.debezium.relational.RelationalSnapshotChangeEventSource:144)
[2025-01-03 11:06:57,477] INFO [mariadb-connector|task-0] Read list of available databases (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:116)
[2025-01-03 11:06:57,480] INFO [mariadb-connector|task-0] 	 list of available databases is: [information_schema, leafy_factory, mysql, performance_schema, sys, test] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:118)
[2025-01-03 11:06:57,480] INFO [mariadb-connector|task-0] Read list of available tables in each database (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:126)
[2025-01-03 11:06:57,527] INFO [mariadb-connector|task-0] 	snapshot continuing with database(s): [leafy_factory] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:147)
[2025-01-03 11:06:57,527] INFO [mariadb-connector|task-0] Adding table leafy_factory.jobs_machines to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 11:06:57,527] INFO [mariadb-connector|task-0] Adding table leafy_factory.product_cost to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 11:06:57,528] INFO [mariadb-connector|task-0] Adding table leafy_factory.production_lines to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 11:06:57,528] INFO [mariadb-connector|task-0] Adding table leafy_factory.products_raw_materials to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 11:06:57,528] INFO [mariadb-connector|task-0] Adding table leafy_factory.machines to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 11:06:57,528] INFO [mariadb-connector|task-0] Adding table leafy_factory.jobs to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 11:06:57,528] INFO [mariadb-connector|task-0] Adding table leafy_factory.raw_materials to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 11:06:57,528] INFO [mariadb-connector|task-0] Adding table leafy_factory.work_orders to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 11:06:57,528] INFO [mariadb-connector|task-0] Adding table leafy_factory.products to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 11:06:57,528] INFO [mariadb-connector|task-0] Adding table leafy_factory.factories to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 11:06:57,528] INFO [mariadb-connector|task-0] Created connection pool with 1 threads (io.debezium.relational.RelationalSnapshotChangeEventSource:236)
[2025-01-03 11:06:57,528] INFO [mariadb-connector|task-0] Snapshot step 3 - Locking captured tables [leafy_factory.factories, leafy_factory.jobs, leafy_factory.jobs_machines, leafy_factory.machines, leafy_factory.product_cost, leafy_factory.production_lines, leafy_factory.products, leafy_factory.products_raw_materials, leafy_factory.raw_materials, leafy_factory.work_orders] (io.debezium.relational.RelationalSnapshotChangeEventSource:153)
[2025-01-03 11:06:57,530] INFO [mariadb-connector|task-0] Flush and obtain global read lock to prevent writes to database (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:488)
[2025-01-03 11:06:57,531] INFO [mariadb-connector|task-0] Snapshot step 4 - Determining snapshot offset (io.debezium.relational.RelationalSnapshotChangeEventSource:159)
[2025-01-03 11:06:57,533] INFO [mariadb-connector|task-0] Read binlog position of MySQL primary server (io.debezium.connector.mysql.MySqlSnapshotChangeEventSource:58)
[2025-01-03 11:06:57,534] INFO [mariadb-connector|task-0] Snapshot step 5 - Reading structure of captured tables (io.debezium.relational.RelationalSnapshotChangeEventSource:162)
[2025-01-03 11:06:57,534] INFO [mariadb-connector|task-0] All eligible tables schema should be captured, capturing: [leafy_factory.factories, leafy_factory.jobs, leafy_factory.jobs_machines, leafy_factory.machines, leafy_factory.product_cost, leafy_factory.production_lines, leafy_factory.products, leafy_factory.products_raw_materials, leafy_factory.raw_materials, leafy_factory.work_orders] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:314)
[2025-01-03 11:06:58,076] INFO [mariadb-connector|task-0] Reading structure of database 'leafy_factory' (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:348)
[2025-01-03 11:06:58,141] INFO [mariadb-connector|task-0] Snapshot step 6 - Persisting schema history (io.debezium.relational.RelationalSnapshotChangeEventSource:166)
[2025-01-03 11:06:58,172] INFO [mariadb-connector|task-0] Releasing global read lock to enable MySQL writes (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:497)
[2025-01-03 11:06:58,172] INFO [mariadb-connector|task-0] Writes to MySQL tables prevented for a total of 00:00:00.641 (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:501)
[2025-01-03 11:06:58,172] INFO [mariadb-connector|task-0] Snapshot step 7 - Snapshotting data (io.debezium.relational.RelationalSnapshotChangeEventSource:178)
[2025-01-03 11:06:58,173] INFO [mariadb-connector|task-0] Creating snapshot worker pool with 1 worker thread(s) (io.debezium.relational.RelationalSnapshotChangeEventSource:480)
[2025-01-03 11:06:58,173] INFO [mariadb-connector|task-0] For table 'leafy_factory.factories' using select statement: 'SELECT `id_factory`, `factory_name`, `factory_location`, `factory_timestamp` FROM `leafy_factory`.`factories`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 11:06:58,175] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.factories is OptionalLong[0] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 11:06:58,175] INFO [mariadb-connector|task-0] For table 'leafy_factory.jobs' using select statement: 'SELECT `id_job`, `target_output`, `nOk_products`, `quality_rate`, `job_status`, `creation_date`, `work_id` FROM `leafy_factory`.`jobs`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 11:06:58,177] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.jobs is OptionalLong[0] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 11:06:58,177] INFO [mariadb-connector|task-0] For table 'leafy_factory.jobs_machines' using select statement: 'SELECT `id_jobs_machines`, `job_id`, `machine_id` FROM `leafy_factory`.`jobs_machines`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 11:06:58,178] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.jobs_machines is OptionalLong[2] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 11:06:58,178] INFO [mariadb-connector|task-0] For table 'leafy_factory.machines' using select statement: 'SELECT `id_machine`, `machine_status`, `last_maintenance`, `operator`, `avg_output`, `reject_count`, `production_line_id` FROM `leafy_factory`.`machines`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 11:06:58,180] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.machines is OptionalLong[4] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 11:06:58,180] INFO [mariadb-connector|task-0] For table 'leafy_factory.product_cost' using select statement: 'SELECT `id_cost`, `raw_material_cost_per_product`, `overhead_per_product`, `total_cost_per_product`, `cost_ok_with_overhead`, `cost_nok_with_overhead`, `actual_total_cost`, `work_id` FROM `leafy_factory`.`product_cost`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 11:06:58,181] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.product_cost is OptionalLong[12] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 11:06:58,181] INFO [mariadb-connector|task-0] For table 'leafy_factory.production_lines' using select statement: 'SELECT `id_production_line`, `factory_id` FROM `leafy_factory`.`production_lines`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 11:06:58,182] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.production_lines is OptionalLong[2] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 11:06:58,182] INFO [mariadb-connector|task-0] For table 'leafy_factory.products' using select statement: 'SELECT `id_product`, `product_name`, `product_description` FROM `leafy_factory`.`products`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 11:06:58,184] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.products is OptionalLong[2] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 11:06:58,184] INFO [mariadb-connector|task-0] For table 'leafy_factory.products_raw_materials' using select statement: 'SELECT `id_products_raw_materials`, `product_id`, `raw_material_id` FROM `leafy_factory`.`products_raw_materials`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 11:06:58,185] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.products_raw_materials is OptionalLong[9] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 11:06:58,185] INFO [mariadb-connector|task-0] For table 'leafy_factory.raw_materials' using select statement: 'SELECT `id_raw_material`, `item_code`, `raw_material_name`, `raw_material_description`, `unit_measurement`, `raw_material_stock`, `raw_material_status`, `raw_material_currency`, `cost_per_part` FROM `leafy_factory`.`raw_materials`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 11:06:58,186] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.raw_materials is OptionalLong[8] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 11:06:58,186] INFO [mariadb-connector|task-0] For table 'leafy_factory.work_orders' using select statement: 'SELECT `id_work`, `planned_start_date`, `planned_end_date`, `actual_start_date`, `actual_end_date`, `quantity`, `wo_status`, `creation_date`, `product_id`, `nOk_products` FROM `leafy_factory`.`work_orders`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 11:06:58,187] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.work_orders is OptionalLong[12] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 11:06:58,188] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.factories' (1 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 11:06:58,199] INFO [mariadb-connector|task-0] 	 Finished exporting 1 records for table 'leafy_factory.factories' (1 of 10 tables); total duration '00:00:00.011' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 11:06:58,200] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.jobs' (2 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 11:06:58,201] INFO [mariadb-connector|task-0] 	 Finished exporting 1 records for table 'leafy_factory.jobs' (2 of 10 tables); total duration '00:00:00.001' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 11:06:58,201] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.jobs_machines' (3 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 11:06:58,203] INFO [mariadb-connector|task-0] 	 Finished exporting 2 records for table 'leafy_factory.jobs_machines' (3 of 10 tables); total duration '00:00:00.002' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 11:06:58,203] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.machines' (4 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 11:06:58,204] INFO [mariadb-connector|task-0] 	 Finished exporting 4 records for table 'leafy_factory.machines' (4 of 10 tables); total duration '00:00:00.001' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 11:06:58,205] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.product_cost' (5 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 11:06:58,208] INFO [mariadb-connector|task-0] 	 Finished exporting 23 records for table 'leafy_factory.product_cost' (5 of 10 tables); total duration '00:00:00.003' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 11:06:58,208] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.production_lines' (6 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 11:06:58,209] INFO [mariadb-connector|task-0] 	 Finished exporting 2 records for table 'leafy_factory.production_lines' (6 of 10 tables); total duration '00:00:00.001' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 11:06:58,210] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.products' (7 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 11:06:58,210] INFO [mariadb-connector|task-0] 	 Finished exporting 2 records for table 'leafy_factory.products' (7 of 10 tables); total duration '00:00:00.0' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 11:06:58,211] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.products_raw_materials' (8 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 11:06:58,212] INFO [mariadb-connector|task-0] 	 Finished exporting 8 records for table 'leafy_factory.products_raw_materials' (8 of 10 tables); total duration '00:00:00.001' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 11:06:58,212] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.raw_materials' (9 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 11:06:58,214] INFO [mariadb-connector|task-0] 	 Finished exporting 8 records for table 'leafy_factory.raw_materials' (9 of 10 tables); total duration '00:00:00.002' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 11:06:58,214] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.work_orders' (10 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 11:06:58,220] INFO [mariadb-connector|task-0] 	 Finished exporting 23 records for table 'leafy_factory.work_orders' (10 of 10 tables); total duration '00:00:00.006' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 11:06:58,221] INFO [mariadb-connector|task-0] Snapshot - Final stage (io.debezium.pipeline.source.AbstractSnapshotChangeEventSource:108)
[2025-01-03 11:06:58,221] INFO [mariadb-connector|task-0] Snapshot completed (io.debezium.pipeline.source.AbstractSnapshotChangeEventSource:112)
[2025-01-03 11:06:58,226] INFO [mariadb-connector|task-0] Snapshot ended with SnapshotResult [status=COMPLETED, offset=BinlogOffsetContext{sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=BinlogSourceInfo{currentGtid='null', currentBinlogFilename='mariadb-bin.000001', currentBinlogPosition=58880, currentRowNumber=0, serverId=0, sourceTime=2025-01-03T17:06:58Z, threadId=-1, currentQuery='null', tableIds=[leafy_factory.work_orders], databaseName='leafy_factory'}, snapshotCompleted=true, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet='null', currentGtidSet='null', restartBinlogFilename='mariadb-bin.000001', restartBinlogPosition=58880, restartRowsToSkip=0, restartEventsToSkip=0, currentEventLengthInBytes=0, inTransaction=false, transactionId='null', incrementalSnapshotContext=IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]}] (io.debezium.pipeline.ChangeEventSourceCoordinator:298)
[2025-01-03 11:06:58,228] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = binlog-client (io.debezium.util.Threads:270)
[2025-01-03 11:06:58,230] INFO [mariadb-connector|task-0] Enable ssl PREFERRED mode for connector db_ (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:1289)
[2025-01-03 11:06:58,233] INFO [mariadb-connector|task-0] SignalProcessor started. Scheduling it every 5000ms (io.debezium.pipeline.signal.SignalProcessor:105)
[2025-01-03 11:06:58,233] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-SignalProcessor (io.debezium.util.Threads:287)
[2025-01-03 11:06:58,234] INFO [mariadb-connector|task-0] Starting streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:323)
[2025-01-03 11:06:58,234] INFO [mariadb-connector|task-0] Skip 0 events on streaming start (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:278)
[2025-01-03 11:06:58,234] INFO [mariadb-connector|task-0] Skip 0 rows on streaming start (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:282)
[2025-01-03 11:06:58,234] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-binlog-client (io.debezium.util.Threads:287)
[2025-01-03 11:06:58,235] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-binlog-client (io.debezium.util.Threads:287)
[2025-01-03 11:06:58,264] INFO [mariadb-connector|task-0] Connected to binlog at localhost:3306, starting at BinlogOffsetContext{sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=BinlogSourceInfo{currentGtid='null', currentBinlogFilename='mariadb-bin.000001', currentBinlogPosition=58880, currentRowNumber=0, serverId=0, sourceTime=2025-01-03T17:06:58Z, threadId=-1, currentQuery='null', tableIds=[leafy_factory.work_orders], databaseName='leafy_factory'}, snapshotCompleted=true, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet='null', currentGtidSet='null', restartBinlogFilename='mariadb-bin.000001', restartBinlogPosition=58880, restartRowsToSkip=0, restartEventsToSkip=0, currentEventLengthInBytes=0, inTransaction=false, transactionId='null', incrementalSnapshotContext=IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]} (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:1232)
[2025-01-03 11:06:58,264] INFO [mariadb-connector|task-0] Waiting for keepalive thread to start (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:299)
[2025-01-03 11:06:58,264] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-binlog-client (io.debezium.util.Threads:287)
[2025-01-03 11:06:58,369] INFO [mariadb-connector|task-0] Keepalive thread is running (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:306)
[2025-01-03 11:06:58,510] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 4 : {db_.leafy_factory.factories=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 11:06:58,644] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 7 : {db_.leafy_factory.jobs=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 11:06:58,789] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 10 : {db_.leafy_factory.jobs_machines=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 11:06:58,926] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 13 : {db_.leafy_factory.machines=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 11:06:59,055] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 17 : {db_.leafy_factory.product_cost=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 11:06:59,189] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 23 : {db_.leafy_factory.production_lines=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 11:06:59,326] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 32 : {db_.leafy_factory.products=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 11:06:59,455] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 36 : {db_.leafy_factory.products_raw_materials=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 11:06:59,589] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 40 : {db_.leafy_factory.raw_materials=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 11:06:59,714] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 45 : {db_.leafy_factory.work_orders=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 11:07:07,230] INFO [mariadb-connector|task-0|offsets] WorkerSourceTask{id=mariadb-connector-0} Committing offsets for 74 acknowledged messages (org.apache.kafka.connect.runtime.WorkerSourceTask:236)
[2025-01-03 11:07:55,114] INFO [0:0:0:0:0:0:0:1] - - [03/Jan/2025:17:07:55 +0000] "GET /connectors/mongodb-sink/status HTTP/1.1" 200 168 "-" "curl/8.7.1" 17 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-01-03 11:08:33,037] INFO [mariadb-connector|task-0] 80 records sent during previous 00:01:35.816, last recorded offset of {server=db_} partition is {ts_sec=1735924113, file=mariadb-bin.000001, pos=0, row=1, server_id=1} (io.debezium.connector.common.BaseSourceTask:351)
[2025-01-03 11:08:37,273] INFO [mariadb-connector|task-0|offsets] WorkerSourceTask{id=mariadb-connector-0} Committing offsets for 6 acknowledged messages (org.apache.kafka.connect.runtime.WorkerSourceTask:236)
[2025-01-03 11:10:48,132] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Request joining group due to: cached metadata has changed from (version2: {}) at the beginning of the rebalance to (version3: {db_.leafy_factory.factories=[NO_RACKS], db_.leafy_factory.production_lines=[NO_RACKS], db_.leafy_factory.machines=[NO_RACKS], db_.leafy_factory.jobs_machines=[NO_RACKS], db_.leafy_factory.work_orders=[NO_RACKS], db_.leafy_factory.jobs=[NO_RACKS], db_.leafy_factory.products_raw_materials=[NO_RACKS], db_.leafy_factory.raw_materials=[NO_RACKS], db_.leafy_factory.product_cost=[NO_RACKS], db_.leafy_factory.products=[NO_RACKS]}) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 11:10:48,138] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Revoke previously assigned partitions  (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:80)
[2025-01-03 11:10:48,139] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:605)
[2025-01-03 11:10:48,143] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Successfully joined group with generation Generation{generationId=2, memberId='connector-consumer-mongodb-sink-0-521fa3b0-7973-4197-a1af-b6762725b2f8', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:666)
[2025-01-03 11:10:48,147] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Finished assignment for group at generation 2: {connector-consumer-mongodb-sink-0-521fa3b0-7973-4197-a1af-b6762725b2f8=Assignment(partitions=[db_.leafy_factory.factories-0, db_.leafy_factory.machines-0, db_.leafy_factory.production_lines-0, db_.leafy_factory.jobs_machines-0, db_.leafy_factory.work_orders-0, db_.leafy_factory.jobs-0, db_.leafy_factory.products_raw_materials-0, db_.leafy_factory.raw_materials-0, db_.leafy_factory.product_cost-0, db_.leafy_factory.products-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:664)
[2025-01-03 11:10:48,151] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Successfully synced group in generation Generation{generationId=2, memberId='connector-consumer-mongodb-sink-0-521fa3b0-7973-4197-a1af-b6762725b2f8', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:843)
[2025-01-03 11:10:48,152] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Notifying assignor about the new Assignment(partitions=[db_.leafy_factory.factories-0, db_.leafy_factory.machines-0, db_.leafy_factory.production_lines-0, db_.leafy_factory.jobs_machines-0, db_.leafy_factory.work_orders-0, db_.leafy_factory.jobs-0, db_.leafy_factory.products_raw_materials-0, db_.leafy_factory.raw_materials-0, db_.leafy_factory.product_cost-0, db_.leafy_factory.products-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:324)
[2025-01-03 11:10:48,152] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Adding newly assigned partitions: db_.leafy_factory.factories-0, db_.leafy_factory.jobs-0, db_.leafy_factory.jobs_machines-0, db_.leafy_factory.machines-0, db_.leafy_factory.product_cost-0, db_.leafy_factory.production_lines-0, db_.leafy_factory.products-0, db_.leafy_factory.products_raw_materials-0, db_.leafy_factory.raw_materials-0, db_.leafy_factory.work_orders-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:58)
[2025-01-03 11:10:48,164] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Found no committed offset for partition db_.leafy_factory.machines-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1508)
[2025-01-03 11:10:48,164] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Found no committed offset for partition db_.leafy_factory.production_lines-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1508)
[2025-01-03 11:10:48,164] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Found no committed offset for partition db_.leafy_factory.products_raw_materials-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1508)
[2025-01-03 11:10:48,164] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Found no committed offset for partition db_.leafy_factory.factories-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1508)
[2025-01-03 11:10:48,165] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Found no committed offset for partition db_.leafy_factory.jobs_machines-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1508)
[2025-01-03 11:10:48,165] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Found no committed offset for partition db_.leafy_factory.work_orders-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1508)
[2025-01-03 11:10:48,165] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Found no committed offset for partition db_.leafy_factory.raw_materials-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1508)
[2025-01-03 11:10:48,165] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Found no committed offset for partition db_.leafy_factory.products-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1508)
[2025-01-03 11:10:48,165] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Found no committed offset for partition db_.leafy_factory.product_cost-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1508)
[2025-01-03 11:10:48,165] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Found no committed offset for partition db_.leafy_factory.jobs-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1508)
[2025-01-03 11:10:48,167] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Resetting offset for partition db_.leafy_factory.machines-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:10:48,167] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Resetting offset for partition db_.leafy_factory.production_lines-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:10:48,167] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Resetting offset for partition db_.leafy_factory.factories-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:10:48,167] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Resetting offset for partition db_.leafy_factory.products_raw_materials-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:10:48,167] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Resetting offset for partition db_.leafy_factory.work_orders-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:10:48,167] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Resetting offset for partition db_.leafy_factory.jobs_machines-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:10:48,167] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Resetting offset for partition db_.leafy_factory.raw_materials-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:10:48,168] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Resetting offset for partition db_.leafy_factory.product_cost-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:10:48,168] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Resetting offset for partition db_.leafy_factory.products-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:10:48,168] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Resetting offset for partition db_.leafy_factory.jobs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:10:48,219] INFO [mongodb-sink|task-0] MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = ${topic}
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = db_.leafy_factory.machines
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 11:10:48,222] INFO [mongodb-sink|task-0] MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = ${topic}
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = db_.leafy_factory.production_lines
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 11:10:48,223] INFO [mongodb-sink|task-0] MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = ${topic}
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = db_.leafy_factory.factories
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 11:10:48,223] INFO [mongodb-sink|task-0] MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = ${topic}
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = db_.leafy_factory.products_raw_materials
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 11:10:48,224] INFO [mongodb-sink|task-0] MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = ${topic}
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = db_.leafy_factory.jobs_machines
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 11:10:48,225] INFO [mongodb-sink|task-0] MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = ${topic}
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = db_.leafy_factory.work_orders
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 11:10:48,226] INFO [mongodb-sink|task-0] MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = ${topic}
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = db_.leafy_factory.raw_materials
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 11:10:48,227] INFO [mongodb-sink|task-0] MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = ${topic}
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = db_.leafy_factory.product_cost
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 11:10:48,227] INFO [mongodb-sink|task-0] MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = ${topic}
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = db_.leafy_factory.products
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 11:10:48,228] INFO [mongodb-sink|task-0] MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = ${topic}
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = db_.leafy_factory.jobs
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 11:10:49,308] INFO [mongodb-sink|task-0] Opened connection [connectionId{localValue:14, serverValue:1295055}] to ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:10:49,482] ERROR [mongodb-sink|task-0] Failed to put into the sink the following records: [SinkRecord{kafkaOffset=0, timestampType=CreateTime, originalTopic=db_.leafy_factory.machines, originalKafkaPartition=0, originalKafkaOffset=0} ConnectRecord{topic='db_.leafy_factory.machines', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_machine"}],"optional":false,"name":"db_.leafy_factory.machines.Key"},"payload":{"id_machine":1}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_machine=1,machine_status=Running,last_maintenance=1730384700000,operator=Ada Lovelace,avg_output=3000.00,reject_count=25.00,production_line_id=1},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=first_in_data_collection,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=machines,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018204,ts_us=1735924018204472,ts_ns=1735924018204472000}, valueSchema=Schema{db_.leafy_factory.machines.Envelope:STRUCT}, timestamp=1735924019034, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=1, timestampType=CreateTime, originalTopic=db_.leafy_factory.machines, originalKafkaPartition=0, originalKafkaOffset=1} ConnectRecord{topic='db_.leafy_factory.machines', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_machine"}],"optional":false,"name":"db_.leafy_factory.machines.Key"},"payload":{"id_machine":2}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_machine=2,machine_status=Running,last_maintenance=1730384700000,operator=Claude Jones,avg_output=3000.00,reject_count=25.00,production_line_id=1},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=machines,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018204,ts_us=1735924018204608,ts_ns=1735924018204608000}, valueSchema=Schema{db_.leafy_factory.machines.Envelope:STRUCT}, timestamp=1735924019035, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=2, timestampType=CreateTime, originalTopic=db_.leafy_factory.machines, originalKafkaPartition=0, originalKafkaOffset=2} ConnectRecord{topic='db_.leafy_factory.machines', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_machine"}],"optional":false,"name":"db_.leafy_factory.machines.Key"},"payload":{"id_machine":3}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_machine=3,machine_status=Available,last_maintenance=1730384700000,operator=Grace Conway,avg_output=3000.00,reject_count=25.00,production_line_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=machines,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018204,ts_us=1735924018204737,ts_ns=1735924018204737000}, valueSchema=Schema{db_.leafy_factory.machines.Envelope:STRUCT}, timestamp=1735924019035, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=3, timestampType=CreateTime, originalTopic=db_.leafy_factory.machines, originalKafkaPartition=0, originalKafkaOffset=3} ConnectRecord{topic='db_.leafy_factory.machines', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_machine"}],"optional":false,"name":"db_.leafy_factory.machines.Key"},"payload":{"id_machine":4}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_machine=4,machine_status=Available,last_maintenance=1730384700000,operator=Frida Sidik,avg_output=3000.00,reject_count=25.00,production_line_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=last_in_data_collection,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=machines,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018204,ts_us=1735924018204874,ts_ns=1735924018204874000}, valueSchema=Schema{db_.leafy_factory.machines.Envelope:STRUCT}, timestamp=1735924019036, headers=ConnectHeaders(headers=)}] (com.mongodb.kafka.connect.sink.MongoSinkTask:247)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924249, "i": 5}}, "signature": {"hash": {"$binary": {"base64": "U83vRdhQoUUXSUut0RPaYcmgRTI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924249, "i": 5}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:49,489] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.machines', partition=0, offset=0, timestamp=1735924019034, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924249, "i": 5}}, "signature": {"hash": {"$binary": {"base64": "U83vRdhQoUUXSUut0RPaYcmgRTI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924249, "i": 5}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:49,490] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.machines', partition=0, offset=1, timestamp=1735924019035, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924249, "i": 5}}, "signature": {"hash": {"$binary": {"base64": "U83vRdhQoUUXSUut0RPaYcmgRTI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924249, "i": 5}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:49,490] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.machines', partition=0, offset=2, timestamp=1735924019035, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924249, "i": 5}}, "signature": {"hash": {"$binary": {"base64": "U83vRdhQoUUXSUut0RPaYcmgRTI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924249, "i": 5}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:49,491] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.machines', partition=0, offset=3, timestamp=1735924019036, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924249, "i": 5}}, "signature": {"hash": {"$binary": {"base64": "U83vRdhQoUUXSUut0RPaYcmgRTI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924249, "i": 5}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:49,595] ERROR [mongodb-sink|task-0] Failed to put into the sink the following records: [SinkRecord{kafkaOffset=0, timestampType=CreateTime, originalTopic=db_.leafy_factory.production_lines, originalKafkaPartition=0, originalKafkaOffset=0} ConnectRecord{topic='db_.leafy_factory.production_lines', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_production_line"}],"optional":false,"name":"db_.leafy_factory.production_lines.Key"},"payload":{"id_production_line":1}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_production_line=1,factory_id=1},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=first_in_data_collection,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=production_lines,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018209,ts_us=1735924018209079,ts_ns=1735924018209079000}, valueSchema=Schema{db_.leafy_factory.production_lines.Envelope:STRUCT}, timestamp=1735924019309, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=1, timestampType=CreateTime, originalTopic=db_.leafy_factory.production_lines, originalKafkaPartition=0, originalKafkaOffset=1} ConnectRecord{topic='db_.leafy_factory.production_lines', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_production_line"}],"optional":false,"name":"db_.leafy_factory.production_lines.Key"},"payload":{"id_production_line":2}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_production_line=2,factory_id=1},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=last_in_data_collection,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=production_lines,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018209,ts_us=1735924018209129,ts_ns=1735924018209129000}, valueSchema=Schema{db_.leafy_factory.production_lines.Envelope:STRUCT}, timestamp=1735924019309, headers=ConnectHeaders(headers=)}] (com.mongodb.kafka.connect.sink.MongoSinkTask:247)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924249, "i": 5}}, "signature": {"hash": {"$binary": {"base64": "U83vRdhQoUUXSUut0RPaYcmgRTI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924249, "i": 5}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:49,596] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.production_lines', partition=0, offset=0, timestamp=1735924019309, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924249, "i": 5}}, "signature": {"hash": {"$binary": {"base64": "U83vRdhQoUUXSUut0RPaYcmgRTI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924249, "i": 5}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:49,597] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.production_lines', partition=0, offset=1, timestamp=1735924019309, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924249, "i": 5}}, "signature": {"hash": {"$binary": {"base64": "U83vRdhQoUUXSUut0RPaYcmgRTI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924249, "i": 5}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:49,700] ERROR [mongodb-sink|task-0] Failed to put into the sink the following records: [SinkRecord{kafkaOffset=0, timestampType=CreateTime, originalTopic=db_.leafy_factory.factories, originalKafkaPartition=0, originalKafkaOffset=0} ConnectRecord{topic='db_.leafy_factory.factories', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_factory"}],"optional":false,"name":"db_.leafy_factory.factories.Key"},"payload":{"id_factory":1}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_factory=1,factory_name=qro_fact_1,factory_location=Plant A,factory_timestamp=1730384700000},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=first,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=factories,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018198,ts_us=1735924018198798,ts_ns=1735924018198798000}, valueSchema=Schema{db_.leafy_factory.factories.Envelope:STRUCT}, timestamp=1735924018623, headers=ConnectHeaders(headers=)}] (com.mongodb.kafka.connect.sink.MongoSinkTask:247)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924249, "i": 6}}, "signature": {"hash": {"$binary": {"base64": "U83vRdhQoUUXSUut0RPaYcmgRTI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924249, "i": 6}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:49,701] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.factories', partition=0, offset=0, timestamp=1735924018623, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924249, "i": 6}}, "signature": {"hash": {"$binary": {"base64": "U83vRdhQoUUXSUut0RPaYcmgRTI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924249, "i": 6}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:49,808] ERROR [mongodb-sink|task-0] Failed to put into the sink the following records: [SinkRecord{kafkaOffset=0, timestampType=CreateTime, originalTopic=db_.leafy_factory.products_raw_materials, originalKafkaPartition=0, originalKafkaOffset=0} ConnectRecord{topic='db_.leafy_factory.products_raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_products_raw_materials"}],"optional":false,"name":"db_.leafy_factory.products_raw_materials.Key"},"payload":{"id_products_raw_materials":1}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_products_raw_materials=1,product_id=1,raw_material_id=1},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=first_in_data_collection,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=products_raw_materials,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018212,ts_us=1735924018212157,ts_ns=1735924018212157000}, valueSchema=Schema{db_.leafy_factory.products_raw_materials.Envelope:STRUCT}, timestamp=1735924019567, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=1, timestampType=CreateTime, originalTopic=db_.leafy_factory.products_raw_materials, originalKafkaPartition=0, originalKafkaOffset=1} ConnectRecord{topic='db_.leafy_factory.products_raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_products_raw_materials"}],"optional":false,"name":"db_.leafy_factory.products_raw_materials.Key"},"payload":{"id_products_raw_materials":2}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_products_raw_materials=2,product_id=1,raw_material_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=products_raw_materials,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018212,ts_us=1735924018212227,ts_ns=1735924018212227000}, valueSchema=Schema{db_.leafy_factory.products_raw_materials.Envelope:STRUCT}, timestamp=1735924019567, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=2, timestampType=CreateTime, originalTopic=db_.leafy_factory.products_raw_materials, originalKafkaPartition=0, originalKafkaOffset=2} ConnectRecord{topic='db_.leafy_factory.products_raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_products_raw_materials"}],"optional":false,"name":"db_.leafy_factory.products_raw_materials.Key"},"payload":{"id_products_raw_materials":3}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_products_raw_materials=3,product_id=1,raw_material_id=3},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=products_raw_materials,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018212,ts_us=1735924018212274,ts_ns=1735924018212274000}, valueSchema=Schema{db_.leafy_factory.products_raw_materials.Envelope:STRUCT}, timestamp=1735924019568, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=3, timestampType=CreateTime, originalTopic=db_.leafy_factory.products_raw_materials, originalKafkaPartition=0, originalKafkaOffset=3} ConnectRecord{topic='db_.leafy_factory.products_raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_products_raw_materials"}],"optional":false,"name":"db_.leafy_factory.products_raw_materials.Key"},"payload":{"id_products_raw_materials":4}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_products_raw_materials=4,product_id=1,raw_material_id=4},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=products_raw_materials,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018212,ts_us=1735924018212315,ts_ns=1735924018212315000}, valueSchema=Schema{db_.leafy_factory.products_raw_materials.Envelope:STRUCT}, timestamp=1735924019568, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=4, timestampType=CreateTime, originalTopic=db_.leafy_factory.products_raw_materials, originalKafkaPartition=0, originalKafkaOffset=4} ConnectRecord{topic='db_.leafy_factory.products_raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_products_raw_materials"}],"optional":false,"name":"db_.leafy_factory.products_raw_materials.Key"},"payload":{"id_products_raw_materials":5}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_products_raw_materials=5,product_id=2,raw_material_id=5},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=products_raw_materials,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018212,ts_us=1735924018212360,ts_ns=1735924018212360000}, valueSchema=Schema{db_.leafy_factory.products_raw_materials.Envelope:STRUCT}, timestamp=1735924019568, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=5, timestampType=CreateTime, originalTopic=db_.leafy_factory.products_raw_materials, originalKafkaPartition=0, originalKafkaOffset=5} ConnectRecord{topic='db_.leafy_factory.products_raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_products_raw_materials"}],"optional":false,"name":"db_.leafy_factory.products_raw_materials.Key"},"payload":{"id_products_raw_materials":6}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_products_raw_materials=6,product_id=2,raw_material_id=6},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=products_raw_materials,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018212,ts_us=1735924018212404,ts_ns=1735924018212404000}, valueSchema=Schema{db_.leafy_factory.products_raw_materials.Envelope:STRUCT}, timestamp=1735924019568, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=6, timestampType=CreateTime, originalTopic=db_.leafy_factory.products_raw_materials, originalKafkaPartition=0, originalKafkaOffset=6} ConnectRecord{topic='db_.leafy_factory.products_raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_products_raw_materials"}],"optional":false,"name":"db_.leafy_factory.products_raw_materials.Key"},"payload":{"id_products_raw_materials":7}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_products_raw_materials=7,product_id=2,raw_material_id=7},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=products_raw_materials,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018212,ts_us=1735924018212448,ts_ns=1735924018212448000}, valueSchema=Schema{db_.leafy_factory.products_raw_materials.Envelope:STRUCT}, timestamp=1735924019568, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=7, timestampType=CreateTime, originalTopic=db_.leafy_factory.products_raw_materials, originalKafkaPartition=0, originalKafkaOffset=7} ConnectRecord{topic='db_.leafy_factory.products_raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_products_raw_materials"}],"optional":false,"name":"db_.leafy_factory.products_raw_materials.Key"},"payload":{"id_products_raw_materials":8}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_products_raw_materials=8,product_id=2,raw_material_id=8},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=last_in_data_collection,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=products_raw_materials,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018212,ts_us=1735924018212491,ts_ns=1735924018212491000}, valueSchema=Schema{db_.leafy_factory.products_raw_materials.Envelope:STRUCT}, timestamp=1735924019569, headers=ConnectHeaders(headers=)}] (com.mongodb.kafka.connect.sink.MongoSinkTask:247)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924249, "i": 6}}, "signature": {"hash": {"$binary": {"base64": "U83vRdhQoUUXSUut0RPaYcmgRTI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924249, "i": 6}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:49,809] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.products_raw_materials', partition=0, offset=0, timestamp=1735924019567, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924249, "i": 6}}, "signature": {"hash": {"$binary": {"base64": "U83vRdhQoUUXSUut0RPaYcmgRTI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924249, "i": 6}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:49,810] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.products_raw_materials', partition=0, offset=1, timestamp=1735924019567, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924249, "i": 6}}, "signature": {"hash": {"$binary": {"base64": "U83vRdhQoUUXSUut0RPaYcmgRTI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924249, "i": 6}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:49,811] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.products_raw_materials', partition=0, offset=2, timestamp=1735924019568, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924249, "i": 6}}, "signature": {"hash": {"$binary": {"base64": "U83vRdhQoUUXSUut0RPaYcmgRTI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924249, "i": 6}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:49,811] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.products_raw_materials', partition=0, offset=3, timestamp=1735924019568, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924249, "i": 6}}, "signature": {"hash": {"$binary": {"base64": "U83vRdhQoUUXSUut0RPaYcmgRTI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924249, "i": 6}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:49,812] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.products_raw_materials', partition=0, offset=4, timestamp=1735924019568, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924249, "i": 6}}, "signature": {"hash": {"$binary": {"base64": "U83vRdhQoUUXSUut0RPaYcmgRTI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924249, "i": 6}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:49,813] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.products_raw_materials', partition=0, offset=5, timestamp=1735924019568, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924249, "i": 6}}, "signature": {"hash": {"$binary": {"base64": "U83vRdhQoUUXSUut0RPaYcmgRTI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924249, "i": 6}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:49,813] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.products_raw_materials', partition=0, offset=6, timestamp=1735924019568, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924249, "i": 6}}, "signature": {"hash": {"$binary": {"base64": "U83vRdhQoUUXSUut0RPaYcmgRTI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924249, "i": 6}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:49,814] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.products_raw_materials', partition=0, offset=7, timestamp=1735924019569, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924249, "i": 6}}, "signature": {"hash": {"$binary": {"base64": "U83vRdhQoUUXSUut0RPaYcmgRTI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924249, "i": 6}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:49,918] ERROR [mongodb-sink|task-0] Failed to put into the sink the following records: [SinkRecord{kafkaOffset=0, timestampType=CreateTime, originalTopic=db_.leafy_factory.jobs_machines, originalKafkaPartition=0, originalKafkaOffset=0} ConnectRecord{topic='db_.leafy_factory.jobs_machines', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_jobs_machines"}],"optional":false,"name":"db_.leafy_factory.jobs_machines.Key"},"payload":{"id_jobs_machines":1}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_jobs_machines=1,job_id=1,machine_id=1},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=first_in_data_collection,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=jobs_machines,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018202,ts_us=1735924018202977,ts_ns=1735924018202977000}, valueSchema=Schema{db_.leafy_factory.jobs_machines.Envelope:STRUCT}, timestamp=1735924018909, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=1, timestampType=CreateTime, originalTopic=db_.leafy_factory.jobs_machines, originalKafkaPartition=0, originalKafkaOffset=1} ConnectRecord{topic='db_.leafy_factory.jobs_machines', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_jobs_machines"}],"optional":false,"name":"db_.leafy_factory.jobs_machines.Key"},"payload":{"id_jobs_machines":2}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_jobs_machines=2,job_id=1,machine_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=last_in_data_collection,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=jobs_machines,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018203,ts_us=1735924018203056,ts_ns=1735924018203056000}, valueSchema=Schema{db_.leafy_factory.jobs_machines.Envelope:STRUCT}, timestamp=1735924018909, headers=ConnectHeaders(headers=)}] (com.mongodb.kafka.connect.sink.MongoSinkTask:247)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924249, "i": 7}}, "signature": {"hash": {"$binary": {"base64": "U83vRdhQoUUXSUut0RPaYcmgRTI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924249, "i": 7}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:49,921] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.jobs_machines', partition=0, offset=0, timestamp=1735924018909, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924249, "i": 7}}, "signature": {"hash": {"$binary": {"base64": "U83vRdhQoUUXSUut0RPaYcmgRTI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924249, "i": 7}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:49,922] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.jobs_machines', partition=0, offset=1, timestamp=1735924018909, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924249, "i": 7}}, "signature": {"hash": {"$binary": {"base64": "U83vRdhQoUUXSUut0RPaYcmgRTI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924249, "i": 7}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,090] ERROR [mongodb-sink|task-0] Failed to put into the sink the following records: [SinkRecord{kafkaOffset=0, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=0} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":1}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=1,planned_start_date=1733961600000,planned_end_date=1733961600000,actual_start_date=1735568659000,quantity=10,wo_status=In Progress,creation_date=1735568647000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=first_in_data_collection,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018215,ts_us=1735924018215748,ts_ns=1735924018215748000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735924019836, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=1, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=1} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":2}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=2,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735821435000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018215,ts_us=1735924018215978,ts_ns=1735924018215978000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735924019837, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=2, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=2} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":3}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=3,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735821471000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018216,ts_us=1735924018216158,ts_ns=1735924018216158000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735924019837, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=3, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=3} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":4}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=4,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735821640000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018217,ts_us=1735924018217186,ts_ns=1735924018217186000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735924019838, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=4, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=4} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":5}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=5,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735822841000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018217,ts_us=1735924018217374,ts_ns=1735924018217374000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735924019838, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=5, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=5} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":6}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=6,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735825365000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018217,ts_us=1735924018217552,ts_ns=1735924018217552000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735924019838, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=6, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=6} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":7}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=7,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735825423000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018217,ts_us=1735924018217710,ts_ns=1735924018217710000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735924019838, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=7, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=7} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":8}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=8,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735825590000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018217,ts_us=1735924018217878,ts_ns=1735924018217878000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735924019838, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=8, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=8} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":9}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=9,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735827154000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018218,ts_us=1735924018218039,ts_ns=1735924018218039000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735924019839, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=9, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=9} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":10}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=10,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735827191000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018218,ts_us=1735924018218194,ts_ns=1735924018218194000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735924019839, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=10, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=10} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":11}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=11,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735827213000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018218,ts_us=1735924018218352,ts_ns=1735924018218352000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735924019839, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=11, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=11} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":12}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=12,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735827239000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018218,ts_us=1735924018218515,ts_ns=1735924018218515000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735924019839, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=12, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=12} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":13}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=13,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735827365000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018218,ts_us=1735924018218655,ts_ns=1735924018218655000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735924019839, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=13, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=13} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":14}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=14,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735827367000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018218,ts_us=1735924018218795,ts_ns=1735924018218795000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735924019839, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=14, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=14} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":15}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=15,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735839623000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018218,ts_us=1735924018218934,ts_ns=1735924018218934000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735924019839, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=15, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=15} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":16}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=16,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735839629000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018219,ts_us=1735924018219074,ts_ns=1735924018219074000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735924019839, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=16, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=16} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":17}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=17,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735840703000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018219,ts_us=1735924018219212,ts_ns=1735924018219212000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735924019840, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=17, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=17} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":18}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=18,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735841700000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018219,ts_us=1735924018219352,ts_ns=1735924018219352000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735924019840, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=18, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=18} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":19}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=19,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735842324000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018219,ts_us=1735924018219491,ts_ns=1735924018219491000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735924019840, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=19, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=19} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":20}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=20,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735842960000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018219,ts_us=1735924018219626,ts_ns=1735924018219626000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735924019840, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=20, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=20} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":21}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=21,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735899898000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018219,ts_us=1735924018219775,ts_ns=1735924018219775000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735924019840, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=21, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=21} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":22}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=22,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735901020000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018219,ts_us=1735924018219923,ts_ns=1735924018219923000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735924019840, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=22, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=22} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":23}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=23,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735901805000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=last,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=work_orders,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018220,ts_us=1735924018220065,ts_ns=1735924018220065000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735924019840, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=23, timestampType=CreateTime, originalTopic=db_.leafy_factory.work_orders, originalKafkaPartition=0, originalKafkaOffset=23} ConnectRecord{topic='db_.leafy_factory.work_orders', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_work"}],"optional":false,"name":"db_.leafy_factory.work_orders.Key"},"payload":{"id_work":24}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_work=24,planned_start_date=1733961600000,planned_end_date=1733961600000,quantity=10,wo_status=Created,creation_date=1735902512000,product_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924113000,snapshot=false,db=leafy_factory,ts_us=1735924113000000,ts_ns=1735924113000000000,table=work_orders,server_id=1,file=mariadb-bin.000001,pos=-72,row=0},op=c,ts_ms=1735924113017,ts_us=1735924113017121,ts_ns=1735924113017121000}, valueSchema=Schema{db_.leafy_factory.work_orders.Envelope:STRUCT}, timestamp=1735924113045, headers=ConnectHeaders(headers=)}] (com.mongodb.kafka.connect.sink.MongoSinkTask:247)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,092] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=0, timestamp=1735924019836, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,093] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=1, timestamp=1735924019837, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,093] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=2, timestamp=1735924019837, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,094] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=3, timestamp=1735924019838, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,094] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=4, timestamp=1735924019838, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,095] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=5, timestamp=1735924019838, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,095] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=6, timestamp=1735924019838, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,096] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=7, timestamp=1735924019838, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,096] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=8, timestamp=1735924019839, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,096] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=9, timestamp=1735924019839, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,097] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=10, timestamp=1735924019839, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,097] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=11, timestamp=1735924019839, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,097] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=12, timestamp=1735924019839, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,098] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=13, timestamp=1735924019839, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,098] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=14, timestamp=1735924019839, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,099] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=15, timestamp=1735924019839, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,099] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=16, timestamp=1735924019840, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,100] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=17, timestamp=1735924019840, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,100] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=18, timestamp=1735924019840, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,100] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=19, timestamp=1735924019840, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,101] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=20, timestamp=1735924019840, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,101] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=21, timestamp=1735924019840, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,101] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=22, timestamp=1735924019840, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,101] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.work_orders', partition=0, offset=23, timestamp=1735924113045, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,268] ERROR [mongodb-sink|task-0] Failed to put into the sink the following records: [SinkRecord{kafkaOffset=0, timestampType=CreateTime, originalTopic=db_.leafy_factory.raw_materials, originalKafkaPartition=0, originalKafkaOffset=0} ConnectRecord{topic='db_.leafy_factory.raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_raw_material"}],"optional":false,"name":"db_.leafy_factory.raw_materials.Key"},"payload":{"id_raw_material":1}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_raw_material=1,item_code=hinges_ss,raw_material_name=Stainless steel hinge,raw_material_description=Stainless steel hinge,unit_measurement=pieces,raw_material_stock=9540,raw_material_status=high,raw_material_currency=USD,cost_per_part=1.50},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=first_in_data_collection,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=raw_materials,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018213,ts_us=1735924018213723,ts_ns=1735924018213723000}, valueSchema=Schema{db_.leafy_factory.raw_materials.Envelope:STRUCT}, timestamp=1735924019696, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=1, timestampType=CreateTime, originalTopic=db_.leafy_factory.raw_materials, originalKafkaPartition=0, originalKafkaOffset=1} ConnectRecord{topic='db_.leafy_factory.raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_raw_material"}],"optional":false,"name":"db_.leafy_factory.raw_materials.Key"},"payload":{"id_raw_material":2}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_raw_material=2,item_code=screw_ss,raw_material_name=Stainless steel screw,raw_material_description=Stainless steel screw,unit_measurement=pieces,raw_material_stock=92640,raw_material_status=high,raw_material_currency=USD,cost_per_part=0.05},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=raw_materials,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018213,ts_us=1735924018213815,ts_ns=1735924018213815000}, valueSchema=Schema{db_.leafy_factory.raw_materials.Envelope:STRUCT}, timestamp=1735924019697, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=2, timestampType=CreateTime, originalTopic=db_.leafy_factory.raw_materials, originalKafkaPartition=0, originalKafkaOffset=2} ConnectRecord{topic='db_.leafy_factory.raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_raw_material"}],"optional":false,"name":"db_.leafy_factory.raw_materials.Key"},"payload":{"id_raw_material":3}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_raw_material=3,item_code=aluminum_6061,raw_material_name=Aluminum,raw_material_description=lightweight aluminum,unit_measurement=kg,raw_material_stock=9563,raw_material_status=high,raw_material_currency=USD,cost_per_part=3.00},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=raw_materials,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018213,ts_us=1735924018213872,ts_ns=1735924018213872000}, valueSchema=Schema{db_.leafy_factory.raw_materials.Envelope:STRUCT}, timestamp=1735924019697, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=3, timestampType=CreateTime, originalTopic=db_.leafy_factory.raw_materials, originalKafkaPartition=0, originalKafkaOffset=3} ConnectRecord{topic='db_.leafy_factory.raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_raw_material"}],"optional":false,"name":"db_.leafy_factory.raw_materials.Key"},"payload":{"id_raw_material":4}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_raw_material=4,item_code=brackets_gs,raw_material_name=Galvanized bracket,raw_material_description=Galvanized brackets anti-corrosion,unit_measurement=pieces,raw_material_stock=8160,raw_material_status=high,raw_material_currency=USD,cost_per_part=2.50},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=raw_materials,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018213,ts_us=1735924018213972,ts_ns=1735924018213972000}, valueSchema=Schema{db_.leafy_factory.raw_materials.Envelope:STRUCT}, timestamp=1735924019697, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=4, timestampType=CreateTime, originalTopic=db_.leafy_factory.raw_materials, originalKafkaPartition=0, originalKafkaOffset=4} ConnectRecord{topic='db_.leafy_factory.raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_raw_material"}],"optional":false,"name":"db_.leafy_factory.raw_materials.Key"},"payload":{"id_raw_material":5}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_raw_material=5,item_code=titanium_lw,raw_material_name=Lightweight titanium,raw_material_description=Lightweight titanium known for its strength and lightweight properties,unit_measurement=kg,raw_material_stock=10000,raw_material_status=high,raw_material_currency=USD,cost_per_part=30.00},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=raw_materials,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018214,ts_us=1735924018214031,ts_ns=1735924018214031000}, valueSchema=Schema{db_.leafy_factory.raw_materials.Envelope:STRUCT}, timestamp=1735924019698, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=5, timestampType=CreateTime, originalTopic=db_.leafy_factory.raw_materials, originalKafkaPartition=0, originalKafkaOffset=5} ConnectRecord{topic='db_.leafy_factory.raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_raw_material"}],"optional":false,"name":"db_.leafy_factory.raw_materials.Key"},"payload":{"id_raw_material":6}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_raw_material=6,item_code=wood_hc,raw_material_name=Hickory wood,raw_material_description=Durable, lightweight, and shock-resistant, ideal for tool handles.,unit_measurement=kg,raw_material_stock=10000,raw_material_status=high,raw_material_currency=USD,cost_per_part=4.00},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=raw_materials,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018214,ts_us=1735924018214088,ts_ns=1735924018214088000}, valueSchema=Schema{db_.leafy_factory.raw_materials.Envelope:STRUCT}, timestamp=1735924019698, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=6, timestampType=CreateTime, originalTopic=db_.leafy_factory.raw_materials, originalKafkaPartition=0, originalKafkaOffset=6} ConnectRecord{topic='db_.leafy_factory.raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_raw_material"}],"optional":false,"name":"db_.leafy_factory.raw_materials.Key"},"payload":{"id_raw_material":7}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_raw_material=7,item_code=magnet_nm,raw_material_name=Neodymium magnet,raw_material_description=Strong magnet embedded in the hammer head to hold nails.,unit_measurement=kg,raw_material_stock=10000,raw_material_status=high,raw_material_currency=USD,cost_per_part=50.00},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=raw_materials,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018214,ts_us=1735924018214147,ts_ns=1735924018214147000}, valueSchema=Schema{db_.leafy_factory.raw_materials.Envelope:STRUCT}, timestamp=1735924019698, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=7, timestampType=CreateTime, originalTopic=db_.leafy_factory.raw_materials, originalKafkaPartition=0, originalKafkaOffset=7} ConnectRecord{topic='db_.leafy_factory.raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_raw_material"}],"optional":false,"name":"db_.leafy_factory.raw_materials.Key"},"payload":{"id_raw_material":8}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_raw_material=8,item_code=fasteners_ham,raw_material_name=Steel fasteners,raw_material_description=Steel fasteners for titanium hammers.,unit_measurement=kg,raw_material_stock=10000,raw_material_status=high,raw_material_currency=USD,cost_per_part=1.00},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=last_in_data_collection,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=raw_materials,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018214,ts_us=1735924018214219,ts_ns=1735924018214219000}, valueSchema=Schema{db_.leafy_factory.raw_materials.Envelope:STRUCT}, timestamp=1735924019698, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=8, timestampType=CreateTime, originalTopic=db_.leafy_factory.raw_materials, originalKafkaPartition=0, originalKafkaOffset=8} ConnectRecord{topic='db_.leafy_factory.raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_raw_material"}],"optional":false,"name":"db_.leafy_factory.raw_materials.Key"},"payload":{"id_raw_material":3}}, keySchema=Schema{STRING}, value=Struct{before=Struct{id_raw_material=3,item_code=aluminum_6061,raw_material_name=Aluminum,raw_material_description=lightweight aluminum,unit_measurement=kg,raw_material_stock=9563,raw_material_status=high,raw_material_currency=USD,cost_per_part=3.00},after=Struct{id_raw_material=3,item_code=aluminum_6061,raw_material_name=Aluminum,raw_material_description=lightweight aluminum,unit_measurement=kg,raw_material_stock=9544,raw_material_status=high,raw_material_currency=USD,cost_per_part=3.00},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924112000,snapshot=false,db=leafy_factory,ts_us=1735924112000000,ts_ns=1735924112000000000,table=raw_materials,server_id=1,file=mariadb-bin.000001,pos=-190,row=0},op=u,ts_ms=1735924113009,ts_us=1735924113009895,ts_ns=1735924113009895000}, valueSchema=Schema{db_.leafy_factory.raw_materials.Envelope:STRUCT}, timestamp=1735924113040, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=9, timestampType=CreateTime, originalTopic=db_.leafy_factory.raw_materials, originalKafkaPartition=0, originalKafkaOffset=9} ConnectRecord{topic='db_.leafy_factory.raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_raw_material"}],"optional":false,"name":"db_.leafy_factory.raw_materials.Key"},"payload":{"id_raw_material":1}}, keySchema=Schema{STRING}, value=Struct{before=Struct{id_raw_material=1,item_code=hinges_ss,raw_material_name=Stainless steel hinge,raw_material_description=Stainless steel hinge,unit_measurement=pieces,raw_material_stock=9540,raw_material_status=high,raw_material_currency=USD,cost_per_part=1.50},after=Struct{id_raw_material=1,item_code=hinges_ss,raw_material_name=Stainless steel hinge,raw_material_description=Stainless steel hinge,unit_measurement=pieces,raw_material_stock=9520,raw_material_status=high,raw_material_currency=USD,cost_per_part=1.50},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924112000,snapshot=false,db=leafy_factory,ts_us=1735924112000000,ts_ns=1735924112000000000,table=raw_materials,server_id=1,file=mariadb-bin.000001,pos=-218,row=0},op=u,ts_ms=1735924113012,ts_us=1735924113012985,ts_ns=1735924113012985000}, valueSchema=Schema{db_.leafy_factory.raw_materials.Envelope:STRUCT}, timestamp=1735924113042, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=10, timestampType=CreateTime, originalTopic=db_.leafy_factory.raw_materials, originalKafkaPartition=0, originalKafkaOffset=10} ConnectRecord{topic='db_.leafy_factory.raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_raw_material"}],"optional":false,"name":"db_.leafy_factory.raw_materials.Key"},"payload":{"id_raw_material":4}}, keySchema=Schema{STRING}, value=Struct{before=Struct{id_raw_material=4,item_code=brackets_gs,raw_material_name=Galvanized bracket,raw_material_description=Galvanized brackets anti-corrosion,unit_measurement=pieces,raw_material_stock=8160,raw_material_status=high,raw_material_currency=USD,cost_per_part=2.50},after=Struct{id_raw_material=4,item_code=brackets_gs,raw_material_name=Galvanized bracket,raw_material_description=Galvanized brackets anti-corrosion,unit_measurement=pieces,raw_material_stock=8080,raw_material_status=high,raw_material_currency=USD,cost_per_part=2.50},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924112000,snapshot=false,db=leafy_factory,ts_us=1735924112000000,ts_ns=1735924112000000000,table=raw_materials,server_id=1,file=mariadb-bin.000001,pos=-242,row=0},op=u,ts_ms=1735924113013,ts_us=1735924113013703,ts_ns=1735924113013703000}, valueSchema=Schema{db_.leafy_factory.raw_materials.Envelope:STRUCT}, timestamp=1735924113042, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=11, timestampType=CreateTime, originalTopic=db_.leafy_factory.raw_materials, originalKafkaPartition=0, originalKafkaOffset=11} ConnectRecord{topic='db_.leafy_factory.raw_materials', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_raw_material"}],"optional":false,"name":"db_.leafy_factory.raw_materials.Key"},"payload":{"id_raw_material":2}}, keySchema=Schema{STRING}, value=Struct{before=Struct{id_raw_material=2,item_code=screw_ss,raw_material_name=Stainless steel screw,raw_material_description=Stainless steel screw,unit_measurement=pieces,raw_material_stock=92640,raw_material_status=high,raw_material_currency=USD,cost_per_part=0.05},after=Struct{id_raw_material=2,item_code=screw_ss,raw_material_name=Stainless steel screw,raw_material_description=Stainless steel screw,unit_measurement=pieces,raw_material_stock=92320,raw_material_status=high,raw_material_currency=USD,cost_per_part=0.05},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924112000,snapshot=false,db=leafy_factory,ts_us=1735924112000000,ts_ns=1735924112000000000,table=raw_materials,server_id=1,file=mariadb-bin.000001,pos=-216,row=0},op=u,ts_ms=1735924113014,ts_us=1735924113014519,ts_ns=1735924113014519000}, valueSchema=Schema{db_.leafy_factory.raw_materials.Envelope:STRUCT}, timestamp=1735924113042, headers=ConnectHeaders(headers=)}] (com.mongodb.kafka.connect.sink.MongoSinkTask:247)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,269] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.raw_materials', partition=0, offset=0, timestamp=1735924019696, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,270] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.raw_materials', partition=0, offset=1, timestamp=1735924019697, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,270] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.raw_materials', partition=0, offset=2, timestamp=1735924019697, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,270] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.raw_materials', partition=0, offset=3, timestamp=1735924019697, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,270] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.raw_materials', partition=0, offset=4, timestamp=1735924019698, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,271] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.raw_materials', partition=0, offset=5, timestamp=1735924019698, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,271] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.raw_materials', partition=0, offset=6, timestamp=1735924019698, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,271] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.raw_materials', partition=0, offset=7, timestamp=1735924019698, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,271] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.raw_materials', partition=0, offset=8, timestamp=1735924113040, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,271] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.raw_materials', partition=0, offset=9, timestamp=1735924113042, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,272] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.raw_materials', partition=0, offset=10, timestamp=1735924113042, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,272] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.raw_materials', partition=0, offset=11, timestamp=1735924113042, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 2}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 2}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,436] ERROR [mongodb-sink|task-0] Failed to put into the sink the following records: [SinkRecord{kafkaOffset=0, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=0} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":1}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=1,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=1},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=first_in_data_collection,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018206,ts_us=1735924018206513,ts_ns=1735924018206513000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735924019167, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=1, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=1} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":2}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=2,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=2},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018206,ts_us=1735924018206587,ts_ns=1735924018206587000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735924019168, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=2, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=2} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":3}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=3,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=3},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018206,ts_us=1735924018206656,ts_ns=1735924018206656000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735924019168, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=3, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=3} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":4}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=4,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=4},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018206,ts_us=1735924018206730,ts_ns=1735924018206730000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735924019168, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=4, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=4} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":5}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=5,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=5},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018206,ts_us=1735924018206797,ts_ns=1735924018206797000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735924019168, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=5, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=5} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":6}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=6,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=6},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018206,ts_us=1735924018206866,ts_ns=1735924018206866000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735924019168, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=6, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=6} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":7}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=7,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=7},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018206,ts_us=1735924018206937,ts_ns=1735924018206937000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735924019169, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=7, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=7} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":8}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=8,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=8},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018207,ts_us=1735924018207004,ts_ns=1735924018207004000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735924019169, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=8, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=8} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":9}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=9,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=9},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018207,ts_us=1735924018207071,ts_ns=1735924018207071000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735924019169, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=9, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=9} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":10}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=10,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=10},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018207,ts_us=1735924018207138,ts_ns=1735924018207138000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735924019169, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=10, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=10} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":11}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=11,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=11},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018207,ts_us=1735924018207213,ts_ns=1735924018207213000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735924019169, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=11, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=11} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":12}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=12,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=12},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018207,ts_us=1735924018207280,ts_ns=1735924018207280000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735924019169, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=12, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=12} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":13}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=13,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=13},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018207,ts_us=1735924018207346,ts_ns=1735924018207346000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735924019169, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=13, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=13} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":14}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=14,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=14},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018207,ts_us=1735924018207408,ts_ns=1735924018207408000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735924019170, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=14, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=14} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":15}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=15,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=15},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018207,ts_us=1735924018207470,ts_ns=1735924018207470000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735924019170, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=15, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=15} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":16}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=16,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=16},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018207,ts_us=1735924018207548,ts_ns=1735924018207548000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735924019170, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=16, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=16} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":17}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=17,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=17},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018207,ts_us=1735924018207611,ts_ns=1735924018207611000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735924019170, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=17, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=17} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":18}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=18,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=18},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018207,ts_us=1735924018207676,ts_ns=1735924018207676000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735924019170, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=18, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=18} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":19}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=19,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=19},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018207,ts_us=1735924018207744,ts_ns=1735924018207744000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735924019171, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=19, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=19} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":20}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=20,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=20},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018207,ts_us=1735924018207812,ts_ns=1735924018207812000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735924019171, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=20, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=20} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":21}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=21,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=21},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018207,ts_us=1735924018207874,ts_ns=1735924018207874000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735924019171, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=21, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=21} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":22}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=22,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=22},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=true,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018207,ts_us=1735924018207934,ts_ns=1735924018207934000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735924019171, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=22, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=22} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":23}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=23,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=23},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=last_in_data_collection,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=product_cost,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018208,ts_us=1735924018208001,ts_ns=1735924018208001000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735924019171, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=23, timestampType=CreateTime, originalTopic=db_.leafy_factory.product_cost, originalKafkaPartition=0, originalKafkaOffset=23} ConnectRecord{topic='db_.leafy_factory.product_cost', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_cost"}],"optional":false,"name":"db_.leafy_factory.product_cost.Key"},"payload":{"id_cost":24}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_cost=24,raw_material_cost_per_product=14.82,overhead_per_product=0.50,total_cost_per_product=15.32,work_id=24},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924113000,snapshot=false,db=leafy_factory,ts_us=1735924113000000,ts_ns=1735924113000000000,table=product_cost,server_id=1,file=mariadb-bin.000001,pos=-57,row=0},op=c,ts_ms=1735924113018,ts_us=1735924113018396,ts_ns=1735924113018396000}, valueSchema=Schema{db_.leafy_factory.product_cost.Envelope:STRUCT}, timestamp=1735924113046, headers=ConnectHeaders(headers=)}] (com.mongodb.kafka.connect.sink.MongoSinkTask:247)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 4}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 4}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,439] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=0, timestamp=1735924019167, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 4}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 4}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,439] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=1, timestamp=1735924019168, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 4}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 4}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,440] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=2, timestamp=1735924019168, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 4}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 4}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,440] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=3, timestamp=1735924019168, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 4}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 4}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,440] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=4, timestamp=1735924019168, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 4}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 4}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,441] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=5, timestamp=1735924019168, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 4}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 4}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,442] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=6, timestamp=1735924019169, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 4}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 4}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,442] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=7, timestamp=1735924019169, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 4}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 4}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,442] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=8, timestamp=1735924019169, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 4}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 4}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,443] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=9, timestamp=1735924019169, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 4}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 4}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,443] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=10, timestamp=1735924019169, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 4}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 4}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,444] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=11, timestamp=1735924019169, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 4}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 4}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,444] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=12, timestamp=1735924019169, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 4}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 4}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,444] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=13, timestamp=1735924019170, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 4}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 4}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,445] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=14, timestamp=1735924019170, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 4}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 4}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,445] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=15, timestamp=1735924019170, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 4}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 4}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,445] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=16, timestamp=1735924019170, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 4}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 4}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,446] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=17, timestamp=1735924019170, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 4}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 4}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,446] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=18, timestamp=1735924019171, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 4}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 4}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,446] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=19, timestamp=1735924019171, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 4}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 4}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,446] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=20, timestamp=1735924019171, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 4}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 4}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,447] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=21, timestamp=1735924019171, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 4}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 4}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,447] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=22, timestamp=1735924019171, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 4}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 4}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,447] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.product_cost', partition=0, offset=23, timestamp=1735924113046, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 4}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 4}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,550] ERROR [mongodb-sink|task-0] Failed to put into the sink the following records: [SinkRecord{kafkaOffset=0, timestampType=CreateTime, originalTopic=db_.leafy_factory.products, originalKafkaPartition=0, originalKafkaOffset=0} ConnectRecord{topic='db_.leafy_factory.products', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_product"}],"optional":false,"name":"db_.leafy_factory.products.Key"},"payload":{"id_product":1}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_product=1,product_name=2 Step ladder,product_description=Two Step Ladder - Folding Small 2 Step Stool 330lbs with Non-Slip Feets, Aluminum Lightweight Metal Step Stool by CHEAGO, Portable Solid Handy Work Ladder for Home, Kitchen, RV, Garage},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=first_in_data_collection,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=products,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018210,ts_us=1735924018210820,ts_ns=1735924018210820000}, valueSchema=Schema{db_.leafy_factory.products.Envelope:STRUCT}, timestamp=1735924019434, headers=ConnectHeaders(headers=)}, SinkRecord{kafkaOffset=1, timestampType=CreateTime, originalTopic=db_.leafy_factory.products, originalKafkaPartition=0, originalKafkaOffset=1} ConnectRecord{topic='db_.leafy_factory.products', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_product"}],"optional":false,"name":"db_.leafy_factory.products.Key"},"payload":{"id_product":2}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_product=2,product_name=Titanium Hammer,product_description=Titanium Hammer With Curved Hickory Handle},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=last_in_data_collection,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=products,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018210,ts_us=1735924018210870,ts_ns=1735924018210870000}, valueSchema=Schema{db_.leafy_factory.products.Envelope:STRUCT}, timestamp=1735924019434, headers=ConnectHeaders(headers=)}] (com.mongodb.kafka.connect.sink.MongoSinkTask:247)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 91}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 91}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,552] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.products', partition=0, offset=0, timestamp=1735924019434, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 91}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 91}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,553] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.products', partition=0, offset=1, timestamp=1735924019434, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 91}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 91}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,699] ERROR [mongodb-sink|task-0] Failed to put into the sink the following records: [SinkRecord{kafkaOffset=0, timestampType=CreateTime, originalTopic=db_.leafy_factory.jobs, originalKafkaPartition=0, originalKafkaOffset=0} ConnectRecord{topic='db_.leafy_factory.jobs', kafkaPartition=0, key={"schema":{"type":"struct","fields":[{"type":"int32","optional":false,"field":"id_job"}],"optional":false,"name":"db_.leafy_factory.jobs.Key"},"payload":{"id_job":1}}, keySchema=Schema{STRING}, value=Struct{after=Struct{id_job=1,target_output=10,nOk_products=2,quality_rate=98,job_status=Completed,creation_date=1735568659000,work_id=1},source=Struct{version=3.0.5.Final,connector=mysql,name=db_,ts_ms=1735924018000,snapshot=last_in_data_collection,db=leafy_factory,ts_us=1735924018000000,ts_ns=1735924018000000000,table=jobs,server_id=0,file=mariadb-bin.000001,pos=58880,row=0},op=r,ts_ms=1735924018201,ts_us=1735924018201675,ts_ns=1735924018201675000}, valueSchema=Schema{db_.leafy_factory.jobs.Envelope:STRUCT}, timestamp=1735924018764, headers=ConnectHeaders(headers=)}] (com.mongodb.kafka.connect.sink.MongoSinkTask:247)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 100}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 100}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:10:50,700] ERROR [mongodb-sink|task-0] Error encountered in task mongodb-sink-0. Executing stage 'TASK_PUT' with class 'org.apache.kafka.connect.sink.SinkTask', where consumed record is {topic='db_.leafy_factory.jobs', partition=0, offset=0, timestamp=1735924018764, timestampType=CreateTime}. (org.apache.kafka.connect.runtime.errors.LogReporter:70)
com.mongodb.MongoCommandException: Command failed with error 73 (InvalidNamespace): 'Invalid collection name: ${topic}' on server ist-shared-shard-00-01.n0kts.mongodb.net:27017. The full response is {"ok": 0.0, "errmsg": "Invalid collection name: ${topic}", "code": 73, "codeName": "InvalidNamespace", "$clusterTime": {"clusterTime": {"$timestamp": {"t": 1735924250, "i": 100}}, "signature": {"hash": {"$binary": {"base64": "r+8hi2mogvj9+hS44ogNzAM65NI=", "subType": "00"}}, "keyId": 7400446423029776390}}, "operationTime": {"$timestamp": {"t": 1735924250, "i": 100}}}
	at com.mongodb.internal.connection.ProtocolHelper.getCommandFailureException(ProtocolHelper.java:198)
	at com.mongodb.internal.connection.InternalStreamConnection.receiveCommandMessageResponse(InternalStreamConnection.java:413)
	at com.mongodb.internal.connection.InternalStreamConnection.sendAndReceive(InternalStreamConnection.java:337)
	at com.mongodb.internal.connection.UsageTrackingInternalConnection.sendAndReceive(UsageTrackingInternalConnection.java:116)
	at com.mongodb.internal.connection.DefaultConnectionPool$PooledConnection.sendAndReceive(DefaultConnectionPool.java:643)
	at com.mongodb.internal.connection.CommandProtocolImpl.execute(CommandProtocolImpl.java:71)
	at com.mongodb.internal.connection.DefaultServer$DefaultServerProtocolExecutor.execute(DefaultServer.java:240)
	at com.mongodb.internal.connection.DefaultServerConnection.executeProtocol(DefaultServerConnection.java:226)
	at com.mongodb.internal.connection.DefaultServerConnection.command(DefaultServerConnection.java:126)
	at com.mongodb.internal.connection.DefaultServer$OperationCountTrackingConnection.command(DefaultServer.java:354)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeCommand(MixedBulkWriteOperation.java:517)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.executeBulkWriteBatch(MixedBulkWriteOperation.java:379)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$2(MixedBulkWriteOperation.java:300)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$2(OperationHelper.java:566)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.lambda$withSourceAndConnection$3(OperationHelper.java:565)
	at com.mongodb.internal.operation.OperationHelper.withSuppliedResource(OperationHelper.java:591)
	at com.mongodb.internal.operation.OperationHelper.withSourceAndConnection(OperationHelper.java:564)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.lambda$execute$3(MixedBulkWriteOperation.java:272)
	at com.mongodb.internal.async.function.RetryingSyncSupplier.get(RetryingSyncSupplier.java:65)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:308)
	at com.mongodb.internal.operation.MixedBulkWriteOperation.execute(MixedBulkWriteOperation.java:85)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:212)
	at com.mongodb.client.internal.MongoCollectionImpl.executeBulkWrite(MongoCollectionImpl.java:448)
	at com.mongodb.client.internal.MongoCollectionImpl.bulkWrite(MongoCollectionImpl.java:428)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.bulkWriteBatch(StartedMongoSinkTask.java:162)
	at com.mongodb.kafka.connect.sink.StartedMongoSinkTask.put(StartedMongoSinkTask.java:118)
	at com.mongodb.kafka.connect.sink.MongoSinkTask.put(MongoSinkTask.java:99)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:317)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
	at java.base/java.lang.Thread.run(Thread.java:1575)
[2025-01-03 11:19:08,342] INFO [0:0:0:0:0:0:0:1] - - [03/Jan/2025:17:19:08 +0000] "GET /connectors/mongodb-sink/status HTTP/1.1" 200 168 "-" "curl/8.7.1" 9 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-01-03 11:27:48,817] INFO Successfully processed removal of connector 'mongodb-sink' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2025-01-03 11:27:48,817] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Connector mongodb-sink config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2432)
[2025-01-03 11:27:48,818] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2025-01-03 11:27:48,818] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 11:27:48,820] INFO [0:0:0:0:0:0:0:1] - - [03/Jan/2025:17:27:48 +0000] "DELETE /connectors/mongodb-sink HTTP/1.1" 204 0 "-" "curl/8.7.1" 29 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-01-03 11:27:48,821] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=6, memberId='connect-192.168.1.5:8083-9840b094-d2a8-462f-a9d3-22e16815ccaa', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2025-01-03 11:27:48,827] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=6, memberId='connect-192.168.1.5:8083-9840b094-d2a8-462f-a9d3-22e16815ccaa', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2025-01-03 11:27:48,828] INFO [mongodb-sink|worker] Stopping connector mongodb-sink (org.apache.kafka.connect.runtime.Worker:452)
[2025-01-03 11:27:48,828] INFO [mongodb-sink|task-0] Stopping task mongodb-sink-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2025-01-03 11:27:48,828] INFO [mongodb-sink|worker] Scheduled shutdown for WorkerConnector{id=mongodb-sink} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2025-01-03 11:27:48,829] INFO [mongodb-sink|task-0] Stopping MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:124)
[2025-01-03 11:27:48,831] INFO [mongodb-sink|worker] Completed shutdown for WorkerConnector{id=mongodb-sink} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2025-01-03 11:27:48,929] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Revoke previously assigned partitions db_.leafy_factory.factories-0, db_.leafy_factory.jobs-0, db_.leafy_factory.jobs_machines-0, db_.leafy_factory.machines-0, db_.leafy_factory.product_cost-0, db_.leafy_factory.production_lines-0, db_.leafy_factory.products-0, db_.leafy_factory.products_raw_materials-0, db_.leafy_factory.raw_materials-0, db_.leafy_factory.work_orders-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:80)
[2025-01-03 11:27:48,930] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Member connector-consumer-mongodb-sink-0-521fa3b0-7973-4197-a1af-b6762725b2f8 sending LeaveGroup request to coordinator 192.168.1.5:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1174)
[2025-01-03 11:27:48,930] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 11:27:48,931] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 11:27:49,281] INFO [mongodb-sink|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:27:49,281] INFO [mongodb-sink|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:27:49,282] INFO [mongodb-sink|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:27:49,282] INFO [mongodb-sink|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:27:49,286] INFO [mongodb-sink|task-0] App info kafka.consumer for connector-consumer-mongodb-sink-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:27:49,290] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2025-01-03 11:27:49,297] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2025-01-03 11:27:49,297] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Joined group at generation 6 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.1.5:8083-9840b094-d2a8-462f-a9d3-22e16815ccaa', leaderUrl='http://192.168.1.5:8083/', offset=9, connectorIds=[mariadb-connector], taskIds=[mariadb-connector-0], revokedConnectorIds=[mongodb-sink], revokedTaskIds=[mongodb-sink-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2025-01-03 11:27:49,298] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 9 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2025-01-03 11:27:49,298] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2025-01-03 11:27:49,299] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2025-01-03 11:27:49,299] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 11:27:49,302] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=7, memberId='connect-192.168.1.5:8083-9840b094-d2a8-462f-a9d3-22e16815ccaa', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2025-01-03 11:27:49,304] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=7, memberId='connect-192.168.1.5:8083-9840b094-d2a8-462f-a9d3-22e16815ccaa', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2025-01-03 11:27:49,304] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Joined group at generation 7 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.1.5:8083-9840b094-d2a8-462f-a9d3-22e16815ccaa', leaderUrl='http://192.168.1.5:8083/', offset=9, connectorIds=[mariadb-connector], taskIds=[mariadb-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2025-01-03 11:27:49,304] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 9 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2025-01-03 11:27:49,304] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2025-01-03 11:27:53,446] INFO [0:0:0:0:0:0:0:1] - - [03/Jan/2025:17:27:53 +0000] "GET /connectors/mongodb-sink/status HTTP/1.1" 404 73 "-" "curl/8.7.1" 21 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-01-03 11:28:02,264] INFO [0:0:0:0:0:0:0:1] - - [03/Jan/2025:17:28:02 +0000] "GET /connectors/mariadb-connector/status HTTP/1.1" 200 175 "-" "curl/8.7.1" 3 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-01-03 11:28:41,516] INFO MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = 
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = db_.leafy_factory.work_orders
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 11:28:41,519] INFO MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = 
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = db_.leafy_factory.jobs
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 11:28:41,531] INFO MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = 
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = db_.leafy_factory.work_orders
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 11:28:41,533] INFO MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = 
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = db_.leafy_factory.jobs
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 11:28:41,542] INFO MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync", "version": "4.7.2"}, "os": {"type": "Darwin", "name": "Mac OS X", "architecture": "aarch64", "version": "15.0"}, "platform": "Java/Oracle Corporation/23+37-2369", "application": {"name": "IST-Shared"}} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=majority, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=MongoCredential{mechanism=null, userName='leafy_factory', source='admin', password=<hidden>, mechanismProperties=<hidden>}, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@19aff91f]}, clusterSettings={hosts=[127.0.0.1:27017], srvHost=ist-shared.n0kts.mongodb.net, srvServiceName=mongodb, mode=MULTIPLE, requiredClusterType=REPLICA_SET, requiredReplicaSetName='atlas-133bp7-shard-0', serverSelector='null', clusterListeners='[com.mongodb.kafka.connect.util.ConnectionValidator$1@3a80da0]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=true, invalidHostNameAllowed=false, context=null}, applicationName='IST-Shared', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null} (org.mongodb.driver.client:71)
[2025-01-03 11:28:41,699] INFO Adding discovered server ist-shared-shard-00-01.n0kts.mongodb.net:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2025-01-03 11:28:41,700] INFO Adding discovered server ist-shared-shard-00-02.n0kts.mongodb.net:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2025-01-03 11:28:41,701] INFO Adding discovered server ist-shared-shard-00-00.n0kts.mongodb.net:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2025-01-03 11:28:42,866] INFO Opened connection [connectionId{localValue:19, serverValue:1159419}] to ist-shared-shard-00-00.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:28:42,866] INFO Opened connection [connectionId{localValue:20, serverValue:1159420}] to ist-shared-shard-00-00.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:28:42,867] INFO Monitor thread successfully connected to server with description ServerDescription{address=ist-shared-shard-00-00.n0kts.mongodb.net:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=484392250, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-00.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az1'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=null, setVersion=71, topologyVersion=TopologyVersion{processId=675b24d1d1431a587be694a8, counter=19}, lastWriteDate=Fri Jan 03 11:28:42 CST 2025, lastUpdateTimeNanos=48373318825500} (org.mongodb.driver.cluster:71)
[2025-01-03 11:28:42,869] INFO No server chosen by ReadPreferenceServerSelector{readPreference=primary} from cluster description ClusterDescription{type=REPLICA_SET, connectionMode=MULTIPLE, serverDescriptions=[ServerDescription{address=ist-shared-shard-00-02.n0kts.mongodb.net:27017, type=UNKNOWN, state=CONNECTING}, ServerDescription{address=ist-shared-shard-00-01.n0kts.mongodb.net:27017, type=UNKNOWN, state=CONNECTING}, ServerDescription{address=ist-shared-shard-00-00.n0kts.mongodb.net:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=484392250, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-00.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az1'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=null, setVersion=71, topologyVersion=TopologyVersion{processId=675b24d1d1431a587be694a8, counter=19}, lastWriteDate=Fri Jan 03 11:28:42 CST 2025, lastUpdateTimeNanos=48373318825500}]}. Waiting for 30000 ms before timing out (org.mongodb.driver.cluster:71)
[2025-01-03 11:28:42,895] INFO Opened connection [connectionId{localValue:17, serverValue:700185}] to ist-shared-shard-00-02.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:28:42,896] INFO Monitor thread successfully connected to server with description ServerDescription{address=ist-shared-shard-00-02.n0kts.mongodb.net:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=513982167, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-02.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az4'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=null, setVersion=71, topologyVersion=TopologyVersion{processId=67694a2575e55e11df78dcd1, counter=14}, lastWriteDate=Fri Jan 03 11:28:42 CST 2025, lastUpdateTimeNanos=48373347797708} (org.mongodb.driver.cluster:71)
[2025-01-03 11:28:42,903] INFO Opened connection [connectionId{localValue:18, serverValue:700184}] to ist-shared-shard-00-02.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:28:43,162] INFO Opened connection [connectionId{localValue:15, serverValue:1296474}] to ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:28:43,641] INFO Opened connection [connectionId{localValue:16, serverValue:1296475}] to ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:28:43,642] INFO Monitor thread successfully connected to server with description ServerDescription{address=ist-shared-shard-00-01.n0kts.mongodb.net:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=939556500, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-01.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az2'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=7fffffff000000000000010a, setVersion=71, topologyVersion=TopologyVersion{processId=67694b4e6118cd58a4791747, counter=41}, lastWriteDate=Fri Jan 03 11:28:43 CST 2025, lastUpdateTimeNanos=48374093546166} (org.mongodb.driver.cluster:71)
[2025-01-03 11:28:43,642] INFO Setting max election id to 7fffffff000000000000010a from replica set primary ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.cluster:71)
[2025-01-03 11:28:43,643] INFO Setting max set version to 71 from replica set primary ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.cluster:71)
[2025-01-03 11:28:43,643] INFO Discovered replica set primary ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.cluster:71)
[2025-01-03 11:28:44,782] INFO Opened connection [connectionId{localValue:21, serverValue:1296476}] to ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:28:45,165] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:371)
[2025-01-03 11:28:45,178] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Connector mongodb-sink config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2025-01-03 11:28:45,178] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2025-01-03 11:28:45,178] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 11:28:45,180] INFO [0:0:0:0:0:0:0:1] - - [03/Jan/2025:17:28:41 +0000] "POST /connectors HTTP/1.1" 201 863 "-" "curl/8.7.1" 3880 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-01-03 11:28:45,181] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=8, memberId='connect-192.168.1.5:8083-9840b094-d2a8-462f-a9d3-22e16815ccaa', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2025-01-03 11:28:45,184] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=8, memberId='connect-192.168.1.5:8083-9840b094-d2a8-462f-a9d3-22e16815ccaa', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2025-01-03 11:28:45,184] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Joined group at generation 8 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.1.5:8083-9840b094-d2a8-462f-a9d3-22e16815ccaa', leaderUrl='http://192.168.1.5:8083/', offset=10, connectorIds=[mongodb-sink, mariadb-connector], taskIds=[mariadb-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2025-01-03 11:28:45,185] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 10 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2025-01-03 11:28:45,185] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connector mongodb-sink (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2025-01-03 11:28:45,185] INFO [mongodb-sink|worker] Creating connector mongodb-sink of type com.mongodb.kafka.connect.MongoSinkConnector (org.apache.kafka.connect.runtime.Worker:313)
[2025-01-03 11:28:45,186] INFO [mongodb-sink|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = true
	errors.log.include.messages = true
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = all
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongodb-sink
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [db_.leafy_factory.work_orders, db_.leafy_factory.jobs]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2025-01-03 11:28:45,186] INFO [mongodb-sink|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = true
	errors.log.include.messages = true
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = all
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongodb-sink
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [db_.leafy_factory.work_orders, db_.leafy_factory.jobs]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 11:28:45,187] INFO [mongodb-sink|worker] Instantiated connector mongodb-sink with version 1.14.1 of type class com.mongodb.kafka.connect.MongoSinkConnector (org.apache.kafka.connect.runtime.Worker:335)
[2025-01-03 11:28:45,188] INFO [mongodb-sink|worker] Finished creating connector mongodb-sink (org.apache.kafka.connect.runtime.Worker:356)
[2025-01-03 11:28:45,188] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2025-01-03 11:28:45,189] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = true
	errors.log.include.messages = true
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = all
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongodb-sink
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [db_.leafy_factory.work_orders, db_.leafy_factory.jobs]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2025-01-03 11:28:45,189] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = true
	errors.log.include.messages = true
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = all
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongodb-sink
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [db_.leafy_factory.work_orders, db_.leafy_factory.jobs]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 11:28:45,195] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Tasks [mongodb-sink-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2025-01-03 11:28:45,196] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2025-01-03 11:28:45,196] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 11:28:45,196] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=9, memberId='connect-192.168.1.5:8083-9840b094-d2a8-462f-a9d3-22e16815ccaa', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2025-01-03 11:28:45,198] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=9, memberId='connect-192.168.1.5:8083-9840b094-d2a8-462f-a9d3-22e16815ccaa', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2025-01-03 11:28:45,198] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Joined group at generation 9 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.1.5:8083-9840b094-d2a8-462f-a9d3-22e16815ccaa', leaderUrl='http://192.168.1.5:8083/', offset=12, connectorIds=[mongodb-sink, mariadb-connector], taskIds=[mongodb-sink-0, mariadb-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2025-01-03 11:28:45,198] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 12 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2025-01-03 11:28:45,198] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting task mongodb-sink-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2025-01-03 11:28:45,198] INFO [mongodb-sink|task-0] Creating task mongodb-sink-0 (org.apache.kafka.connect.runtime.Worker:646)
[2025-01-03 11:28:45,199] INFO [mongodb-sink|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = true
	errors.log.include.messages = true
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = all
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongodb-sink
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2025-01-03 11:28:45,199] INFO [mongodb-sink|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = true
	errors.log.include.messages = true
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = all
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongodb-sink
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 11:28:45,199] INFO [mongodb-sink|task-0] TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.sink.MongoSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2025-01-03 11:28:45,199] INFO [mongodb-sink|task-0] Instantiated task mongodb-sink-0 with version 1.14.1 of type com.mongodb.kafka.connect.sink.MongoSinkTask (org.apache.kafka.connect.runtime.Worker:665)
[2025-01-03 11:28:45,199] INFO [mongodb-sink|task-0] StringConverterConfig values: 
	converter.encoding = UTF-8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:371)
[2025-01-03 11:28:45,199] INFO [mongodb-sink|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 11:28:45,199] INFO [mongodb-sink|task-0] Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task mongodb-sink-0 using the connector config (org.apache.kafka.connect.runtime.Worker:680)
[2025-01-03 11:28:45,199] INFO [mongodb-sink|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongodb-sink-0 using the connector config (org.apache.kafka.connect.runtime.Worker:686)
[2025-01-03 11:28:45,199] INFO [mongodb-sink|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongodb-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2025-01-03 11:28:45,200] INFO [mongodb-sink|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:1795)
[2025-01-03 11:28:45,200] INFO [mongodb-sink|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = true
	errors.log.include.messages = true
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = all
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongodb-sink
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [db_.leafy_factory.work_orders, db_.leafy_factory.jobs]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2025-01-03 11:28:45,200] INFO [mongodb-sink|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = true
	errors.log.include.messages = true
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = all
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongodb-sink
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [db_.leafy_factory.work_orders, db_.leafy_factory.jobs]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 11:28:45,200] INFO [mongodb-sink|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mongodb-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mongodb-sink
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 11:28:45,201] INFO [mongodb-sink|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:28:45,203] INFO [mongodb-sink|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2025-01-03 11:28:45,203] INFO [mongodb-sink|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:28:45,203] INFO [mongodb-sink|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:28:45,203] INFO [mongodb-sink|task-0] Kafka startTimeMs: 1735925325203 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:28:45,204] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2025-01-03 11:28:45,204] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Subscribed to topic(s): db_.leafy_factory.work_orders, db_.leafy_factory.jobs (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:481)
[2025-01-03 11:28:45,204] INFO [mongodb-sink|task-0] Starting MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:66)
[2025-01-03 11:28:45,216] INFO [mongodb-sink|task-0] MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = 
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = db_.leafy_factory.work_orders
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 11:28:45,216] INFO [mongodb-sink|task-0] MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = 
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = db_.leafy_factory.jobs
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 11:28:45,217] INFO [mongodb-sink|task-0] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|mongo-kafka|sink", "version": "4.7.2|1.14.1"}, "os": {"type": "Darwin", "name": "Mac OS X", "architecture": "aarch64", "version": "15.0"}, "platform": "Java/Oracle Corporation/23+37-2369", "application": {"name": "IST-Shared"}} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=majority, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=MongoCredential{mechanism=null, userName='leafy_factory', source='admin', password=<hidden>, mechanismProperties=<hidden>}, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@19aff91f]}, clusterSettings={hosts=[127.0.0.1:27017], srvHost=ist-shared.n0kts.mongodb.net, srvServiceName=mongodb, mode=MULTIPLE, requiredClusterType=REPLICA_SET, requiredReplicaSetName='atlas-133bp7-shard-0', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=true, invalidHostNameAllowed=false, context=null}, applicationName='IST-Shared', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null} (org.mongodb.driver.client:71)
[2025-01-03 11:28:45,217] INFO [mongodb-sink|task-0] WorkerSinkTask{id=mongodb-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:325)
[2025-01-03 11:28:45,217] INFO [mongodb-sink|task-0] WorkerSinkTask{id=mongodb-sink-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:211)
[2025-01-03 11:28:45,221] INFO [mongodb-sink|task-0] Adding discovered server ist-shared-shard-00-01.n0kts.mongodb.net:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2025-01-03 11:28:45,221] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Cluster ID: l71C-upmSGSQL-StD83PcA (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:28:45,221] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Discovered group coordinator 192.168.1.5:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:937)
[2025-01-03 11:28:45,222] INFO [mongodb-sink|task-0] Adding discovered server ist-shared-shard-00-02.n0kts.mongodb.net:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2025-01-03 11:28:45,222] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:605)
[2025-01-03 11:28:45,222] INFO [mongodb-sink|task-0] Adding discovered server ist-shared-shard-00-00.n0kts.mongodb.net:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2025-01-03 11:28:45,229] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Request joining group due to: need to re-join with the given member-id: connector-consumer-mongodb-sink-0-47e9593d-370a-45d3-9ae2-b2a40c723706 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 11:28:45,229] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:605)
[2025-01-03 11:28:45,230] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Successfully joined group with generation Generation{generationId=4, memberId='connector-consumer-mongodb-sink-0-47e9593d-370a-45d3-9ae2-b2a40c723706', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:666)
[2025-01-03 11:28:45,231] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Finished assignment for group at generation 4: {connector-consumer-mongodb-sink-0-47e9593d-370a-45d3-9ae2-b2a40c723706=Assignment(partitions=[db_.leafy_factory.work_orders-0, db_.leafy_factory.jobs-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:664)
[2025-01-03 11:28:45,232] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Successfully synced group in generation Generation{generationId=4, memberId='connector-consumer-mongodb-sink-0-47e9593d-370a-45d3-9ae2-b2a40c723706', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:843)
[2025-01-03 11:28:45,233] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Notifying assignor about the new Assignment(partitions=[db_.leafy_factory.work_orders-0, db_.leafy_factory.jobs-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:324)
[2025-01-03 11:28:45,233] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Adding newly assigned partitions: db_.leafy_factory.jobs-0, db_.leafy_factory.work_orders-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:58)
[2025-01-03 11:28:45,236] INFO [mongodb-sink|task-0] Setting offset for partition db_.leafy_factory.work_orders-0 to the committed offset FetchPosition{offset=24, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerUtils:209)
[2025-01-03 11:28:45,237] INFO [mongodb-sink|task-0] Setting offset for partition db_.leafy_factory.jobs-0 to the committed offset FetchPosition{offset=1, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}} (org.apache.kafka.clients.consumer.internals.ConsumerUtils:209)
[2025-01-03 11:28:45,671] INFO [mongodb-sink|task-0] Opened connection [connectionId{localValue:22, serverValue:1296482}] to ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:28:45,671] INFO [mongodb-sink|task-0] Monitor thread successfully connected to server with description ServerDescription{address=ist-shared-shard-00-01.n0kts.mongodb.net:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=269811959, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-01.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az2'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=7fffffff000000000000010a, setVersion=71, topologyVersion=TopologyVersion{processId=67694b4e6118cd58a4791747, counter=41}, lastWriteDate=Fri Jan 03 11:28:45 CST 2025, lastUpdateTimeNanos=48376123157166} (org.mongodb.driver.cluster:71)
[2025-01-03 11:28:45,672] INFO [mongodb-sink|task-0] Setting max election id to 7fffffff000000000000010a from replica set primary ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.cluster:71)
[2025-01-03 11:28:45,672] INFO [mongodb-sink|task-0] Setting max set version to 71 from replica set primary ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.cluster:71)
[2025-01-03 11:28:45,672] INFO [mongodb-sink|task-0] Discovered replica set primary ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.cluster:71)
[2025-01-03 11:28:45,684] INFO [mongodb-sink|task-0] Opened connection [connectionId{localValue:27, serverValue:1159423}] to ist-shared-shard-00-00.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:28:45,684] INFO [mongodb-sink|task-0] Monitor thread successfully connected to server with description ServerDescription{address=ist-shared-shard-00-00.n0kts.mongodb.net:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=276097709, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-00.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az1'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=null, setVersion=71, topologyVersion=TopologyVersion{processId=675b24d1d1431a587be694a8, counter=19}, lastWriteDate=Fri Jan 03 11:28:45 CST 2025, lastUpdateTimeNanos=48376136579583} (org.mongodb.driver.cluster:71)
[2025-01-03 11:28:45,684] INFO [mongodb-sink|task-0] Opened connection [connectionId{localValue:26, serverValue:1159424}] to ist-shared-shard-00-00.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:28:45,742] INFO [mongodb-sink|task-0] Opened connection [connectionId{localValue:25, serverValue:700189}] to ist-shared-shard-00-02.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:28:45,752] INFO [mongodb-sink|task-0] Opened connection [connectionId{localValue:24, serverValue:700188}] to ist-shared-shard-00-02.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:28:45,752] INFO [mongodb-sink|task-0] Monitor thread successfully connected to server with description ServerDescription{address=ist-shared-shard-00-02.n0kts.mongodb.net:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=361762416, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-02.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az4'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=null, setVersion=71, topologyVersion=TopologyVersion{processId=67694a2575e55e11df78dcd1, counter=14}, lastWriteDate=Fri Jan 03 11:28:45 CST 2025, lastUpdateTimeNanos=48376204162958} (org.mongodb.driver.cluster:71)
[2025-01-03 11:28:45,810] INFO [mongodb-sink|task-0] Opened connection [connectionId{localValue:23, serverValue:1296481}] to ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:35:45,037] INFO [mariadb-connector|task-0] 6 records sent during previous 00:27:11.999, last recorded offset of {server=db_} partition is {ts_sec=1735925744, file=mariadb-bin.000001, pos=0, row=1, server_id=1} (io.debezium.connector.common.BaseSourceTask:351)
[2025-01-03 11:35:46,064] INFO [mongodb-sink|task-0] Opened connection [connectionId{localValue:28, serverValue:1297026}] to ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:35:47,989] INFO [mariadb-connector|task-0|offsets] WorkerSourceTask{id=mariadb-connector-0} Committing offsets for 6 acknowledged messages (org.apache.kafka.connect.runtime.WorkerSourceTask:236)
[2025-01-03 11:36:24,496] INFO [0:0:0:0:0:0:0:1] - - [03/Jan/2025:17:36:24 +0000] "GET /connectors/mongodb-sink/status HTTP/1.1" 200 168 "-" "curl/8.7.1" 6 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-01-03 11:39:06,074] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:87)
[2025-01-03 11:39:06,076] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:358)
[2025-01-03 11:39:06,101] INFO Stopped http_8083@66b31d46{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:383)
[2025-01-03 11:39:06,101] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:149)
[2025-01-03 11:39:06,117] INFO Stopped o.e.j.s.ServletContextHandler@2c15034f{/,null,STOPPED} (org.eclipse.jetty.server.handler.ContextHandler:1159)
[2025-01-03 11:39:06,118] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:387)
[2025-01-03 11:39:06,118] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder stopping (org.apache.kafka.connect.runtime.distributed.DistributedHerder:851)
[2025-01-03 11:39:06,118] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Stopping connectors and tasks that are still assigned to this worker. (org.apache.kafka.connect.runtime.distributed.DistributedHerder:808)
[2025-01-03 11:39:06,118] INFO [mariadb-connector|worker] Stopping connector mariadb-connector (org.apache.kafka.connect.runtime.Worker:452)
[2025-01-03 11:39:06,119] INFO [mariadb-connector|worker] Scheduled shutdown for WorkerConnector{id=mariadb-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2025-01-03 11:39:06,119] INFO [mongodb-sink|worker] Stopping connector mongodb-sink (org.apache.kafka.connect.runtime.Worker:452)
[2025-01-03 11:39:06,119] INFO [mongodb-sink|worker] Scheduled shutdown for WorkerConnector{id=mongodb-sink} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2025-01-03 11:39:06,120] INFO [mongodb-sink|worker] Completed shutdown for WorkerConnector{id=mongodb-sink} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2025-01-03 11:39:06,120] INFO [mariadb-connector|worker] Completed shutdown for WorkerConnector{id=mariadb-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2025-01-03 11:39:06,122] INFO [mariadb-connector|task-0] Stopping task mariadb-connector-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2025-01-03 11:39:06,122] INFO [mongodb-sink|task-0] Stopping task mongodb-sink-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2025-01-03 11:39:06,122] INFO [mongodb-sink|task-0] Stopping MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:124)
[2025-01-03 11:39:06,224] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Revoke previously assigned partitions db_.leafy_factory.jobs-0, db_.leafy_factory.work_orders-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:80)
[2025-01-03 11:39:06,225] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Member connector-consumer-mongodb-sink-0-47e9593d-370a-45d3-9ae2-b2a40c723706 sending LeaveGroup request to coordinator 192.168.1.5:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1174)
[2025-01-03 11:39:06,225] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 11:39:06,225] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 11:39:06,313] INFO [mongodb-sink|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:39:06,313] INFO [mongodb-sink|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:39:06,313] INFO [mongodb-sink|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:39:06,313] INFO [mongodb-sink|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:39:06,315] INFO [mongodb-sink|task-0] App info kafka.consumer for connector-consumer-mongodb-sink-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:39:06,526] INFO [mariadb-connector|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:434)
[2025-01-03 11:39:06,536] INFO [mariadb-connector|task-0] Finished streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:325)
[2025-01-03 11:39:06,537] INFO [mariadb-connector|task-0] Stopped reading binlog after 84 events, last recorded offset: {ts_sec=1735925744, file=mariadb-bin.000001, pos=65386, server_id=1, event=1} (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:1218)
[2025-01-03 11:39:06,538] INFO [mariadb-connector|task-0] SignalProcessor stopped (io.debezium.pipeline.signal.SignalProcessor:127)
[2025-01-03 11:39:06,539] INFO [mariadb-connector|task-0] Debezium ServiceRegistry stopped. (io.debezium.service.DefaultServiceRegistry:105)
[2025-01-03 11:39:06,544] INFO [mariadb-connector|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:983)
[2025-01-03 11:39:06,545] INFO [mariadb-connector|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:983)
[2025-01-03 11:39:06,545] INFO [mariadb-connector|task-0] [Producer clientId=db_-schemahistory] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 11:39:06,548] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:39:06,548] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:39:06,548] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:39:06,548] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:39:06,549] INFO [mariadb-connector|task-0] App info kafka.producer for db_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:39:06,550] INFO [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 11:39:06,553] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:39:06,553] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:39:06,553] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:39:06,553] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:39:06,554] INFO [mariadb-connector|task-0] App info kafka.producer for connector-producer-mariadb-connector-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:39:06,556] INFO Stopping KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:317)
[2025-01-03 11:39:06,557] INFO [Producer clientId=connect-cluster-statuses] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 11:39:06,560] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:39:06,560] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:39:06,560] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:39:06,560] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:39:06,561] INFO App info kafka.producer for connect-cluster-statuses unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:39:06,561] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 11:39:06,561] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 11:39:06,563] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Node 0 sent an invalid full fetch response with extraIds=(xZZWLxkeRv-_bXKBjngb7w), response=() (org.apache.kafka.clients.FetchSessionHandler:556)
[2025-01-03 11:39:06,565] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:39:06,565] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:39:06,565] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:39:06,565] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:39:06,567] INFO App info kafka.consumer for connect-cluster-statuses unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:39:06,567] INFO Stopped KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:341)
[2025-01-03 11:39:06,567] INFO Closing KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:407)
[2025-01-03 11:39:06,568] INFO Stopping KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:317)
[2025-01-03 11:39:06,568] INFO [Producer clientId=connect-cluster-configs] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 11:39:06,569] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:39:06,569] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:39:06,569] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:39:06,569] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:39:06,569] INFO App info kafka.producer for connect-cluster-configs unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:39:06,569] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 11:39:06,570] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 11:39:06,653] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:39:06,653] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:39:06,654] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:39:06,654] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:39:06,654] INFO App info kafka.consumer for connect-cluster-configs unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:39:06,655] INFO Stopped KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:341)
[2025-01-03 11:39:06,655] INFO Closed KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:412)
[2025-01-03 11:39:06,655] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:250)
[2025-01-03 11:39:06,655] INFO Stopping KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:261)
[2025-01-03 11:39:06,655] INFO Stopping KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:317)
[2025-01-03 11:39:06,656] INFO [Producer clientId=connect-cluster-offsets] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2025-01-03 11:39:06,657] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:39:06,657] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:39:06,657] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:39:06,657] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:39:06,657] INFO App info kafka.producer for connect-cluster-offsets unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:39:06,657] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 11:39:06,657] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 11:39:07,156] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:39:07,157] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:39:07,157] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:39:07,157] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:39:07,160] INFO App info kafka.consumer for connect-cluster-offsets unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:39:07,160] INFO Stopped KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:341)
[2025-01-03 11:39:07,160] INFO Stopped KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:263)
[2025-01-03 11:39:07,160] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:39:07,160] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:39:07,160] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:39:07,161] INFO App info kafka.connect for 192.168.1.5:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:39:07,161] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:271)
[2025-01-03 11:39:07,162] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Member connect-192.168.1.5:8083-9840b094-d2a8-462f-a9d3-22e16815ccaa sending LeaveGroup request to coordinator 192.168.1.5:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1174)
[2025-01-03 11:39:07,163] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1056)
[2025-01-03 11:39:07,163] WARN [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Close timed out with 1 pending requests to coordinator, terminating client connections (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1141)
[2025-01-03 11:39:07,164] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:39:07,164] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:39:07,164] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:39:07,166] INFO App info kafka.connect for connect-192.168.1.5:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:39:07,167] INFO App info kafka.admin.client for connect-cluster-shared-admin unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:39:07,169] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:39:07,169] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:39:07,169] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:39:07,170] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder stopped (org.apache.kafka.connect.runtime.distributed.DistributedHerder:394)
[2025-01-03 11:39:07,171] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder stopped (org.apache.kafka.connect.runtime.distributed.DistributedHerder:858)
[2025-01-03 11:39:07,171] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:92)
[2025-01-03 11:42:05,725] INFO Kafka Connect worker initializing ... (org.apache.kafka.connect.cli.AbstractConnectCli:114)
[2025-01-03 11:42:05,727] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote=true, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../logs, -Dlog4j.configuration=file:bin/../config/connect-log4j.properties
	jvm.spec = Oracle Corporation, Java HotSpot(TM) 64-Bit Server VM, 23, 23+37-2369
	jvm.classpath = /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/activation-1.1.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/aopalliance-repackaged-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/argparse4j-0.7.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/audience-annotations-0.12.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/caffeine-2.9.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-beanutils-1.9.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-cli-1.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-collections-3.2.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-digester-2.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-io-2.14.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-lang3-3.12.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-logging-1.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/commons-validator-1.7.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-basic-auth-extension-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-json-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-mirror-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-mirror-client-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-runtime-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/connect-transforms-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/error_prone_annotations-2.10.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-api-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-locator-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/hk2-utils-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-annotations-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-core-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-databind-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-dataformat-csv-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-datatype-jdk8-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-jaxrs-base-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-jaxrs-json-provider-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-afterburner-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-jaxb-annotations-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jackson-module-scala_2.13-2.16.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.activation-api-1.2.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.annotation-api-1.3.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.inject-2.6.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.validation-api-2.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jakarta.xml.bind-api-2.3.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javassist-3.29.2-GA.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.activation-api-1.2.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.annotation-api-1.3.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.servlet-api-3.1.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/javax.ws.rs-api-2.1.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jaxb-api-2.3.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-client-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-common-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-container-servlet-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-container-servlet-core-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-hk2-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jersey-server-2.39.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-client-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-continuation-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-http-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-io-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-security-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-server-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-servlet-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-servlets-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-util-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jetty-util-ajax-9.4.56.v20240826.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jline-3.25.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jopt-simple-5.0.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jose4j-0.9.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/jsr305-3.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-clients-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-group-coordinator-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-group-coordinator-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-metadata-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-raft-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-server-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-server-common-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-shell-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-storage-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-storage-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-examples-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-scala_2.13-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-streams-test-utils-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-tools-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-tools-api-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka-transaction-coordinator-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/kafka_2.13-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/lz4-java-1.8.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/maven-artifact-3.9.6.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/metrics-core-2.2.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/metrics-core-4.1.12.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-buffer-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-codec-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-common-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-handler-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-resolver-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-classes-epoll-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-native-epoll-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/netty-transport-native-unix-common-4.1.111.Final.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/opentelemetry-proto-1.0.0-alpha.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/osgi-resource-locator-1.0.3.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/paranamer-2.8.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/pcollections-4.0.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/plexus-utils-3.5.1.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/protobuf-java-3.25.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/reflections-0.10.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/reload4j-1.2.25.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/rocksdbjni-7.9.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-collection-compat_2.13-2.10.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-java8-compat_2.13-1.0.2.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-library-2.13.14.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-logging_2.13-3.9.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/scala-reflect-2.13.14.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/slf4j-api-1.7.36.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/slf4j-reload4j-1.7.36.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/snappy-java-1.1.10.5.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/swagger-annotations-2.2.8.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/trogdor-3.9.0.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zookeeper-3.8.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zookeeper-jute-3.8.4.jar:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/bin/../libs/zstd-jni-1.5.6-4.jar
	os.spec = Mac OS X, aarch64, 15.0
	os.vcpus = 11
 (org.apache.kafka.connect.runtime.WorkerInfo:72)
[2025-01-03 11:42:05,728] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.AbstractConnectCli:120)
[2025-01-03 11:42:05,740] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 11:42:05,771] INFO Using up-to-date JsonConverter implementation (io.debezium.converters.CloudEventsConverter:120)
[2025-01-03 11:42:05,872] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 11:42:05,872] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1 (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 11:42:05,876] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 11:42:05,876] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 11:42:05,887] INFO Using up-to-date JsonConverter implementation (io.debezium.converters.CloudEventsConverter:120)
[2025-01-03 11:42:05,901] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 11:42:05,903] INFO Loading plugin from: classpath (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 11:42:05,905] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@5a07e868 (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 11:42:05,905] INFO Scanning plugins with ServiceLoaderScanner took 166 ms (org.apache.kafka.connect.runtime.isolation.PluginScanner:71)
[2025-01-03 11:42:05,906] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 11:42:06,030] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 11:42:06,031] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1 (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 11:42:06,092] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 11:42:06,092] INFO Loading plugin from: /Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 11:42:06,245] INFO Registered loader: PluginClassLoader{pluginLocation=file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mysql/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 11:42:06,245] INFO Loading plugin from: classpath (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2025-01-03 11:42:06,649] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@5a07e868 (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2025-01-03 11:42:06,650] INFO Scanning plugins with ReflectionScanner took 744 ms (org.apache.kafka.connect.runtime.isolation.PluginScanner:71)
[2025-01-03 11:42:06,651] WARN One or more plugins are missing ServiceLoader manifests may not be usable with plugin.discovery=service_load: [
file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1/	com.mongodb.kafka.connect.MongoSinkConnector	sink	1.14.1
file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/mongodb-kafka-connect-mongodb-1.14.1/	com.mongodb.kafka.connect.MongoSourceConnector	source	1.14.1
file:/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins/debezium-connector-mongodb/	io.debezium.connector.mongodb.MongoDbSinkConnector	sink	3.0.5.Final
]
Read the documentation at https://kafka.apache.org/documentation.html#connect_plugindiscovery for instructions on migrating your plugins to take advantage of the performance improvements of service_load mode. To silence this warning, set plugin.discovery=only_scan in the worker config. (org.apache.kafka.connect.runtime.isolation.Plugins:123)
[2025-01-03 11:42:06,651] INFO Added plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,651] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,651] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,651] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,651] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,651] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,652] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,652] INFO Added plugin 'io.debezium.transforms.HeaderToValue' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,652] INFO Added plugin 'io.debezium.transforms.SchemaChangeEventFilter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,652] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,652] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,652] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,652] INFO Added plugin 'io.debezium.transforms.ExtractSchemaToNewRecord' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,652] INFO Added plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,652] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,652] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,652] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,652] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,652] INFO Added plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,652] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,652] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,652] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,652] INFO Added plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,652] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,652] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,652] INFO Added plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,652] INFO Added plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,653] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,653] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,653] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,653] INFO Added plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,653] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,653] INFO Added plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,653] INFO Added plugin 'io.debezium.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,653] INFO Added plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,653] INFO Added plugin 'io.debezium.transforms.partitions.PartitionRouting' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,653] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,653] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,653] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,653] INFO Added plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,653] INFO Added plugin 'io.debezium.transforms.TimezoneConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,653] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,653] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,653] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,653] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,653] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,653] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,653] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,653] INFO Added plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,653] INFO Added plugin 'io.debezium.transforms.ExtractChangedRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,653] INFO Added plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,654] INFO Added plugin 'io.debezium.connector.mongodb.transforms.outbox.MongoEventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,654] INFO Added plugin 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,654] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,654] INFO Added plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,654] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,654] INFO Added plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,654] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,654] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,654] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,654] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,654] INFO Added plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,654] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,654] INFO Added plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,654] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,654] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,654] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,654] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,654] INFO Added plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,654] INFO Added plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,654] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,654] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2025-01-03 11:42:06,655] INFO Added alias 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,655] INFO Added alias 'ExtractNewDocumentState' to plugin 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,655] INFO Added alias 'JsonConverter' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,655] INFO Added alias 'DebeziumMySql' to plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,655] INFO Added alias 'MongoSourceConnector' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,655] INFO Added alias 'CloudEventsConverter' to plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,655] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,655] INFO Added alias 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,655] INFO Added alias 'DebeziumMySqlConnectRestExtension' to plugin 'io.debezium.connector.mysql.rest.DebeziumMySqlConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,655] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,655] INFO Added alias 'MySql' to plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,655] INFO Added alias 'MongoDbSinkConnector' to plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,655] INFO Added alias 'EnvVar' to plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,655] INFO Added alias 'EnvVarConfigProvider' to plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,655] INFO Added alias 'MongoDb' to plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,656] INFO Added alias 'MirrorCheckpointConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,656] INFO Added alias 'Boolean' to plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,656] INFO Added alias 'MySqlConnector' to plugin 'io.debezium.connector.mysql.MySqlConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,656] INFO Added alias 'NoneConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,656] INFO Added alias 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,656] INFO Added alias 'HeaderToValue' to plugin 'io.debezium.transforms.HeaderToValue' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,656] INFO Added alias 'PartitionRouting' to plugin 'io.debezium.transforms.partitions.PartitionRouting' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,656] INFO Added alias 'MongoDbSink' to plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,656] INFO Added alias 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,656] INFO Added alias 'StringConverter' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,656] INFO Added alias 'MongoSink' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,656] INFO Added alias 'ByLogicalTableRouter' to plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,656] INFO Added alias 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,656] INFO Added alias 'MongoDbConnector' to plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,656] INFO Added alias 'FileConfigProvider' to plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,656] INFO Added alias 'IntegerConverter' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,656] INFO Added alias 'EventRouter' to plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,656] INFO Added alias 'SchemaChangeEventFilter' to plugin 'io.debezium.transforms.SchemaChangeEventFilter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,656] INFO Added alias 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,656] INFO Added alias 'LongConverter' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,656] INFO Added alias 'DropHeaders' to plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,656] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,656] INFO Added alias 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,656] INFO Added alias 'ExtractSchemaToNewRecord' to plugin 'io.debezium.transforms.ExtractSchemaToNewRecord' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,656] INFO Added alias 'BinaryData' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,656] INFO Added alias 'File' to plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,657] INFO Added alias 'FloatConverter' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,657] INFO Added alias 'DirectoryConfigProvider' to plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,657] INFO Added alias 'ReadToInsertEvent' to plugin 'io.debezium.connector.mysql.transforms.ReadToInsertEvent' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,657] INFO Added alias 'ShortConverter' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,657] INFO Added alias 'CloudEvents' to plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,657] INFO Added alias 'ActivateTracingSpan' to plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,657] INFO Added alias 'TimezoneConverter' to plugin 'io.debezium.transforms.TimezoneConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,657] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,657] INFO Added alias 'DoubleConverter' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,657] INFO Added alias 'Simple' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,657] INFO Added alias 'BinaryDataConverter' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,657] INFO Added alias 'AllConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,657] INFO Added alias 'ExtractNewRecordState' to plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,657] INFO Added alias 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,657] INFO Added alias 'DebeziumMongoDb' to plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,657] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,657] INFO Added alias 'MirrorHeartbeatConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,657] INFO Added alias 'InsertHeader' to plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,657] INFO Added alias 'Directory' to plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,657] INFO Added alias 'MirrorSourceConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,657] INFO Added alias 'MongoSource' to plugin 'com.mongodb.kafka.connect.MongoSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,657] INFO Added alias 'MongoSinkConnector' to plugin 'com.mongodb.kafka.connect.MongoSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,657] INFO Added alias 'PrincipalConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,657] INFO Added alias 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,657] INFO Added alias 'DebeziumMongoDbConnectRestExtension' to plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,657] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,657] INFO Added alias 'BooleanConverter' to plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,657] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,657] INFO Added alias 'MongoEventRouter' to plugin 'io.debezium.connector.mongodb.transforms.outbox.MongoEventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,657] INFO Added alias 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,657] INFO Added alias 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,658] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,658] INFO Added alias 'ExtractChangedRecordState' to plugin 'io.debezium.transforms.ExtractChangedRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,658] INFO Added alias 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,658] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,658] INFO Added alias 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2025-01-03 11:42:06,673] INFO DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	auto.include.jmx.reporter = true
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	config.providers = []
	config.storage.replication.factor = 1
	config.storage.topic = connect-configs
	connect.protocol = sessioned
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = All
	exactly.once.source.support = disabled
	group.id = connect-cluster
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	heartbeat.interval.ms = 3000
	inter.worker.key.generation.algorithm = HmacSHA256
	inter.worker.key.size = null
	inter.worker.key.ttl.ms = 3600000
	inter.worker.signature.algorithm = HmacSHA256
	inter.worker.verification.algorithms = [HmacSHA256]
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = [http://:8083]
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 1
	offset.storage.topic = connect-offsets
	plugin.discovery = hybrid_warn
	plugin.path = [/Users/gio.rodriguez/Documents/github_repositories/leafy_factory/backend/kafka_2.13-3.9.0/plugins]
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	status.storage.partitions = 5
	status.storage.replication.factor = 1
	status.storage.topic = connect-status
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000
 (org.apache.kafka.connect.runtime.distributed.DistributedConfig:371)
[2025-01-03 11:42:06,674] INFO Creating Kafka admin client (org.apache.kafka.connect.runtime.WorkerConfig:281)
[2025-01-03 11:42:06,675] INFO AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:371)
[2025-01-03 11:42:06,694] INFO These configurations '[config.storage.topic, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2025-01-03 11:42:06,694] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:42:06,695] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:42:06,695] INFO Kafka startTimeMs: 1735926126694 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:42:06,803] INFO Kafka cluster ID: gSBA4IMHSNGutK6FKyF6Lw (org.apache.kafka.connect.runtime.WorkerConfig:298)
[2025-01-03 11:42:06,804] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:42:06,806] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:42:06,806] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:42:06,806] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:42:06,808] INFO PublicConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	listeners = [http://:8083]
	response.http.headers.config = 
	rest.advertised.host.name = null
	rest.advertised.listener = null
	rest.advertised.port = null
	rest.extension.classes = []
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
 (org.apache.kafka.connect.runtime.rest.RestServerConfig$PublicConfig:371)
[2025-01-03 11:42:06,812] INFO Logging initialized @1332ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:170)
[2025-01-03 11:42:06,823] INFO Added connector for http://:8083 (org.apache.kafka.connect.runtime.rest.RestServer:125)
[2025-01-03 11:42:06,824] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:196)
[2025-01-03 11:42:06,832] INFO jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 23+37-2369 (org.eclipse.jetty.server.Server:375)
[2025-01-03 11:42:06,840] INFO Started http_8083@97cb8dc{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:333)
[2025-01-03 11:42:06,840] INFO Started @1361ms (org.eclipse.jetty.server.Server:415)
[2025-01-03 11:42:06,847] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 11:42:06,848] INFO REST server listening at http://192.168.1.5:8083/, advertising URL http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:216)
[2025-01-03 11:42:06,848] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 11:42:06,848] INFO REST admin endpoints at http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2025-01-03 11:42:06,848] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 11:42:06,848] INFO Setting up All Policy for ConnectorClientConfigOverride. This will allow all client configurations to be overridden (org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy:45)
[2025-01-03 11:42:06,849] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 11:42:06,857] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:42:06,857] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:42:06,857] INFO Kafka startTimeMs: 1735926126857 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:42:06,859] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 11:42:06,859] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 11:42:06,866] INFO Advertised URI: http://192.168.1.5:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2025-01-03 11:42:06,878] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:42:06,878] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:42:06,878] INFO Kafka startTimeMs: 1735926126878 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:42:06,879] INFO Kafka Connect worker initialization took 1153ms (org.apache.kafka.connect.cli.AbstractConnectCli:141)
[2025-01-03 11:42:06,879] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:67)
[2025-01-03 11:42:06,880] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2025-01-03 11:42:06,880] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder starting (org.apache.kafka.connect.runtime.distributed.DistributedHerder:375)
[2025-01-03 11:42:06,881] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:233)
[2025-01-03 11:42:06,881] INFO Starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:232)
[2025-01-03 11:42:06,881] INFO Starting KafkaBasedLog with topic connect-offsets reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog:254)
[2025-01-03 11:42:06,881] INFO AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-shared-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:371)
[2025-01-03 11:42:06,883] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2025-01-03 11:42:06,883] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:42:06,883] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:42:06,883] INFO Kafka startTimeMs: 1735926126883 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:42:06,891] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:238)
[2025-01-03 11:42:06,905] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:334)
[2025-01-03 11:42:06,905] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:339)
[2025-01-03 11:42:06,905] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:132)
[2025-01-03 11:42:07,059] INFO Started o.e.j.s.ServletContextHandler@662e5590{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:921)
[2025-01-03 11:42:07,059] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:303)
[2025-01-03 11:42:07,060] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:77)
[2025-01-03 11:42:07,154] INFO Created topic (name=connect-offsets, numPartitions=25, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:9092 (org.apache.kafka.connect.util.TopicAdmin:416)
[2025-01-03 11:42:07,156] INFO ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-offsets
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 11:42:07,164] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:42:07,170] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 11:42:07,170] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:42:07,170] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:42:07,170] INFO Kafka startTimeMs: 1735926127170 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:42:07,173] INFO [Producer clientId=connect-cluster-offsets] Cluster ID: gSBA4IMHSNGutK6FKyF6Lw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:42:07,173] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-offsets
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 11:42:07,176] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:42:07,185] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2025-01-03 11:42:07,185] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:42:07,185] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:42:07,185] INFO Kafka startTimeMs: 1735926127185 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:42:07,188] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Cluster ID: gSBA4IMHSNGutK6FKyF6Lw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:42:07,190] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Assigned to partition(s): connect-offsets-0, connect-offsets-5, connect-offsets-10, connect-offsets-20, connect-offsets-15, connect-offsets-9, connect-offsets-11, connect-offsets-4, connect-offsets-16, connect-offsets-17, connect-offsets-3, connect-offsets-24, connect-offsets-23, connect-offsets-13, connect-offsets-18, connect-offsets-22, connect-offsets-8, connect-offsets-2, connect-offsets-12, connect-offsets-19, connect-offsets-14, connect-offsets-1, connect-offsets-6, connect-offsets-7, connect-offsets-21 (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:579)
[2025-01-03 11:42:07,191] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:42:07,191] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-5 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:42:07,191] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-10 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:42:07,191] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-20 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:42:07,191] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-15 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:42:07,191] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-9 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:42:07,191] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-11 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:42:07,192] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:42:07,192] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-16 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:42:07,192] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-17 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:42:07,192] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:42:07,192] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-24 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:42:07,192] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-23 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:42:07,192] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-13 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:42:07,192] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-18 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:42:07,192] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-22 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:42:07,192] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-8 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:42:07,192] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:42:07,192] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-12 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:42:07,192] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-19 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:42:07,192] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-14 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:42:07,192] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:42:07,192] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-6 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:42:07,192] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-7 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:42:07,192] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Seeking to earliest offset of partition connect-offsets-21 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:42:07,205] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:42:07,205] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:42:07,206] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:42:07,206] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:42:07,206] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:42:07,206] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:42:07,206] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:42:07,206] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:42:07,206] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:42:07,206] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:42:07,206] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:42:07,206] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:42:07,206] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:42:07,206] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:42:07,206] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:42:07,206] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:42:07,206] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:42:07,206] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:42:07,206] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:42:07,206] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:42:07,206] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:42:07,206] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:42:07,207] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:42:07,207] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:42:07,207] INFO [Consumer clientId=connect-cluster-offsets, groupId=connect-cluster] Resetting offset for partition connect-offsets-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:42:07,207] INFO Finished reading KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:311)
[2025-01-03 11:42:07,207] INFO Started KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:313)
[2025-01-03 11:42:07,207] INFO Finished reading offsets topic and starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:249)
[2025-01-03 11:42:07,208] INFO Worker started (org.apache.kafka.connect.runtime.Worker:243)
[2025-01-03 11:42:07,208] INFO Starting KafkaBasedLog with topic connect-status reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog:254)
[2025-01-03 11:42:07,277] INFO Created topic (name=connect-status, numPartitions=5, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:9092 (org.apache.kafka.connect.util.TopicAdmin:416)
[2025-01-03 11:42:07,277] INFO ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-statuses
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 11:42:07,278] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:42:07,279] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 11:42:07,279] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:42:07,279] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:42:07,279] INFO Kafka startTimeMs: 1735926127279 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:42:07,280] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-statuses
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 11:42:07,280] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:42:07,281] INFO [Producer clientId=connect-cluster-statuses] Cluster ID: gSBA4IMHSNGutK6FKyF6Lw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:42:07,281] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2025-01-03 11:42:07,281] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:42:07,281] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:42:07,281] INFO Kafka startTimeMs: 1735926127281 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:42:07,283] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Cluster ID: gSBA4IMHSNGutK6FKyF6Lw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:42:07,283] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Assigned to partition(s): connect-status-0, connect-status-4, connect-status-1, connect-status-2, connect-status-3 (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:579)
[2025-01-03 11:42:07,283] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:42:07,283] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:42:07,283] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:42:07,283] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:42:07,283] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Seeking to earliest offset of partition connect-status-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:42:07,289] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:42:07,289] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:42:07,289] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:42:07,289] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:42:07,290] INFO [Consumer clientId=connect-cluster-statuses, groupId=connect-cluster] Resetting offset for partition connect-status-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:42:07,290] INFO Finished reading KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:311)
[2025-01-03 11:42:07,290] INFO Started KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:313)
[2025-01-03 11:42:07,291] INFO Starting KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:378)
[2025-01-03 11:42:07,291] INFO Starting KafkaBasedLog with topic connect-configs reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog:254)
[2025-01-03 11:42:07,322] INFO Created topic (name=connect-configs, numPartitions=1, replicationFactor=1, replicasAssignments=null, configs={cleanup.policy=compact}) on brokers at localhost:9092 (org.apache.kafka.connect.util.TopicAdmin:416)
[2025-01-03 11:42:07,323] INFO ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-configs
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 11:42:07,324] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:42:07,325] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, group.id, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 11:42:07,325] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:42:07,325] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:42:07,325] INFO Kafka startTimeMs: 1735926127325 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:42:07,325] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connect-cluster-configs
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-cluster
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 11:42:07,326] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:42:07,327] INFO These configurations '[config.storage.topic, metrics.context.connect.group.id, status.storage.topic, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2025-01-03 11:42:07,327] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:42:07,327] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:42:07,327] INFO Kafka startTimeMs: 1735926127327 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:42:07,329] INFO [Producer clientId=connect-cluster-configs] Cluster ID: gSBA4IMHSNGutK6FKyF6Lw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:42:07,330] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Cluster ID: gSBA4IMHSNGutK6FKyF6Lw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:42:07,331] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Assigned to partition(s): connect-configs-0 (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:579)
[2025-01-03 11:42:07,331] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Seeking to earliest offset of partition connect-configs-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2025-01-03 11:42:07,335] INFO [Consumer clientId=connect-cluster-configs, groupId=connect-cluster] Resetting offset for partition connect-configs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:42:07,335] INFO Finished reading KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:311)
[2025-01-03 11:42:07,335] INFO Started KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:313)
[2025-01-03 11:42:07,335] INFO Started KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:402)
[2025-01-03 11:42:07,339] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Cluster ID: gSBA4IMHSNGutK6FKyF6Lw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:42:07,339] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Discovered group coordinator 192.168.1.5:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:937)
[2025-01-03 11:42:07,340] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2025-01-03 11:42:07,340] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 11:42:07,343] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 11:42:07,344] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=1, memberId='connect-192.168.1.5:8083-00eb67ad-2d9d-41ef-89eb-6147d4c19418', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2025-01-03 11:42:07,349] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=1, memberId='connect-192.168.1.5:8083-00eb67ad-2d9d-41ef-89eb-6147d4c19418', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2025-01-03 11:42:07,349] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Joined group at generation 1 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.1.5:8083-00eb67ad-2d9d-41ef-89eb-6147d4c19418', leaderUrl='http://192.168.1.5:8083/', offset=-1, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2025-01-03 11:42:07,349] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Herder started (org.apache.kafka.connect.runtime.distributed.DistributedHerder:387)
[2025-01-03 11:42:07,349] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connectors and tasks using config offset -1 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2025-01-03 11:42:07,349] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2025-01-03 11:42:07,378] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Session key updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2510)
[2025-01-03 11:42:39,968] INFO Loading the custom source info struct maker plugin: io.debezium.connector.mysql.MySqlSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2025-01-03 11:42:40,402] INFO Using 'SHOW MASTER STATUS' to get binary log status (io.debezium.connector.mysql.jdbc.MySqlConnection:41)
[2025-01-03 11:42:40,405] INFO Successfully tested connection for jdbc:mysql://localhost:3306/?useInformationSchema=true&nullCatalogMeansCurrent=false&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8&zeroDateTimeBehavior=CONVERT_TO_NULL&connectTimeout=30000 with user 'root' (io.debezium.connector.binlog.BinlogConnector:66)
[2025-01-03 11:42:40,407] INFO Connection gracefully closed (io.debezium.jdbc.JdbcConnection:983)
[2025-01-03 11:42:40,408] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:371)
[2025-01-03 11:42:40,413] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Connector mariadb-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2025-01-03 11:42:40,413] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2025-01-03 11:42:40,413] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 11:42:40,415] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=2, memberId='connect-192.168.1.5:8083-00eb67ad-2d9d-41ef-89eb-6147d4c19418', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2025-01-03 11:42:40,417] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=2, memberId='connect-192.168.1.5:8083-00eb67ad-2d9d-41ef-89eb-6147d4c19418', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2025-01-03 11:42:40,418] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Joined group at generation 2 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.1.5:8083-00eb67ad-2d9d-41ef-89eb-6147d4c19418', leaderUrl='http://192.168.1.5:8083/', offset=2, connectorIds=[mariadb-connector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2025-01-03 11:42:40,418] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 2 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2025-01-03 11:42:40,418] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connector mariadb-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2025-01-03 11:42:40,419] INFO [mariadb-connector|worker] Creating connector mariadb-connector of type io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:313)
[2025-01-03 11:42:40,419] INFO [mariadb-connector|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2025-01-03 11:42:40,419] INFO [mariadb-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 11:42:40,421] INFO [mariadb-connector|worker] Instantiated connector mariadb-connector with version 3.0.5.Final of type class io.debezium.connector.mysql.MySqlConnector (org.apache.kafka.connect.runtime.Worker:335)
[2025-01-03 11:42:40,421] INFO [mariadb-connector|worker] Finished creating connector mariadb-connector (org.apache.kafka.connect.runtime.Worker:356)
[2025-01-03 11:42:40,421] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2025-01-03 11:42:40,424] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2025-01-03 11:42:40,424] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 11:42:40,427] INFO [0:0:0:0:0:0:0:1] - - [03/Jan/2025:17:42:39 +0000] "POST /connectors HTTP/1.1" 201 681 "-" "curl/8.7.1" 526 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-01-03 11:42:40,432] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Tasks [mariadb-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2025-01-03 11:42:40,433] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2025-01-03 11:42:40,433] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 11:42:40,434] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=3, memberId='connect-192.168.1.5:8083-00eb67ad-2d9d-41ef-89eb-6147d4c19418', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2025-01-03 11:42:40,436] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=3, memberId='connect-192.168.1.5:8083-00eb67ad-2d9d-41ef-89eb-6147d4c19418', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2025-01-03 11:42:40,436] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Joined group at generation 3 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.1.5:8083-00eb67ad-2d9d-41ef-89eb-6147d4c19418', leaderUrl='http://192.168.1.5:8083/', offset=4, connectorIds=[mariadb-connector], taskIds=[mariadb-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2025-01-03 11:42:40,437] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 4 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2025-01-03 11:42:40,437] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting task mariadb-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2025-01-03 11:42:40,438] INFO [mariadb-connector|task-0] Creating task mariadb-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2025-01-03 11:42:40,439] INFO [mariadb-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mariadb-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2025-01-03 11:42:40,439] INFO [mariadb-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = null
	name = mariadb-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 11:42:40,439] INFO [mariadb-connector|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.mysql.MySqlConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2025-01-03 11:42:40,441] INFO [mariadb-connector|task-0] Instantiated task mariadb-connector-0 with version 3.0.5.Final of type io.debezium.connector.mysql.MySqlConnectorTask (org.apache.kafka.connect.runtime.Worker:665)
[2025-01-03 11:42:40,441] INFO [mariadb-connector|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 11:42:40,441] INFO [mariadb-connector|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mariadb-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:678)
[2025-01-03 11:42:40,441] INFO [mariadb-connector|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 11:42:40,441] INFO [mariadb-connector|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mariadb-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:684)
[2025-01-03 11:42:40,441] INFO [mariadb-connector|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mariadb-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2025-01-03 11:42:40,442] INFO [mariadb-connector|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:1795)
[2025-01-03 11:42:40,443] INFO [mariadb-connector|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2025-01-03 11:42:40,443] INFO [mariadb-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mysql.MySqlConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = null
	name = mariadb-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = []
	value.converter = null
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 11:42:40,443] INFO [mariadb-connector|task-0] ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-mariadb-connector-0
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 11:42:40,443] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:42:40,444] INFO [mariadb-connector|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2025-01-03 11:42:40,444] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:42:40,444] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:42:40,444] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735926160444 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:42:40,447] INFO [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] Cluster ID: gSBA4IMHSNGutK6FKyF6Lw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:42:40,448] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2025-01-03 11:42:40,449] INFO [mariadb-connector|task-0] Starting MySqlConnectorTask with configuration:
   connector.class = io.debezium.connector.mysql.MySqlConnector
   database.user = root
   database.server.id = 184054
   database.history.kafka.bootstrap.servers = localhost:9092
   database.history.kafka.topic = db.history.leafy_factory
   database.server.name = leafy_factory
   schema.history.internal.kafka.bootstrap.servers = localhost:9092
   database.port = 3306
   include.schema.changes = false
   topic.prefix = db_
   schema.history.internal.kafka.topic = db.history.internal
   task.class = io.debezium.connector.mysql.MySqlConnectorTask
   database.hostname = localhost
   database.password = ********
   name = mariadb-connector
   database.include.list = leafy_factory
 (io.debezium.connector.common.BaseSourceTask:250)
[2025-01-03 11:42:40,449] INFO [mariadb-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mysql.MySqlSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2025-01-03 11:42:40,449] INFO [mariadb-connector|task-0] Loading the custom topic naming strategy plugin: io.debezium.schema.DefaultTopicNamingStrategy (io.debezium.config.CommonConnectorConfig:1401)
[2025-01-03 11:42:40,503] INFO [mariadb-connector|task-0] Using 'SHOW MASTER STATUS' to get binary log status (io.debezium.connector.mysql.jdbc.MySqlConnection:41)
[2025-01-03 11:42:40,507] INFO [mariadb-connector|task-0] No previous offsets found (io.debezium.connector.common.BaseSourceTask:536)
[2025-01-03 11:42:40,525] INFO [mariadb-connector|task-0] KafkaSchemaHistory Consumer config: {key.deserializer=org.apache.kafka.common.serialization.StringDeserializer, value.deserializer=org.apache.kafka.common.serialization.StringDeserializer, enable.auto.commit=false, group.id=db_-schemahistory, bootstrap.servers=localhost:9092, fetch.min.bytes=1, session.timeout.ms=10000, auto.offset.reset=earliest, client.id=db_-schemahistory} (io.debezium.storage.kafka.history.KafkaSchemaHistory:245)
[2025-01-03 11:42:40,525] INFO [mariadb-connector|task-0] KafkaSchemaHistory Producer config: {retries=1, value.serializer=org.apache.kafka.common.serialization.StringSerializer, acks=1, batch.size=32768, max.block.ms=10000, bootstrap.servers=localhost:9092, buffer.memory=1048576, key.serializer=org.apache.kafka.common.serialization.StringSerializer, client.id=db_-schemahistory, linger.ms=0} (io.debezium.storage.kafka.history.KafkaSchemaHistory:246)
[2025-01-03 11:42:40,525] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = db-history-config-check (io.debezium.util.Threads:270)
[2025-01-03 11:42:40,526] INFO [mariadb-connector|task-0] Idempotence will be disabled because acks is set to 1, not set to 'all'. (org.apache.kafka.clients.producer.ProducerConfig:587)
[2025-01-03 11:42:40,526] INFO [mariadb-connector|task-0] ProducerConfig values: 
	acks = 1
	auto.include.jmx.reporter = true
	batch.size = 32768
	bootstrap.servers = [localhost:9092]
	buffer.memory = 1048576
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2025-01-03 11:42:40,526] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:42:40,527] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:42:40,527] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:42:40,527] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735926160527 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:42:40,529] INFO [mariadb-connector|task-0] [Producer clientId=db_-schemahistory] Cluster ID: gSBA4IMHSNGutK6FKyF6Lw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:42:40,582] INFO [mariadb-connector|task-0] Using 'SHOW MASTER STATUS' to get binary log status (io.debezium.connector.mysql.jdbc.MySqlConnection:41)
[2025-01-03 11:42:40,588] INFO [mariadb-connector|task-0] Closing connection before starting schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:123)
[2025-01-03 11:42:40,588] INFO [mariadb-connector|task-0] Connection gracefully closed (io.debezium.jdbc.JdbcConnection:983)
[2025-01-03 11:42:40,588] INFO [mariadb-connector|task-0] Connector started for the first time. (io.debezium.connector.common.BaseSourceTask:89)
[2025-01-03 11:42:40,589] INFO [mariadb-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = db_-schemahistory
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 11:42:40,589] INFO [mariadb-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:42:40,590] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:42:40,590] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:42:40,590] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735926160590 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:42:40,592] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Cluster ID: gSBA4IMHSNGutK6FKyF6Lw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:42:40,593] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2025-01-03 11:42:40,593] INFO [mariadb-connector|task-0] [Consumer clientId=db_-schemahistory, groupId=db_-schemahistory] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 11:42:40,595] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:42:40,595] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:42:40,595] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:42:40,595] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:42:40,596] INFO [mariadb-connector|task-0] App info kafka.consumer for db_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:42:40,596] INFO [mariadb-connector|task-0] AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [localhost:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = db_-schemahistory
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 1
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:371)
[2025-01-03 11:42:40,596] INFO [mariadb-connector|task-0] These configurations '[value.serializer, acks, batch.size, max.block.ms, buffer.memory, key.serializer, linger.ms]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2025-01-03 11:42:40,596] INFO [mariadb-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:42:40,596] INFO [mariadb-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:42:40,596] INFO [mariadb-connector|task-0] Kafka startTimeMs: 1735926160596 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:42:40,639] INFO [mariadb-connector|task-0] Database schema history topic '(name=db.history.internal, numPartitions=1, replicationFactor=default, replicasAssignments=null, configs={cleanup.policy=delete, retention.ms=9223372036854775807, retention.bytes=-1})' created (io.debezium.storage.kafka.history.KafkaSchemaHistory:555)
[2025-01-03 11:42:40,639] INFO [mariadb-connector|task-0] App info kafka.admin.client for db_-schemahistory unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2025-01-03 11:42:40,639] INFO [mariadb-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2025-01-03 11:42:40,639] INFO [mariadb-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2025-01-03 11:42:40,639] INFO [mariadb-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2025-01-03 11:42:40,639] INFO [mariadb-connector|task-0] Reconnecting after finishing schema recovery (io.debezium.connector.mysql.MySqlConnectorTask:136)
[2025-01-03 11:42:40,675] INFO [mariadb-connector|task-0] No previous offset found (io.debezium.connector.mysql.MySqlConnectorTask:147)
[2025-01-03 11:42:40,682] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = SignalProcessor (io.debezium.util.Threads:270)
[2025-01-03 11:42:40,688] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = change-event-source-coordinator (io.debezium.util.Threads:270)
[2025-01-03 11:42:40,688] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = blocking-snapshot (io.debezium.util.Threads:270)
[2025-01-03 11:42:40,689] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-change-event-source-coordinator (io.debezium.util.Threads:287)
[2025-01-03 11:42:40,689] INFO [mariadb-connector|task-0] WorkerSourceTask{id=mariadb-connector-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.AbstractWorkerSourceTask:280)
[2025-01-03 11:42:40,690] INFO [mariadb-connector|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:137)
[2025-01-03 11:42:40,690] INFO [mariadb-connector|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:140)
[2025-01-03 11:42:40,692] INFO [mariadb-connector|task-0] According to the connector configuration both schema and data will be snapshot. (io.debezium.relational.RelationalSnapshotChangeEventSource:282)
[2025-01-03 11:42:40,693] INFO [mariadb-connector|task-0] Snapshot step 1 - Preparing (io.debezium.relational.RelationalSnapshotChangeEventSource:135)
[2025-01-03 11:42:40,695] INFO [mariadb-connector|task-0] Snapshot step 2 - Determining captured tables (io.debezium.relational.RelationalSnapshotChangeEventSource:144)
[2025-01-03 11:42:40,695] INFO [mariadb-connector|task-0] Read list of available databases (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:116)
[2025-01-03 11:42:40,697] INFO [mariadb-connector|task-0] 	 list of available databases is: [information_schema, leafy_factory, mysql, performance_schema, sys, test] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:118)
[2025-01-03 11:42:40,697] INFO [mariadb-connector|task-0] Read list of available tables in each database (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:126)
[2025-01-03 11:42:40,746] INFO [mariadb-connector|task-0] 	snapshot continuing with database(s): [leafy_factory] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:147)
[2025-01-03 11:42:40,746] INFO [mariadb-connector|task-0] Adding table leafy_factory.jobs_machines to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 11:42:40,746] INFO [mariadb-connector|task-0] Adding table leafy_factory.product_cost to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 11:42:40,746] INFO [mariadb-connector|task-0] Adding table leafy_factory.production_lines to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 11:42:40,746] INFO [mariadb-connector|task-0] Adding table leafy_factory.products_raw_materials to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 11:42:40,746] INFO [mariadb-connector|task-0] Adding table leafy_factory.machines to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 11:42:40,746] INFO [mariadb-connector|task-0] Adding table leafy_factory.jobs to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 11:42:40,747] INFO [mariadb-connector|task-0] Adding table leafy_factory.raw_materials to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 11:42:40,747] INFO [mariadb-connector|task-0] Adding table leafy_factory.work_orders to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 11:42:40,747] INFO [mariadb-connector|task-0] Adding table leafy_factory.products to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 11:42:40,747] INFO [mariadb-connector|task-0] Adding table leafy_factory.factories to the list of capture schema tables (io.debezium.relational.RelationalSnapshotChangeEventSource:350)
[2025-01-03 11:42:40,747] INFO [mariadb-connector|task-0] Created connection pool with 1 threads (io.debezium.relational.RelationalSnapshotChangeEventSource:236)
[2025-01-03 11:42:40,747] INFO [mariadb-connector|task-0] Snapshot step 3 - Locking captured tables [leafy_factory.factories, leafy_factory.jobs, leafy_factory.jobs_machines, leafy_factory.machines, leafy_factory.product_cost, leafy_factory.production_lines, leafy_factory.products, leafy_factory.products_raw_materials, leafy_factory.raw_materials, leafy_factory.work_orders] (io.debezium.relational.RelationalSnapshotChangeEventSource:153)
[2025-01-03 11:42:40,749] INFO [mariadb-connector|task-0] Flush and obtain global read lock to prevent writes to database (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:488)
[2025-01-03 11:42:40,750] INFO [mariadb-connector|task-0] Snapshot step 4 - Determining snapshot offset (io.debezium.relational.RelationalSnapshotChangeEventSource:159)
[2025-01-03 11:42:40,751] INFO [mariadb-connector|task-0] Read binlog position of MySQL primary server (io.debezium.connector.mysql.MySqlSnapshotChangeEventSource:58)
[2025-01-03 11:42:40,752] INFO [mariadb-connector|task-0] Snapshot step 5 - Reading structure of captured tables (io.debezium.relational.RelationalSnapshotChangeEventSource:162)
[2025-01-03 11:42:40,752] INFO [mariadb-connector|task-0] All eligible tables schema should be captured, capturing: [leafy_factory.factories, leafy_factory.jobs, leafy_factory.jobs_machines, leafy_factory.machines, leafy_factory.product_cost, leafy_factory.production_lines, leafy_factory.products, leafy_factory.products_raw_materials, leafy_factory.raw_materials, leafy_factory.work_orders] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:314)
[2025-01-03 11:42:41,286] INFO [mariadb-connector|task-0] Reading structure of database 'leafy_factory' (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:348)
[2025-01-03 11:42:41,359] INFO [mariadb-connector|task-0] Snapshot step 6 - Persisting schema history (io.debezium.relational.RelationalSnapshotChangeEventSource:166)
[2025-01-03 11:42:41,391] INFO [mariadb-connector|task-0] Releasing global read lock to enable MySQL writes (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:497)
[2025-01-03 11:42:41,392] INFO [mariadb-connector|task-0] Writes to MySQL tables prevented for a total of 00:00:00.642 (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:501)
[2025-01-03 11:42:41,392] INFO [mariadb-connector|task-0] Snapshot step 7 - Snapshotting data (io.debezium.relational.RelationalSnapshotChangeEventSource:178)
[2025-01-03 11:42:41,392] INFO [mariadb-connector|task-0] Creating snapshot worker pool with 1 worker thread(s) (io.debezium.relational.RelationalSnapshotChangeEventSource:480)
[2025-01-03 11:42:41,393] INFO [mariadb-connector|task-0] For table 'leafy_factory.factories' using select statement: 'SELECT `id_factory`, `factory_name`, `factory_location`, `factory_timestamp` FROM `leafy_factory`.`factories`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 11:42:41,395] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.factories is OptionalLong[0] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 11:42:41,395] INFO [mariadb-connector|task-0] For table 'leafy_factory.jobs' using select statement: 'SELECT `id_job`, `target_output`, `nOk_products`, `quality_rate`, `job_status`, `creation_date`, `work_id` FROM `leafy_factory`.`jobs`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 11:42:41,397] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.jobs is OptionalLong[0] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 11:42:41,397] INFO [mariadb-connector|task-0] For table 'leafy_factory.jobs_machines' using select statement: 'SELECT `id_jobs_machines`, `job_id`, `machine_id` FROM `leafy_factory`.`jobs_machines`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 11:42:41,399] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.jobs_machines is OptionalLong[2] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 11:42:41,399] INFO [mariadb-connector|task-0] For table 'leafy_factory.machines' using select statement: 'SELECT `id_machine`, `machine_status`, `last_maintenance`, `operator`, `avg_output`, `reject_count`, `production_line_id` FROM `leafy_factory`.`machines`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 11:42:41,400] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.machines is OptionalLong[4] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 11:42:41,400] INFO [mariadb-connector|task-0] For table 'leafy_factory.product_cost' using select statement: 'SELECT `id_cost`, `raw_material_cost_per_product`, `overhead_per_product`, `total_cost_per_product`, `cost_ok_with_overhead`, `cost_nok_with_overhead`, `actual_total_cost`, `work_id` FROM `leafy_factory`.`product_cost`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 11:42:41,402] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.product_cost is OptionalLong[12] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 11:42:41,402] INFO [mariadb-connector|task-0] For table 'leafy_factory.production_lines' using select statement: 'SELECT `id_production_line`, `factory_id` FROM `leafy_factory`.`production_lines`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 11:42:41,403] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.production_lines is OptionalLong[2] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 11:42:41,403] INFO [mariadb-connector|task-0] For table 'leafy_factory.products' using select statement: 'SELECT `id_product`, `product_name`, `product_description` FROM `leafy_factory`.`products`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 11:42:41,404] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.products is OptionalLong[2] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 11:42:41,404] INFO [mariadb-connector|task-0] For table 'leafy_factory.products_raw_materials' using select statement: 'SELECT `id_products_raw_materials`, `product_id`, `raw_material_id` FROM `leafy_factory`.`products_raw_materials`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 11:42:41,405] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.products_raw_materials is OptionalLong[9] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 11:42:41,405] INFO [mariadb-connector|task-0] For table 'leafy_factory.raw_materials' using select statement: 'SELECT `id_raw_material`, `item_code`, `raw_material_name`, `raw_material_description`, `unit_measurement`, `raw_material_stock`, `raw_material_status`, `raw_material_currency`, `cost_per_part` FROM `leafy_factory`.`raw_materials`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 11:42:41,406] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.raw_materials is OptionalLong[8] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 11:42:41,406] INFO [mariadb-connector|task-0] For table 'leafy_factory.work_orders' using select statement: 'SELECT `id_work`, `planned_start_date`, `planned_end_date`, `actual_start_date`, `actual_end_date`, `quantity`, `wo_status`, `creation_date`, `product_id`, `nOk_products` FROM `leafy_factory`.`work_orders`' (io.debezium.relational.RelationalSnapshotChangeEventSource:489)
[2025-01-03 11:42:41,407] INFO [mariadb-connector|task-0] Estimated row count for table leafy_factory.work_orders is OptionalLong[12] (io.debezium.connector.binlog.BinlogSnapshotChangeEventSource:554)
[2025-01-03 11:42:41,408] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.factories' (1 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 11:42:41,419] INFO [mariadb-connector|task-0] 	 Finished exporting 1 records for table 'leafy_factory.factories' (1 of 10 tables); total duration '00:00:00.011' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 11:42:41,420] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.jobs' (2 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 11:42:41,421] INFO [mariadb-connector|task-0] 	 Finished exporting 1 records for table 'leafy_factory.jobs' (2 of 10 tables); total duration '00:00:00.001' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 11:42:41,422] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.jobs_machines' (3 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 11:42:41,423] INFO [mariadb-connector|task-0] 	 Finished exporting 2 records for table 'leafy_factory.jobs_machines' (3 of 10 tables); total duration '00:00:00.001' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 11:42:41,423] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.machines' (4 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 11:42:41,425] INFO [mariadb-connector|task-0] 	 Finished exporting 4 records for table 'leafy_factory.machines' (4 of 10 tables); total duration '00:00:00.002' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 11:42:41,425] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.product_cost' (5 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 11:42:41,428] INFO [mariadb-connector|task-0] 	 Finished exporting 25 records for table 'leafy_factory.product_cost' (5 of 10 tables); total duration '00:00:00.003' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 11:42:41,428] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.production_lines' (6 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 11:42:41,429] INFO [mariadb-connector|task-0] 	 Finished exporting 2 records for table 'leafy_factory.production_lines' (6 of 10 tables); total duration '00:00:00.001' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 11:42:41,429] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.products' (7 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 11:42:41,430] INFO [mariadb-connector|task-0] 	 Finished exporting 2 records for table 'leafy_factory.products' (7 of 10 tables); total duration '00:00:00.001' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 11:42:41,431] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.products_raw_materials' (8 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 11:42:41,433] INFO [mariadb-connector|task-0] 	 Finished exporting 8 records for table 'leafy_factory.products_raw_materials' (8 of 10 tables); total duration '00:00:00.002' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 11:42:41,434] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.raw_materials' (9 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 11:42:41,435] INFO [mariadb-connector|task-0] 	 Finished exporting 8 records for table 'leafy_factory.raw_materials' (9 of 10 tables); total duration '00:00:00.001' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 11:42:41,436] INFO [mariadb-connector|task-0] Exporting data from table 'leafy_factory.work_orders' (10 of 10 tables) (io.debezium.relational.RelationalSnapshotChangeEventSource:614)
[2025-01-03 11:42:41,441] INFO [mariadb-connector|task-0] 	 Finished exporting 25 records for table 'leafy_factory.work_orders' (10 of 10 tables); total duration '00:00:00.005' (io.debezium.relational.RelationalSnapshotChangeEventSource:660)
[2025-01-03 11:42:41,442] INFO [mariadb-connector|task-0] Snapshot - Final stage (io.debezium.pipeline.source.AbstractSnapshotChangeEventSource:108)
[2025-01-03 11:42:41,442] INFO [mariadb-connector|task-0] Snapshot completed (io.debezium.pipeline.source.AbstractSnapshotChangeEventSource:112)
[2025-01-03 11:42:41,447] INFO [mariadb-connector|task-0] Snapshot ended with SnapshotResult [status=COMPLETED, offset=BinlogOffsetContext{sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=BinlogSourceInfo{currentGtid='null', currentBinlogFilename='mariadb-bin.000001', currentBinlogPosition=65386, currentRowNumber=0, serverId=0, sourceTime=2025-01-03T17:42:41Z, threadId=-1, currentQuery='null', tableIds=[leafy_factory.work_orders], databaseName='leafy_factory'}, snapshotCompleted=true, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet='null', currentGtidSet='null', restartBinlogFilename='mariadb-bin.000001', restartBinlogPosition=65386, restartRowsToSkip=0, restartEventsToSkip=0, currentEventLengthInBytes=0, inTransaction=false, transactionId='null', incrementalSnapshotContext=IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]}] (io.debezium.pipeline.ChangeEventSourceCoordinator:298)
[2025-01-03 11:42:41,449] INFO [mariadb-connector|task-0] Requested thread factory for component MySqlConnector, id = db_ named = binlog-client (io.debezium.util.Threads:270)
[2025-01-03 11:42:41,451] INFO [mariadb-connector|task-0] Enable ssl PREFERRED mode for connector db_ (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:1289)
[2025-01-03 11:42:41,454] INFO [mariadb-connector|task-0] SignalProcessor started. Scheduling it every 5000ms (io.debezium.pipeline.signal.SignalProcessor:105)
[2025-01-03 11:42:41,454] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-SignalProcessor (io.debezium.util.Threads:287)
[2025-01-03 11:42:41,455] INFO [mariadb-connector|task-0] Starting streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:323)
[2025-01-03 11:42:41,455] INFO [mariadb-connector|task-0] Skip 0 events on streaming start (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:278)
[2025-01-03 11:42:41,455] INFO [mariadb-connector|task-0] Skip 0 rows on streaming start (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:282)
[2025-01-03 11:42:41,455] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-binlog-client (io.debezium.util.Threads:287)
[2025-01-03 11:42:41,456] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-binlog-client (io.debezium.util.Threads:287)
[2025-01-03 11:42:41,486] INFO [mariadb-connector|task-0] Connected to binlog at localhost:3306, starting at BinlogOffsetContext{sourceInfoSchema=Schema{io.debezium.connector.mysql.Source:STRUCT}, sourceInfo=BinlogSourceInfo{currentGtid='null', currentBinlogFilename='mariadb-bin.000001', currentBinlogPosition=65386, currentRowNumber=0, serverId=0, sourceTime=2025-01-03T17:42:41Z, threadId=-1, currentQuery='null', tableIds=[leafy_factory.work_orders], databaseName='leafy_factory'}, snapshotCompleted=true, transactionContext=TransactionContext [currentTransactionId=null, perTableEventCount={}, totalEventCount=0], restartGtidSet='null', currentGtidSet='null', restartBinlogFilename='mariadb-bin.000001', restartBinlogPosition=65386, restartRowsToSkip=0, restartEventsToSkip=0, currentEventLengthInBytes=0, inTransaction=false, transactionId='null', incrementalSnapshotContext=IncrementalSnapshotContext [windowOpened=false, chunkEndPosition=null, dataCollectionsToSnapshot=[], lastEventKeySent=null, maximumKey=null]} (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:1232)
[2025-01-03 11:42:41,486] INFO [mariadb-connector|task-0] Waiting for keepalive thread to start (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:299)
[2025-01-03 11:42:41,486] INFO [mariadb-connector|task-0] Creating thread debezium-mysqlconnector-db_-binlog-client (io.debezium.util.Threads:287)
[2025-01-03 11:42:41,591] INFO [mariadb-connector|task-0] Keepalive thread is running (io.debezium.connector.binlog.BinlogStreamingChangeEventSource:306)
[2025-01-03 11:42:41,719] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 4 : {db_.leafy_factory.factories=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 11:42:41,839] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 7 : {db_.leafy_factory.jobs=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 11:42:41,984] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 10 : {db_.leafy_factory.jobs_machines=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 11:42:42,124] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 13 : {db_.leafy_factory.machines=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 11:42:42,251] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 17 : {db_.leafy_factory.product_cost=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 11:42:42,395] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 23 : {db_.leafy_factory.production_lines=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 11:42:42,533] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 32 : {db_.leafy_factory.products=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 11:42:42,665] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 36 : {db_.leafy_factory.products_raw_materials=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 11:42:42,804] WARN [mariadb-connector|task-0] [Producer clientId=connector-producer-mariadb-connector-0] The metadata response from the cluster reported a recoverable issue with correlation id 40 : {db_.leafy_factory.raw_materials=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient:1218)
[2025-01-03 11:42:50,454] INFO [mariadb-connector|task-0|offsets] WorkerSourceTask{id=mariadb-connector-0} Committing offsets for 78 acknowledged messages (org.apache.kafka.connect.runtime.WorkerSourceTask:236)
[2025-01-03 11:43:16,317] INFO MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = 
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = db_.leafy_factory.work_orders
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 11:43:16,321] INFO MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = 
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = db_.leafy_factory.jobs
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 11:43:16,334] INFO MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = 
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = db_.leafy_factory.work_orders
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 11:43:16,336] INFO MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = 
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = db_.leafy_factory.jobs
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 11:43:16,385] INFO MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync", "version": "4.7.2"}, "os": {"type": "Darwin", "name": "Mac OS X", "architecture": "aarch64", "version": "15.0"}, "platform": "Java/Oracle Corporation/23+37-2369", "application": {"name": "IST-Shared"}} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=majority, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=MongoCredential{mechanism=null, userName='leafy_factory', source='admin', password=<hidden>, mechanismProperties=<hidden>}, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@1e900c4a]}, clusterSettings={hosts=[127.0.0.1:27017], srvHost=ist-shared.n0kts.mongodb.net, srvServiceName=mongodb, mode=MULTIPLE, requiredClusterType=REPLICA_SET, requiredReplicaSetName='atlas-133bp7-shard-0', serverSelector='null', clusterListeners='[com.mongodb.kafka.connect.util.ConnectionValidator$1@668f8e3d]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=true, invalidHostNameAllowed=false, context=null}, applicationName='IST-Shared', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null} (org.mongodb.driver.client:71)
[2025-01-03 11:43:16,536] INFO Adding discovered server ist-shared-shard-00-01.n0kts.mongodb.net:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2025-01-03 11:43:16,551] INFO Adding discovered server ist-shared-shard-00-02.n0kts.mongodb.net:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2025-01-03 11:43:16,552] INFO Adding discovered server ist-shared-shard-00-00.n0kts.mongodb.net:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2025-01-03 11:43:17,458] INFO Opened connection [connectionId{localValue:2, serverValue:1159991}] to ist-shared-shard-00-00.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:43:17,459] INFO Opened connection [connectionId{localValue:1, serverValue:700786}] to ist-shared-shard-00-02.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:43:17,458] INFO Opened connection [connectionId{localValue:4, serverValue:700784}] to ist-shared-shard-00-02.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:43:17,458] INFO Opened connection [connectionId{localValue:5, serverValue:1159990}] to ist-shared-shard-00-00.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:43:17,470] INFO Monitor thread successfully connected to server with description ServerDescription{address=ist-shared-shard-00-00.n0kts.mongodb.net:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=507149791, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-00.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az1'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=null, setVersion=71, topologyVersion=TopologyVersion{processId=675b24d1d1431a587be694a8, counter=19}, lastWriteDate=Fri Jan 03 11:43:17 CST 2025, lastUpdateTimeNanos=49247888947125} (org.mongodb.driver.cluster:71)
[2025-01-03 11:43:17,470] INFO Monitor thread successfully connected to server with description ServerDescription{address=ist-shared-shard-00-02.n0kts.mongodb.net:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=506957875, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-02.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az4'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=null, setVersion=71, topologyVersion=TopologyVersion{processId=67694a2575e55e11df78dcd1, counter=14}, lastWriteDate=Fri Jan 03 11:43:17 CST 2025, lastUpdateTimeNanos=49247888751250} (org.mongodb.driver.cluster:71)
[2025-01-03 11:43:17,486] INFO No server chosen by ReadPreferenceServerSelector{readPreference=primary} from cluster description ClusterDescription{type=REPLICA_SET, connectionMode=MULTIPLE, serverDescriptions=[ServerDescription{address=ist-shared-shard-00-02.n0kts.mongodb.net:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=506957875, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-02.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az4'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=null, setVersion=71, topologyVersion=TopologyVersion{processId=67694a2575e55e11df78dcd1, counter=14}, lastWriteDate=Fri Jan 03 11:43:17 CST 2025, lastUpdateTimeNanos=49247888751250}, ServerDescription{address=ist-shared-shard-00-01.n0kts.mongodb.net:27017, type=UNKNOWN, state=CONNECTING}, ServerDescription{address=ist-shared-shard-00-00.n0kts.mongodb.net:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=507149791, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-00.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az1'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=null, setVersion=71, topologyVersion=TopologyVersion{processId=675b24d1d1431a587be694a8, counter=19}, lastWriteDate=Fri Jan 03 11:43:17 CST 2025, lastUpdateTimeNanos=49247888947125}]}. Waiting for 30000 ms before timing out (org.mongodb.driver.cluster:71)
[2025-01-03 11:43:17,500] INFO Opened connection [connectionId{localValue:6, serverValue:1297612}] to ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:43:17,500] INFO Monitor thread successfully connected to server with description ServerDescription{address=ist-shared-shard-00-01.n0kts.mongodb.net:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=523431917, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-01.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az2'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=7fffffff000000000000010a, setVersion=71, topologyVersion=TopologyVersion{processId=67694b4e6118cd58a4791747, counter=41}, lastWriteDate=Fri Jan 03 11:43:17 CST 2025, lastUpdateTimeNanos=49247931896041} (org.mongodb.driver.cluster:71)
[2025-01-03 11:43:17,502] INFO Setting max election id to 7fffffff000000000000010a from replica set primary ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.cluster:71)
[2025-01-03 11:43:17,502] INFO Setting max set version to 71 from replica set primary ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.cluster:71)
[2025-01-03 11:43:17,502] INFO Discovered replica set primary ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.cluster:71)
[2025-01-03 11:43:17,504] INFO Opened connection [connectionId{localValue:3, serverValue:1297614}] to ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:43:18,256] INFO Opened connection [connectionId{localValue:7, serverValue:1297618}] to ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:43:18,638] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:371)
[2025-01-03 11:43:18,649] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Connector mongodb-sink config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2025-01-03 11:43:18,650] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2025-01-03 11:43:18,650] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 11:43:18,652] INFO [0:0:0:0:0:0:0:1] - - [03/Jan/2025:17:43:16 +0000] "POST /connectors HTTP/1.1" 201 863 "-" "curl/8.7.1" 2620 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2025-01-03 11:43:18,653] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=4, memberId='connect-192.168.1.5:8083-00eb67ad-2d9d-41ef-89eb-6147d4c19418', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2025-01-03 11:43:18,656] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=4, memberId='connect-192.168.1.5:8083-00eb67ad-2d9d-41ef-89eb-6147d4c19418', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2025-01-03 11:43:18,657] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Joined group at generation 4 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.1.5:8083-00eb67ad-2d9d-41ef-89eb-6147d4c19418', leaderUrl='http://192.168.1.5:8083/', offset=5, connectorIds=[mongodb-sink, mariadb-connector], taskIds=[mariadb-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2025-01-03 11:43:18,657] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 5 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2025-01-03 11:43:18,657] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connector mongodb-sink (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2025-01-03 11:43:18,657] INFO [mongodb-sink|worker] Creating connector mongodb-sink of type com.mongodb.kafka.connect.MongoSinkConnector (org.apache.kafka.connect.runtime.Worker:313)
[2025-01-03 11:43:18,658] INFO [mongodb-sink|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = true
	errors.log.include.messages = true
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = all
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongodb-sink
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [db_.leafy_factory.work_orders, db_.leafy_factory.jobs]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2025-01-03 11:43:18,658] INFO [mongodb-sink|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = true
	errors.log.include.messages = true
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = all
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongodb-sink
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [db_.leafy_factory.work_orders, db_.leafy_factory.jobs]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 11:43:18,658] INFO [mongodb-sink|worker] Instantiated connector mongodb-sink with version 1.14.1 of type class com.mongodb.kafka.connect.MongoSinkConnector (org.apache.kafka.connect.runtime.Worker:335)
[2025-01-03 11:43:18,659] INFO [mongodb-sink|worker] Finished creating connector mongodb-sink (org.apache.kafka.connect.runtime.Worker:356)
[2025-01-03 11:43:18,659] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2025-01-03 11:43:18,661] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = true
	errors.log.include.messages = true
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = all
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongodb-sink
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [db_.leafy_factory.work_orders, db_.leafy_factory.jobs]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2025-01-03 11:43:18,662] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = true
	errors.log.include.messages = true
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = all
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongodb-sink
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [db_.leafy_factory.work_orders, db_.leafy_factory.jobs]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 11:43:18,671] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Tasks [mongodb-sink-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2025-01-03 11:43:18,671] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2025-01-03 11:43:18,671] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2025-01-03 11:43:18,673] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully joined group with generation Generation{generationId=5, memberId='connect-192.168.1.5:8083-00eb67ad-2d9d-41ef-89eb-6147d4c19418', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2025-01-03 11:43:18,675] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Successfully synced group in generation Generation{generationId=5, memberId='connect-192.168.1.5:8083-00eb67ad-2d9d-41ef-89eb-6147d4c19418', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2025-01-03 11:43:18,675] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Joined group at generation 5 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-192.168.1.5:8083-00eb67ad-2d9d-41ef-89eb-6147d4c19418', leaderUrl='http://192.168.1.5:8083/', offset=7, connectorIds=[mongodb-sink, mariadb-connector], taskIds=[mongodb-sink-0, mariadb-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2025-01-03 11:43:18,675] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting connectors and tasks using config offset 7 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2025-01-03 11:43:18,675] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Starting task mongodb-sink-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2025-01-03 11:43:18,677] INFO [mongodb-sink|task-0] Creating task mongodb-sink-0 (org.apache.kafka.connect.runtime.Worker:646)
[2025-01-03 11:43:18,678] INFO [mongodb-sink|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = true
	errors.log.include.messages = true
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = all
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongodb-sink
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2025-01-03 11:43:18,678] INFO [mongodb-sink|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.log.enable = true
	errors.log.include.messages = true
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = all
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongodb-sink
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 11:43:18,678] INFO [mongodb-sink|task-0] TaskConfig values: 
	task.class = class com.mongodb.kafka.connect.sink.MongoSinkTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2025-01-03 11:43:18,678] INFO [mongodb-sink|task-0] Instantiated task mongodb-sink-0 with version 1.14.1 of type com.mongodb.kafka.connect.sink.MongoSinkTask (org.apache.kafka.connect.runtime.Worker:665)
[2025-01-03 11:43:18,678] INFO [mongodb-sink|task-0] StringConverterConfig values: 
	converter.encoding = UTF-8
	converter.type = key
 (org.apache.kafka.connect.storage.StringConverterConfig:371)
[2025-01-03 11:43:18,678] INFO [mongodb-sink|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2025-01-03 11:43:18,679] INFO [mongodb-sink|task-0] Set up the key converter class org.apache.kafka.connect.storage.StringConverter for task mongodb-sink-0 using the connector config (org.apache.kafka.connect.runtime.Worker:680)
[2025-01-03 11:43:18,679] INFO [mongodb-sink|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongodb-sink-0 using the connector config (org.apache.kafka.connect.runtime.Worker:686)
[2025-01-03 11:43:18,679] INFO [mongodb-sink|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongodb-sink-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2025-01-03 11:43:18,679] INFO [mongodb-sink|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{} (org.apache.kafka.connect.runtime.Worker:1795)
[2025-01-03 11:43:18,679] INFO [mongodb-sink|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = true
	errors.log.include.messages = true
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = all
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongodb-sink
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [db_.leafy_factory.work_orders, db_.leafy_factory.jobs]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2025-01-03 11:43:18,679] INFO [mongodb-sink|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = com.mongodb.kafka.connect.MongoSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = true
	errors.log.include.messages = true
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = all
	header.converter = null
	key.converter = class org.apache.kafka.connect.storage.StringConverter
	name = mongodb-sink
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [db_.leafy_factory.work_orders, db_.leafy_factory.jobs]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2025-01-03 11:43:18,680] INFO [mongodb-sink|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mongodb-sink-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mongodb-sink
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2025-01-03 11:43:18,680] INFO [mongodb-sink|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2025-01-03 11:43:18,682] INFO [mongodb-sink|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2025-01-03 11:43:18,682] INFO [mongodb-sink|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2025-01-03 11:43:18,682] INFO [mongodb-sink|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2025-01-03 11:43:18,682] INFO [mongodb-sink|task-0] Kafka startTimeMs: 1735926198682 (org.apache.kafka.common.utils.AppInfoParser:127)
[2025-01-03 11:43:18,685] INFO [Worker clientId=connect-192.168.1.5:8083, groupId=connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2025-01-03 11:43:18,686] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Subscribed to topic(s): db_.leafy_factory.work_orders, db_.leafy_factory.jobs (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:481)
[2025-01-03 11:43:18,686] INFO [mongodb-sink|task-0] Starting MongoDB sink task (com.mongodb.kafka.connect.sink.MongoSinkTask:66)
[2025-01-03 11:43:18,703] INFO [mongodb-sink|task-0] MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = 
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = db_.leafy_factory.work_orders
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 11:43:18,704] INFO [mongodb-sink|task-0] MongoSinkTopicConfig values: 
	bulk.write.ordered = true
	change.data.capture.handler = 
	collection = 
	database = leafy_factory
	delete.on.null.values = false
	delete.writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.DeleteOneDefaultStrategy
	document.id.strategy = com.mongodb.kafka.connect.sink.processor.id.strategy.BsonOidStrategy
	document.id.strategy.overwrite.existing = false
	document.id.strategy.partial.key.projection.list = 
	document.id.strategy.partial.key.projection.type = 
	document.id.strategy.partial.value.projection.list = 
	document.id.strategy.partial.value.projection.type = 
	document.id.strategy.uuid.format = string
	errors.log.enable = true
	errors.tolerance = all
	field.renamer.mapping = []
	field.renamer.regexp = []
	key.projection.list = 
	key.projection.type = none
	max.batch.size = 0
	mongo.errors.log.enable = false
	mongo.errors.tolerance = none
	namespace.mapper = com.mongodb.kafka.connect.sink.namespace.mapping.DefaultNamespaceMapper
	namespace.mapper.error.if.invalid = false
	namespace.mapper.key.collection.field = 
	namespace.mapper.key.database.field = 
	namespace.mapper.value.collection.field = 
	namespace.mapper.value.database.field = 
	post.processor.chain = [com.mongodb.kafka.connect.sink.processor.DocumentIdAdder]
	rate.limiting.every.n = 0
	rate.limiting.timeout = 0
	timeseries.expire.after.seconds = 0
	timeseries.granularity = 
	timeseries.metafield = 
	timeseries.timefield = 
	timeseries.timefield.auto.convert = false
	timeseries.timefield.auto.convert.date.format = yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]]
	timeseries.timefield.auto.convert.locale.language.tag = 
	topic = db_.leafy_factory.jobs
	value.projection.list = 
	value.projection.type = none
	writemodel.strategy = com.mongodb.kafka.connect.sink.writemodel.strategy.ReplaceOneDefaultStrategy
 (com.mongodb.kafka.connect.sink.MongoSinkTopicConfig:371)
[2025-01-03 11:43:18,705] INFO [mongodb-sink|task-0] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync|mongo-kafka|sink", "version": "4.7.2|1.14.1"}, "os": {"type": "Darwin", "name": "Mac OS X", "architecture": "aarch64", "version": "15.0"}, "platform": "Java/Oracle Corporation/23+37-2369", "application": {"name": "IST-Shared"}} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=majority, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=MongoCredential{mechanism=null, userName='leafy_factory', source='admin', password=<hidden>, mechanismProperties=<hidden>}, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.Jep395RecordCodecProvider@1e900c4a]}, clusterSettings={hosts=[127.0.0.1:27017], srvHost=ist-shared.n0kts.mongodb.net, srvServiceName=mongodb, mode=MULTIPLE, requiredClusterType=REPLICA_SET, requiredReplicaSetName='atlas-133bp7-shard-0', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='30000 ms', localThreshold='30000 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, sendBufferSize=0}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, sendBufferSize=0}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=true, invalidHostNameAllowed=false, context=null}, applicationName='IST-Shared', compressorList=[], uuidRepresentation=UNSPECIFIED, serverApi=null, autoEncryptionSettings=null, contextProvider=null} (org.mongodb.driver.client:71)
[2025-01-03 11:43:18,711] INFO [mongodb-sink|task-0] Adding discovered server ist-shared-shard-00-01.n0kts.mongodb.net:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2025-01-03 11:43:18,711] INFO [mongodb-sink|task-0] Adding discovered server ist-shared-shard-00-02.n0kts.mongodb.net:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2025-01-03 11:43:18,711] INFO [mongodb-sink|task-0] WorkerSinkTask{id=mongodb-sink-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:325)
[2025-01-03 11:43:18,712] INFO [mongodb-sink|task-0] WorkerSinkTask{id=mongodb-sink-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:211)
[2025-01-03 11:43:18,712] INFO [mongodb-sink|task-0] Adding discovered server ist-shared-shard-00-00.n0kts.mongodb.net:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2025-01-03 11:43:18,717] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Cluster ID: gSBA4IMHSNGutK6FKyF6Lw (org.apache.kafka.clients.Metadata:365)
[2025-01-03 11:43:18,717] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Discovered group coordinator 192.168.1.5:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:937)
[2025-01-03 11:43:18,718] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:605)
[2025-01-03 11:43:18,724] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Request joining group due to: need to re-join with the given member-id: connector-consumer-mongodb-sink-0-a45d2847-95a8-4aa4-9f7d-017a8cf8190d (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2025-01-03 11:43:18,724] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:605)
[2025-01-03 11:43:18,725] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Successfully joined group with generation Generation{generationId=1, memberId='connector-consumer-mongodb-sink-0-a45d2847-95a8-4aa4-9f7d-017a8cf8190d', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:666)
[2025-01-03 11:43:18,728] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Finished assignment for group at generation 1: {connector-consumer-mongodb-sink-0-a45d2847-95a8-4aa4-9f7d-017a8cf8190d=Assignment(partitions=[db_.leafy_factory.work_orders-0, db_.leafy_factory.jobs-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:664)
[2025-01-03 11:43:18,731] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Successfully synced group in generation Generation{generationId=1, memberId='connector-consumer-mongodb-sink-0-a45d2847-95a8-4aa4-9f7d-017a8cf8190d', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:843)
[2025-01-03 11:43:18,731] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Notifying assignor about the new Assignment(partitions=[db_.leafy_factory.work_orders-0, db_.leafy_factory.jobs-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:324)
[2025-01-03 11:43:18,732] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Adding newly assigned partitions: db_.leafy_factory.jobs-0, db_.leafy_factory.work_orders-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:58)
[2025-01-03 11:43:18,733] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Found no committed offset for partition db_.leafy_factory.work_orders-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1508)
[2025-01-03 11:43:18,733] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Found no committed offset for partition db_.leafy_factory.jobs-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1508)
[2025-01-03 11:43:18,738] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Resetting offset for partition db_.leafy_factory.work_orders-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:43:18,738] INFO [mongodb-sink|task-0] [Consumer clientId=connector-consumer-mongodb-sink-0, groupId=connect-mongodb-sink] Resetting offset for partition db_.leafy_factory.jobs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.1.5:9092 (id: 0 rack: null)], epoch=0}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2025-01-03 11:43:18,766] INFO [mongodb-sink|task-0] No server chosen by com.mongodb.client.internal.MongoClientDelegate$1@564f538b from cluster description ClusterDescription{type=REPLICA_SET, connectionMode=MULTIPLE, serverDescriptions=[ServerDescription{address=ist-shared-shard-00-02.n0kts.mongodb.net:27017, type=UNKNOWN, state=CONNECTING}, ServerDescription{address=ist-shared-shard-00-01.n0kts.mongodb.net:27017, type=UNKNOWN, state=CONNECTING}, ServerDescription{address=ist-shared-shard-00-00.n0kts.mongodb.net:27017, type=UNKNOWN, state=CONNECTING}]}. Waiting for 30000 ms before timing out (org.mongodb.driver.cluster:71)
[2025-01-03 11:43:18,984] INFO [mongodb-sink|task-0] Opened connection [connectionId{localValue:11, serverValue:700788}] to ist-shared-shard-00-02.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:43:18,984] INFO [mongodb-sink|task-0] Opened connection [connectionId{localValue:13, serverValue:1159995}] to ist-shared-shard-00-00.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:43:18,984] INFO [mongodb-sink|task-0] Opened connection [connectionId{localValue:10, serverValue:700789}] to ist-shared-shard-00-02.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:43:18,984] INFO [mongodb-sink|task-0] Opened connection [connectionId{localValue:12, serverValue:1159994}] to ist-shared-shard-00-00.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:43:18,985] INFO [mongodb-sink|task-0] Monitor thread successfully connected to server with description ServerDescription{address=ist-shared-shard-00-00.n0kts.mongodb.net:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=178887250, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-00.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az1'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=null, setVersion=71, topologyVersion=TopologyVersion{processId=675b24d1d1431a587be694a8, counter=19}, lastWriteDate=Fri Jan 03 11:43:18 CST 2025, lastUpdateTimeNanos=49249416201916} (org.mongodb.driver.cluster:71)
[2025-01-03 11:43:18,985] INFO [mongodb-sink|task-0] Monitor thread successfully connected to server with description ServerDescription{address=ist-shared-shard-00-02.n0kts.mongodb.net:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=178908625, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-02.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az4'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=null, setVersion=71, topologyVersion=TopologyVersion{processId=67694a2575e55e11df78dcd1, counter=14}, lastWriteDate=Fri Jan 03 11:43:18 CST 2025, lastUpdateTimeNanos=49249416201875} (org.mongodb.driver.cluster:71)
[2025-01-03 11:43:18,987] INFO [mongodb-sink|task-0] No server chosen by WritableServerSelector from cluster description ClusterDescription{type=REPLICA_SET, connectionMode=MULTIPLE, serverDescriptions=[ServerDescription{address=ist-shared-shard-00-02.n0kts.mongodb.net:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=178908625, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-02.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az4'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=null, setVersion=71, topologyVersion=TopologyVersion{processId=67694a2575e55e11df78dcd1, counter=14}, lastWriteDate=Fri Jan 03 11:43:18 CST 2025, lastUpdateTimeNanos=49249416201875}, ServerDescription{address=ist-shared-shard-00-01.n0kts.mongodb.net:27017, type=UNKNOWN, state=CONNECTING}, ServerDescription{address=ist-shared-shard-00-00.n0kts.mongodb.net:27017, type=REPLICA_SET_SECONDARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=178887250, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-00.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az1'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=null, setVersion=71, topologyVersion=TopologyVersion{processId=675b24d1d1431a587be694a8, counter=19}, lastWriteDate=Fri Jan 03 11:43:18 CST 2025, lastUpdateTimeNanos=49249416201916}]}. Waiting for 30000 ms before timing out (org.mongodb.driver.cluster:71)
[2025-01-03 11:43:19,070] INFO [mongodb-sink|task-0] Opened connection [connectionId{localValue:8, serverValue:1297619}] to ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:43:19,070] INFO [mongodb-sink|task-0] Opened connection [connectionId{localValue:9, serverValue:1297620}] to ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:43:19,071] INFO [mongodb-sink|task-0] Monitor thread successfully connected to server with description ServerDescription{address=ist-shared-shard-00-01.n0kts.mongodb.net:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=25, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=234353875, setName='atlas-133bp7-shard-0', canonicalAddress=ist-shared-shard-00-01.n0kts.mongodb.net:27017, hosts=[ist-shared-shard-00-02.n0kts.mongodb.net:27017, ist-shared-shard-00-01.n0kts.mongodb.net:27017, ist-shared-shard-00-00.n0kts.mongodb.net:27017], passives=[], arbiters=[], primary='ist-shared-shard-00-01.n0kts.mongodb.net:27017', tagSet=TagSet{[Tag{name='availabilityZone', value='cac1-az2'}, Tag{name='diskState', value='READY'}, Tag{name='nodeType', value='ELECTABLE'}, Tag{name='provider', value='AWS'}, Tag{name='region', value='CA_CENTRAL_1'}, Tag{name='workloadType', value='OPERATIONAL'}]}, electionId=7fffffff000000000000010a, setVersion=71, topologyVersion=TopologyVersion{processId=67694b4e6118cd58a4791747, counter=41}, lastWriteDate=Fri Jan 03 11:43:18 CST 2025, lastUpdateTimeNanos=49249502137500} (org.mongodb.driver.cluster:71)
[2025-01-03 11:43:19,072] INFO [mongodb-sink|task-0] Setting max election id to 7fffffff000000000000010a from replica set primary ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.cluster:71)
[2025-01-03 11:43:19,072] INFO [mongodb-sink|task-0] Setting max set version to 71 from replica set primary ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.cluster:71)
[2025-01-03 11:43:19,072] INFO [mongodb-sink|task-0] Discovered replica set primary ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.cluster:71)
[2025-01-03 11:43:19,881] INFO [mongodb-sink|task-0] Opened connection [connectionId{localValue:14, serverValue:1297621}] to ist-shared-shard-00-01.n0kts.mongodb.net:27017 (org.mongodb.driver.connection:71)
[2025-01-03 11:44:15,122] INFO [mariadb-connector|task-0] 84 records sent during previous 00:01:34.681, last recorded offset of {server=db_} partition is {ts_sec=1735926254, file=mariadb-bin.000001, pos=0, row=1, server_id=1} (io.debezium.connector.common.BaseSourceTask:351)
[2025-01-03 11:44:20,506] INFO [mariadb-connector|task-0|offsets] WorkerSourceTask{id=mariadb-connector-0} Committing offsets for 6 acknowledged messages (org.apache.kafka.connect.runtime.WorkerSourceTask:236)
[2025-01-03 11:53:46,819] INFO [mariadb-connector|task-0] 6 records sent during previous 00:09:31.697, last recorded offset of {server=db_} partition is {ts_sec=1735926826, file=mariadb-bin.000001, pos=0, row=1, server_id=1} (io.debezium.connector.common.BaseSourceTask:351)
[2025-01-03 11:53:50,738] INFO [mariadb-connector|task-0|offsets] WorkerSourceTask{id=mariadb-connector-0} Committing offsets for 6 acknowledged messages (org.apache.kafka.connect.runtime.WorkerSourceTask:236)
